{
  "best_metric": 0.962,
  "best_model_checkpoint": "news-classification-2/checkpoint-7500",
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 7500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 2.340062379837036,
      "learning_rate": 1.9982222222222224e-05,
      "loss": 2.4833,
      "step": 10
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 2.3409371376037598,
      "learning_rate": 1.9964444444444447e-05,
      "loss": 2.1901,
      "step": 20
    },
    {
      "epoch": 0.016,
      "grad_norm": 2.1555848121643066,
      "learning_rate": 1.9946666666666667e-05,
      "loss": 1.9212,
      "step": 30
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 2.0559258460998535,
      "learning_rate": 1.992888888888889e-05,
      "loss": 1.7008,
      "step": 40
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 2.086430549621582,
      "learning_rate": 1.9911111111111112e-05,
      "loss": 1.4714,
      "step": 50
    },
    {
      "epoch": 0.032,
      "grad_norm": 1.9018769264221191,
      "learning_rate": 1.9893333333333335e-05,
      "loss": 1.3258,
      "step": 60
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 2.6979124546051025,
      "learning_rate": 1.9875555555555558e-05,
      "loss": 1.2274,
      "step": 70
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 2.662576675415039,
      "learning_rate": 1.985777777777778e-05,
      "loss": 1.041,
      "step": 80
    },
    {
      "epoch": 0.048,
      "grad_norm": 1.9613078832626343,
      "learning_rate": 1.9840000000000003e-05,
      "loss": 0.9608,
      "step": 90
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 2.3540279865264893,
      "learning_rate": 1.9822222222222226e-05,
      "loss": 0.867,
      "step": 100
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 2.195451259613037,
      "learning_rate": 1.9804444444444445e-05,
      "loss": 0.805,
      "step": 110
    },
    {
      "epoch": 0.064,
      "grad_norm": 2.2827296257019043,
      "learning_rate": 1.9786666666666668e-05,
      "loss": 0.717,
      "step": 120
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 2.4951789379119873,
      "learning_rate": 1.976888888888889e-05,
      "loss": 0.6991,
      "step": 130
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 2.4580349922180176,
      "learning_rate": 1.9751111111111114e-05,
      "loss": 0.6415,
      "step": 140
    },
    {
      "epoch": 0.08,
      "grad_norm": 2.92366099357605,
      "learning_rate": 1.9733333333333336e-05,
      "loss": 0.5664,
      "step": 150
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 2.836576461791992,
      "learning_rate": 1.9715555555555556e-05,
      "loss": 0.5436,
      "step": 160
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 2.117300510406494,
      "learning_rate": 1.969777777777778e-05,
      "loss": 0.5142,
      "step": 170
    },
    {
      "epoch": 0.096,
      "grad_norm": 1.8074114322662354,
      "learning_rate": 1.968e-05,
      "loss": 0.4426,
      "step": 180
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 2.525524139404297,
      "learning_rate": 1.9662222222222224e-05,
      "loss": 0.5051,
      "step": 190
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 1.9512096643447876,
      "learning_rate": 1.9644444444444447e-05,
      "loss": 0.429,
      "step": 200
    },
    {
      "epoch": 0.112,
      "grad_norm": 2.3497812747955322,
      "learning_rate": 1.9626666666666666e-05,
      "loss": 0.4546,
      "step": 210
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 2.7469656467437744,
      "learning_rate": 1.960888888888889e-05,
      "loss": 0.4736,
      "step": 220
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 2.4587910175323486,
      "learning_rate": 1.9591111111111112e-05,
      "loss": 0.4574,
      "step": 230
    },
    {
      "epoch": 0.128,
      "grad_norm": 2.678929090499878,
      "learning_rate": 1.9573333333333335e-05,
      "loss": 0.4563,
      "step": 240
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 2.524336099624634,
      "learning_rate": 1.9555555555555557e-05,
      "loss": 0.4102,
      "step": 250
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 3.5036823749542236,
      "learning_rate": 1.953777777777778e-05,
      "loss": 0.3958,
      "step": 260
    },
    {
      "epoch": 0.144,
      "grad_norm": 2.847472906112671,
      "learning_rate": 1.9520000000000003e-05,
      "loss": 0.3817,
      "step": 270
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 2.873018503189087,
      "learning_rate": 1.9502222222222226e-05,
      "loss": 0.4008,
      "step": 280
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 2.7622461318969727,
      "learning_rate": 1.9484444444444445e-05,
      "loss": 0.3814,
      "step": 290
    },
    {
      "epoch": 0.16,
      "grad_norm": 3.2881574630737305,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 0.3392,
      "step": 300
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 2.1540071964263916,
      "learning_rate": 1.944888888888889e-05,
      "loss": 0.3804,
      "step": 310
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 2.4652674198150635,
      "learning_rate": 1.9431111111111113e-05,
      "loss": 0.3656,
      "step": 320
    },
    {
      "epoch": 0.176,
      "grad_norm": 2.6359810829162598,
      "learning_rate": 1.9413333333333336e-05,
      "loss": 0.3478,
      "step": 330
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 1.7288661003112793,
      "learning_rate": 1.9395555555555555e-05,
      "loss": 0.345,
      "step": 340
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 4.310972690582275,
      "learning_rate": 1.9377777777777778e-05,
      "loss": 0.3711,
      "step": 350
    },
    {
      "epoch": 0.192,
      "grad_norm": 2.8951938152313232,
      "learning_rate": 1.936e-05,
      "loss": 0.3385,
      "step": 360
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 2.653503656387329,
      "learning_rate": 1.9342222222222224e-05,
      "loss": 0.3717,
      "step": 370
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 1.7922152280807495,
      "learning_rate": 1.9324444444444447e-05,
      "loss": 0.3024,
      "step": 380
    },
    {
      "epoch": 0.208,
      "grad_norm": 2.9919824600219727,
      "learning_rate": 1.930666666666667e-05,
      "loss": 0.3837,
      "step": 390
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 4.290222644805908,
      "learning_rate": 1.928888888888889e-05,
      "loss": 0.3877,
      "step": 400
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 1.981816053390503,
      "learning_rate": 1.9271111111111115e-05,
      "loss": 0.3121,
      "step": 410
    },
    {
      "epoch": 0.224,
      "grad_norm": 2.787489652633667,
      "learning_rate": 1.9253333333333334e-05,
      "loss": 0.29,
      "step": 420
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 2.284292221069336,
      "learning_rate": 1.9235555555555557e-05,
      "loss": 0.2845,
      "step": 430
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 2.2889513969421387,
      "learning_rate": 1.921777777777778e-05,
      "loss": 0.302,
      "step": 440
    },
    {
      "epoch": 0.24,
      "grad_norm": 3.1597371101379395,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.3186,
      "step": 450
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 3.7679572105407715,
      "learning_rate": 1.9182222222222225e-05,
      "loss": 0.3009,
      "step": 460
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 3.072195529937744,
      "learning_rate": 1.9164444444444445e-05,
      "loss": 0.3224,
      "step": 470
    },
    {
      "epoch": 0.256,
      "grad_norm": 2.4539129734039307,
      "learning_rate": 1.9146666666666667e-05,
      "loss": 0.2782,
      "step": 480
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 2.8653109073638916,
      "learning_rate": 1.912888888888889e-05,
      "loss": 0.2589,
      "step": 490
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 2.474698305130005,
      "learning_rate": 1.9111111111111113e-05,
      "loss": 0.2742,
      "step": 500
    },
    {
      "epoch": 0.272,
      "grad_norm": 1.7876826524734497,
      "learning_rate": 1.9093333333333336e-05,
      "loss": 0.2879,
      "step": 510
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 1.8565173149108887,
      "learning_rate": 1.9075555555555555e-05,
      "loss": 0.2343,
      "step": 520
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 2.3621513843536377,
      "learning_rate": 1.9057777777777778e-05,
      "loss": 0.2787,
      "step": 530
    },
    {
      "epoch": 0.288,
      "grad_norm": 2.279662609100342,
      "learning_rate": 1.904e-05,
      "loss": 0.298,
      "step": 540
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 2.2688517570495605,
      "learning_rate": 1.9022222222222223e-05,
      "loss": 0.2551,
      "step": 550
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 2.961923122406006,
      "learning_rate": 1.9004444444444446e-05,
      "loss": 0.2878,
      "step": 560
    },
    {
      "epoch": 0.304,
      "grad_norm": 3.4555869102478027,
      "learning_rate": 1.898666666666667e-05,
      "loss": 0.2674,
      "step": 570
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 3.4741828441619873,
      "learning_rate": 1.896888888888889e-05,
      "loss": 0.2678,
      "step": 580
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 2.9928531646728516,
      "learning_rate": 1.8951111111111115e-05,
      "loss": 0.2409,
      "step": 590
    },
    {
      "epoch": 0.32,
      "grad_norm": 3.1215834617614746,
      "learning_rate": 1.8933333333333334e-05,
      "loss": 0.2827,
      "step": 600
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 2.1197874546051025,
      "learning_rate": 1.8915555555555557e-05,
      "loss": 0.2862,
      "step": 610
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 1.7706376314163208,
      "learning_rate": 1.889777777777778e-05,
      "loss": 0.2415,
      "step": 620
    },
    {
      "epoch": 0.336,
      "grad_norm": 1.7263665199279785,
      "learning_rate": 1.8880000000000002e-05,
      "loss": 0.2782,
      "step": 630
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 2.8460729122161865,
      "learning_rate": 1.8862222222222225e-05,
      "loss": 0.3126,
      "step": 640
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 3.8757948875427246,
      "learning_rate": 1.8844444444444444e-05,
      "loss": 0.2266,
      "step": 650
    },
    {
      "epoch": 0.352,
      "grad_norm": 2.9246137142181396,
      "learning_rate": 1.8826666666666667e-05,
      "loss": 0.2851,
      "step": 660
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 2.8725013732910156,
      "learning_rate": 1.880888888888889e-05,
      "loss": 0.2669,
      "step": 670
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 2.025454044342041,
      "learning_rate": 1.8791111111111113e-05,
      "loss": 0.2901,
      "step": 680
    },
    {
      "epoch": 0.368,
      "grad_norm": 2.534980535507202,
      "learning_rate": 1.8773333333333335e-05,
      "loss": 0.2823,
      "step": 690
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 2.8048534393310547,
      "learning_rate": 1.8755555555555558e-05,
      "loss": 0.2384,
      "step": 700
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 2.4496400356292725,
      "learning_rate": 1.8737777777777778e-05,
      "loss": 0.285,
      "step": 710
    },
    {
      "epoch": 0.384,
      "grad_norm": 1.9532679319381714,
      "learning_rate": 1.8720000000000004e-05,
      "loss": 0.1976,
      "step": 720
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 3.9647562503814697,
      "learning_rate": 1.8702222222222223e-05,
      "loss": 0.2677,
      "step": 730
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 3.576298236846924,
      "learning_rate": 1.8684444444444446e-05,
      "loss": 0.2581,
      "step": 740
    },
    {
      "epoch": 0.4,
      "grad_norm": 3.9200901985168457,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.2654,
      "step": 750
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 2.4719555377960205,
      "learning_rate": 1.8648888888888888e-05,
      "loss": 0.2445,
      "step": 760
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 2.23500394821167,
      "learning_rate": 1.8631111111111114e-05,
      "loss": 0.2681,
      "step": 770
    },
    {
      "epoch": 0.416,
      "grad_norm": 3.358092784881592,
      "learning_rate": 1.8613333333333334e-05,
      "loss": 0.2571,
      "step": 780
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 2.4395995140075684,
      "learning_rate": 1.8595555555555556e-05,
      "loss": 0.2871,
      "step": 790
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 2.0386977195739746,
      "learning_rate": 1.857777777777778e-05,
      "loss": 0.2569,
      "step": 800
    },
    {
      "epoch": 0.432,
      "grad_norm": 2.3983476161956787,
      "learning_rate": 1.8560000000000002e-05,
      "loss": 0.2103,
      "step": 810
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 2.9427950382232666,
      "learning_rate": 1.8542222222222225e-05,
      "loss": 0.24,
      "step": 820
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 2.028043508529663,
      "learning_rate": 1.8524444444444444e-05,
      "loss": 0.2415,
      "step": 830
    },
    {
      "epoch": 0.448,
      "grad_norm": 2.403047800064087,
      "learning_rate": 1.8506666666666667e-05,
      "loss": 0.2305,
      "step": 840
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 3.737492084503174,
      "learning_rate": 1.848888888888889e-05,
      "loss": 0.2183,
      "step": 850
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 1.7343499660491943,
      "learning_rate": 1.8471111111111112e-05,
      "loss": 0.2477,
      "step": 860
    },
    {
      "epoch": 0.464,
      "grad_norm": 2.6099650859832764,
      "learning_rate": 1.8453333333333335e-05,
      "loss": 0.2471,
      "step": 870
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 2.934739828109741,
      "learning_rate": 1.8435555555555558e-05,
      "loss": 0.2116,
      "step": 880
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 3.5620553493499756,
      "learning_rate": 1.8417777777777777e-05,
      "loss": 0.2487,
      "step": 890
    },
    {
      "epoch": 0.48,
      "grad_norm": 2.037273645401001,
      "learning_rate": 1.8400000000000003e-05,
      "loss": 0.2105,
      "step": 900
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 1.7175401449203491,
      "learning_rate": 1.8382222222222223e-05,
      "loss": 0.2401,
      "step": 910
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 3.304821729660034,
      "learning_rate": 1.8364444444444446e-05,
      "loss": 0.2362,
      "step": 920
    },
    {
      "epoch": 0.496,
      "grad_norm": 2.1050398349761963,
      "learning_rate": 1.834666666666667e-05,
      "loss": 0.2473,
      "step": 930
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 3.5820703506469727,
      "learning_rate": 1.832888888888889e-05,
      "loss": 0.2546,
      "step": 940
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 4.120608329772949,
      "learning_rate": 1.8311111111111114e-05,
      "loss": 0.2051,
      "step": 950
    },
    {
      "epoch": 0.512,
      "grad_norm": 2.4124093055725098,
      "learning_rate": 1.8293333333333333e-05,
      "loss": 0.213,
      "step": 960
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 3.3322222232818604,
      "learning_rate": 1.8275555555555556e-05,
      "loss": 0.2348,
      "step": 970
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 2.556062698364258,
      "learning_rate": 1.825777777777778e-05,
      "loss": 0.191,
      "step": 980
    },
    {
      "epoch": 0.528,
      "grad_norm": 3.3162920475006104,
      "learning_rate": 1.824e-05,
      "loss": 0.1954,
      "step": 990
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 3.3258445262908936,
      "learning_rate": 1.8222222222222224e-05,
      "loss": 0.2115,
      "step": 1000
    },
    {
      "epoch": 0.5386666666666666,
      "grad_norm": 2.0840632915496826,
      "learning_rate": 1.8204444444444447e-05,
      "loss": 0.2379,
      "step": 1010
    },
    {
      "epoch": 0.544,
      "grad_norm": 2.310572385787964,
      "learning_rate": 1.8186666666666666e-05,
      "loss": 0.1978,
      "step": 1020
    },
    {
      "epoch": 0.5493333333333333,
      "grad_norm": 3.293782949447632,
      "learning_rate": 1.8168888888888893e-05,
      "loss": 0.1668,
      "step": 1030
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 4.34323787689209,
      "learning_rate": 1.8151111111111112e-05,
      "loss": 0.211,
      "step": 1040
    },
    {
      "epoch": 0.56,
      "grad_norm": 3.9913852214813232,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 0.2473,
      "step": 1050
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 4.405982494354248,
      "learning_rate": 1.8115555555555558e-05,
      "loss": 0.2394,
      "step": 1060
    },
    {
      "epoch": 0.5706666666666667,
      "grad_norm": 2.1960771083831787,
      "learning_rate": 1.8097777777777777e-05,
      "loss": 0.2123,
      "step": 1070
    },
    {
      "epoch": 0.576,
      "grad_norm": 3.6924808025360107,
      "learning_rate": 1.8080000000000003e-05,
      "loss": 0.1912,
      "step": 1080
    },
    {
      "epoch": 0.5813333333333334,
      "grad_norm": 1.9034916162490845,
      "learning_rate": 1.8062222222222222e-05,
      "loss": 0.1843,
      "step": 1090
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 3.310030221939087,
      "learning_rate": 1.8044444444444445e-05,
      "loss": 0.2214,
      "step": 1100
    },
    {
      "epoch": 0.592,
      "grad_norm": 2.9158389568328857,
      "learning_rate": 1.8026666666666668e-05,
      "loss": 0.209,
      "step": 1110
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 2.7702107429504395,
      "learning_rate": 1.800888888888889e-05,
      "loss": 0.1999,
      "step": 1120
    },
    {
      "epoch": 0.6026666666666667,
      "grad_norm": 2.4558396339416504,
      "learning_rate": 1.7991111111111114e-05,
      "loss": 0.2504,
      "step": 1130
    },
    {
      "epoch": 0.608,
      "grad_norm": 1.7969404458999634,
      "learning_rate": 1.7973333333333333e-05,
      "loss": 0.2028,
      "step": 1140
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 3.390717029571533,
      "learning_rate": 1.7955555555555556e-05,
      "loss": 0.2005,
      "step": 1150
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 2.6017730236053467,
      "learning_rate": 1.793777777777778e-05,
      "loss": 0.2209,
      "step": 1160
    },
    {
      "epoch": 0.624,
      "grad_norm": 2.357858419418335,
      "learning_rate": 1.792e-05,
      "loss": 0.2234,
      "step": 1170
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 4.677999973297119,
      "learning_rate": 1.7902222222222224e-05,
      "loss": 0.2309,
      "step": 1180
    },
    {
      "epoch": 0.6346666666666667,
      "grad_norm": 1.8672105073928833,
      "learning_rate": 1.7884444444444447e-05,
      "loss": 0.197,
      "step": 1190
    },
    {
      "epoch": 0.64,
      "grad_norm": 3.6874146461486816,
      "learning_rate": 1.7866666666666666e-05,
      "loss": 0.1959,
      "step": 1200
    },
    {
      "epoch": 0.6453333333333333,
      "grad_norm": 4.376802921295166,
      "learning_rate": 1.7848888888888892e-05,
      "loss": 0.2181,
      "step": 1210
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 4.053673267364502,
      "learning_rate": 1.783111111111111e-05,
      "loss": 0.216,
      "step": 1220
    },
    {
      "epoch": 0.656,
      "grad_norm": 4.080118656158447,
      "learning_rate": 1.7813333333333334e-05,
      "loss": 0.2424,
      "step": 1230
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 1.7412792444229126,
      "learning_rate": 1.7795555555555557e-05,
      "loss": 0.1821,
      "step": 1240
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 3.284057140350342,
      "learning_rate": 1.7777777777777777e-05,
      "loss": 0.2298,
      "step": 1250
    },
    {
      "epoch": 0.672,
      "grad_norm": 3.2456586360931396,
      "learning_rate": 1.7760000000000003e-05,
      "loss": 0.1813,
      "step": 1260
    },
    {
      "epoch": 0.6773333333333333,
      "grad_norm": 2.6079695224761963,
      "learning_rate": 1.7742222222222222e-05,
      "loss": 0.2,
      "step": 1270
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 3.006305456161499,
      "learning_rate": 1.7724444444444445e-05,
      "loss": 0.1972,
      "step": 1280
    },
    {
      "epoch": 0.688,
      "grad_norm": 2.8665385246276855,
      "learning_rate": 1.7706666666666668e-05,
      "loss": 0.1762,
      "step": 1290
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 3.0240628719329834,
      "learning_rate": 1.768888888888889e-05,
      "loss": 0.2271,
      "step": 1300
    },
    {
      "epoch": 0.6986666666666667,
      "grad_norm": 3.154467821121216,
      "learning_rate": 1.7671111111111113e-05,
      "loss": 0.1822,
      "step": 1310
    },
    {
      "epoch": 0.704,
      "grad_norm": 2.85445499420166,
      "learning_rate": 1.7653333333333336e-05,
      "loss": 0.2175,
      "step": 1320
    },
    {
      "epoch": 0.7093333333333334,
      "grad_norm": 1.9365614652633667,
      "learning_rate": 1.7635555555555555e-05,
      "loss": 0.1735,
      "step": 1330
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 1.8405073881149292,
      "learning_rate": 1.761777777777778e-05,
      "loss": 0.2431,
      "step": 1340
    },
    {
      "epoch": 0.72,
      "grad_norm": 3.301166534423828,
      "learning_rate": 1.76e-05,
      "loss": 0.1927,
      "step": 1350
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 2.8007845878601074,
      "learning_rate": 1.7582222222222224e-05,
      "loss": 0.2075,
      "step": 1360
    },
    {
      "epoch": 0.7306666666666667,
      "grad_norm": 2.0972020626068115,
      "learning_rate": 1.7564444444444446e-05,
      "loss": 0.2142,
      "step": 1370
    },
    {
      "epoch": 0.736,
      "grad_norm": 2.102510690689087,
      "learning_rate": 1.7546666666666666e-05,
      "loss": 0.2069,
      "step": 1380
    },
    {
      "epoch": 0.7413333333333333,
      "grad_norm": 2.2479143142700195,
      "learning_rate": 1.7528888888888892e-05,
      "loss": 0.1962,
      "step": 1390
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 2.675490379333496,
      "learning_rate": 1.751111111111111e-05,
      "loss": 0.2127,
      "step": 1400
    },
    {
      "epoch": 0.752,
      "grad_norm": 2.9734835624694824,
      "learning_rate": 1.7493333333333334e-05,
      "loss": 0.1647,
      "step": 1410
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 3.5934033393859863,
      "learning_rate": 1.7475555555555557e-05,
      "loss": 0.2008,
      "step": 1420
    },
    {
      "epoch": 0.7626666666666667,
      "grad_norm": 2.324141025543213,
      "learning_rate": 1.745777777777778e-05,
      "loss": 0.2008,
      "step": 1430
    },
    {
      "epoch": 0.768,
      "grad_norm": 2.764238119125366,
      "learning_rate": 1.7440000000000002e-05,
      "loss": 0.1899,
      "step": 1440
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 2.915379524230957,
      "learning_rate": 1.7422222222222222e-05,
      "loss": 0.1946,
      "step": 1450
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 3.5540740489959717,
      "learning_rate": 1.7404444444444445e-05,
      "loss": 0.2025,
      "step": 1460
    },
    {
      "epoch": 0.784,
      "grad_norm": 2.3161509037017822,
      "learning_rate": 1.7386666666666667e-05,
      "loss": 0.1835,
      "step": 1470
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 1.988091230392456,
      "learning_rate": 1.736888888888889e-05,
      "loss": 0.1699,
      "step": 1480
    },
    {
      "epoch": 0.7946666666666666,
      "grad_norm": 5.1438374519348145,
      "learning_rate": 1.7351111111111113e-05,
      "loss": 0.2117,
      "step": 1490
    },
    {
      "epoch": 0.8,
      "grad_norm": 4.64500617980957,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.2147,
      "step": 1500
    },
    {
      "epoch": 0.8053333333333333,
      "grad_norm": 3.162074327468872,
      "learning_rate": 1.7315555555555555e-05,
      "loss": 0.2074,
      "step": 1510
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 3.123638153076172,
      "learning_rate": 1.729777777777778e-05,
      "loss": 0.222,
      "step": 1520
    },
    {
      "epoch": 0.816,
      "grad_norm": 3.1180825233459473,
      "learning_rate": 1.728e-05,
      "loss": 0.1994,
      "step": 1530
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 2.9998488426208496,
      "learning_rate": 1.7262222222222223e-05,
      "loss": 0.2055,
      "step": 1540
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 3.0389208793640137,
      "learning_rate": 1.7244444444444446e-05,
      "loss": 0.2007,
      "step": 1550
    },
    {
      "epoch": 0.832,
      "grad_norm": 2.6446969509124756,
      "learning_rate": 1.7226666666666665e-05,
      "loss": 0.2006,
      "step": 1560
    },
    {
      "epoch": 0.8373333333333334,
      "grad_norm": 3.8680787086486816,
      "learning_rate": 1.720888888888889e-05,
      "loss": 0.1862,
      "step": 1570
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 3.1376025676727295,
      "learning_rate": 1.719111111111111e-05,
      "loss": 0.1932,
      "step": 1580
    },
    {
      "epoch": 0.848,
      "grad_norm": 3.2271485328674316,
      "learning_rate": 1.7173333333333334e-05,
      "loss": 0.1996,
      "step": 1590
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 2.6277849674224854,
      "learning_rate": 1.7155555555555557e-05,
      "loss": 0.1438,
      "step": 1600
    },
    {
      "epoch": 0.8586666666666667,
      "grad_norm": 4.179690837860107,
      "learning_rate": 1.713777777777778e-05,
      "loss": 0.1793,
      "step": 1610
    },
    {
      "epoch": 0.864,
      "grad_norm": 2.0434560775756836,
      "learning_rate": 1.7120000000000002e-05,
      "loss": 0.1621,
      "step": 1620
    },
    {
      "epoch": 0.8693333333333333,
      "grad_norm": 3.5542125701904297,
      "learning_rate": 1.7102222222222225e-05,
      "loss": 0.1815,
      "step": 1630
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 2.0336673259735107,
      "learning_rate": 1.7084444444444444e-05,
      "loss": 0.1871,
      "step": 1640
    },
    {
      "epoch": 0.88,
      "grad_norm": 2.940936803817749,
      "learning_rate": 1.706666666666667e-05,
      "loss": 0.1845,
      "step": 1650
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 2.652810573577881,
      "learning_rate": 1.704888888888889e-05,
      "loss": 0.2206,
      "step": 1660
    },
    {
      "epoch": 0.8906666666666667,
      "grad_norm": 2.7457945346832275,
      "learning_rate": 1.7031111111111113e-05,
      "loss": 0.1743,
      "step": 1670
    },
    {
      "epoch": 0.896,
      "grad_norm": 3.310251474380493,
      "learning_rate": 1.7013333333333335e-05,
      "loss": 0.2144,
      "step": 1680
    },
    {
      "epoch": 0.9013333333333333,
      "grad_norm": 1.6278772354125977,
      "learning_rate": 1.6995555555555555e-05,
      "loss": 0.1921,
      "step": 1690
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 2.5803816318511963,
      "learning_rate": 1.697777777777778e-05,
      "loss": 0.1905,
      "step": 1700
    },
    {
      "epoch": 0.912,
      "grad_norm": 2.628675937652588,
      "learning_rate": 1.696e-05,
      "loss": 0.1888,
      "step": 1710
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 2.649833917617798,
      "learning_rate": 1.6942222222222223e-05,
      "loss": 0.1945,
      "step": 1720
    },
    {
      "epoch": 0.9226666666666666,
      "grad_norm": 3.0059049129486084,
      "learning_rate": 1.6924444444444446e-05,
      "loss": 0.1921,
      "step": 1730
    },
    {
      "epoch": 0.928,
      "grad_norm": 2.3337936401367188,
      "learning_rate": 1.690666666666667e-05,
      "loss": 0.1673,
      "step": 1740
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 3.3886091709136963,
      "learning_rate": 1.688888888888889e-05,
      "loss": 0.1799,
      "step": 1750
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 3.1414694786071777,
      "learning_rate": 1.687111111111111e-05,
      "loss": 0.1949,
      "step": 1760
    },
    {
      "epoch": 0.944,
      "grad_norm": 3.861191987991333,
      "learning_rate": 1.6853333333333333e-05,
      "loss": 0.2004,
      "step": 1770
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 3.5039045810699463,
      "learning_rate": 1.6835555555555556e-05,
      "loss": 0.1855,
      "step": 1780
    },
    {
      "epoch": 0.9546666666666667,
      "grad_norm": 2.468499183654785,
      "learning_rate": 1.681777777777778e-05,
      "loss": 0.1543,
      "step": 1790
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.7443112134933472,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.1443,
      "step": 1800
    },
    {
      "epoch": 0.9653333333333334,
      "grad_norm": 2.543004274368286,
      "learning_rate": 1.6782222222222225e-05,
      "loss": 0.2252,
      "step": 1810
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 2.444000482559204,
      "learning_rate": 1.6764444444444444e-05,
      "loss": 0.1437,
      "step": 1820
    },
    {
      "epoch": 0.976,
      "grad_norm": 2.1584396362304688,
      "learning_rate": 1.674666666666667e-05,
      "loss": 0.1633,
      "step": 1830
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 3.2354915142059326,
      "learning_rate": 1.672888888888889e-05,
      "loss": 0.1837,
      "step": 1840
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 1.441145420074463,
      "learning_rate": 1.6711111111111112e-05,
      "loss": 0.1456,
      "step": 1850
    },
    {
      "epoch": 0.992,
      "grad_norm": 1.8785091638565063,
      "learning_rate": 1.6693333333333335e-05,
      "loss": 0.1388,
      "step": 1860
    },
    {
      "epoch": 0.9973333333333333,
      "grad_norm": 2.640275478363037,
      "learning_rate": 1.6675555555555554e-05,
      "loss": 0.1874,
      "step": 1870
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9522,
      "eval_loss": 0.16177479922771454,
      "eval_runtime": 47.3293,
      "eval_samples_per_second": 422.571,
      "eval_steps_per_second": 4.416,
      "step": 1875
    },
    {
      "epoch": 1.0026666666666666,
      "grad_norm": 2.7024121284484863,
      "learning_rate": 1.665777777777778e-05,
      "loss": 0.1813,
      "step": 1880
    },
    {
      "epoch": 1.008,
      "grad_norm": 2.4247000217437744,
      "learning_rate": 1.664e-05,
      "loss": 0.1849,
      "step": 1890
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 2.5334632396698,
      "learning_rate": 1.6622222222222223e-05,
      "loss": 0.1499,
      "step": 1900
    },
    {
      "epoch": 1.0186666666666666,
      "grad_norm": 2.380298137664795,
      "learning_rate": 1.6604444444444445e-05,
      "loss": 0.1636,
      "step": 1910
    },
    {
      "epoch": 1.024,
      "grad_norm": 3.6392579078674316,
      "learning_rate": 1.6586666666666668e-05,
      "loss": 0.1699,
      "step": 1920
    },
    {
      "epoch": 1.0293333333333334,
      "grad_norm": 3.3060684204101562,
      "learning_rate": 1.656888888888889e-05,
      "loss": 0.1763,
      "step": 1930
    },
    {
      "epoch": 1.0346666666666666,
      "grad_norm": 2.6601383686065674,
      "learning_rate": 1.6551111111111114e-05,
      "loss": 0.169,
      "step": 1940
    },
    {
      "epoch": 1.04,
      "grad_norm": 3.023397207260132,
      "learning_rate": 1.6533333333333333e-05,
      "loss": 0.1955,
      "step": 1950
    },
    {
      "epoch": 1.0453333333333332,
      "grad_norm": 2.5607194900512695,
      "learning_rate": 1.651555555555556e-05,
      "loss": 0.1433,
      "step": 1960
    },
    {
      "epoch": 1.0506666666666666,
      "grad_norm": 3.2736470699310303,
      "learning_rate": 1.649777777777778e-05,
      "loss": 0.1447,
      "step": 1970
    },
    {
      "epoch": 1.056,
      "grad_norm": 4.233142852783203,
      "learning_rate": 1.648e-05,
      "loss": 0.1547,
      "step": 1980
    },
    {
      "epoch": 1.0613333333333332,
      "grad_norm": 2.2918291091918945,
      "learning_rate": 1.6462222222222224e-05,
      "loss": 0.1457,
      "step": 1990
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 4.092354774475098,
      "learning_rate": 1.6444444444444444e-05,
      "loss": 0.194,
      "step": 2000
    },
    {
      "epoch": 1.072,
      "grad_norm": 2.0499868392944336,
      "learning_rate": 1.642666666666667e-05,
      "loss": 0.1691,
      "step": 2010
    },
    {
      "epoch": 1.0773333333333333,
      "grad_norm": 3.1880156993865967,
      "learning_rate": 1.640888888888889e-05,
      "loss": 0.1564,
      "step": 2020
    },
    {
      "epoch": 1.0826666666666667,
      "grad_norm": 2.7348995208740234,
      "learning_rate": 1.6391111111111112e-05,
      "loss": 0.1736,
      "step": 2030
    },
    {
      "epoch": 1.088,
      "grad_norm": 3.316302537918091,
      "learning_rate": 1.6373333333333335e-05,
      "loss": 0.1381,
      "step": 2040
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 1.785122275352478,
      "learning_rate": 1.6355555555555557e-05,
      "loss": 0.1945,
      "step": 2050
    },
    {
      "epoch": 1.0986666666666667,
      "grad_norm": 3.1424431800842285,
      "learning_rate": 1.633777777777778e-05,
      "loss": 0.1614,
      "step": 2060
    },
    {
      "epoch": 1.104,
      "grad_norm": 2.629422187805176,
      "learning_rate": 1.632e-05,
      "loss": 0.1648,
      "step": 2070
    },
    {
      "epoch": 1.1093333333333333,
      "grad_norm": 3.1990158557891846,
      "learning_rate": 1.6302222222222222e-05,
      "loss": 0.1631,
      "step": 2080
    },
    {
      "epoch": 1.1146666666666667,
      "grad_norm": 2.902897596359253,
      "learning_rate": 1.6284444444444445e-05,
      "loss": 0.1496,
      "step": 2090
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.5730047225952148,
      "learning_rate": 1.6266666666666668e-05,
      "loss": 0.137,
      "step": 2100
    },
    {
      "epoch": 1.1253333333333333,
      "grad_norm": 2.105755567550659,
      "learning_rate": 1.624888888888889e-05,
      "loss": 0.1209,
      "step": 2110
    },
    {
      "epoch": 1.1306666666666667,
      "grad_norm": 3.0864460468292236,
      "learning_rate": 1.6231111111111113e-05,
      "loss": 0.152,
      "step": 2120
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 4.047958850860596,
      "learning_rate": 1.6213333333333333e-05,
      "loss": 0.1543,
      "step": 2130
    },
    {
      "epoch": 1.1413333333333333,
      "grad_norm": 3.352796792984009,
      "learning_rate": 1.619555555555556e-05,
      "loss": 0.2019,
      "step": 2140
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 2.140758991241455,
      "learning_rate": 1.617777777777778e-05,
      "loss": 0.1372,
      "step": 2150
    },
    {
      "epoch": 1.152,
      "grad_norm": 3.941197633743286,
      "learning_rate": 1.616e-05,
      "loss": 0.1749,
      "step": 2160
    },
    {
      "epoch": 1.1573333333333333,
      "grad_norm": 4.9209465980529785,
      "learning_rate": 1.6142222222222224e-05,
      "loss": 0.1715,
      "step": 2170
    },
    {
      "epoch": 1.1626666666666667,
      "grad_norm": 2.121079683303833,
      "learning_rate": 1.6124444444444443e-05,
      "loss": 0.1538,
      "step": 2180
    },
    {
      "epoch": 1.168,
      "grad_norm": 1.96491277217865,
      "learning_rate": 1.610666666666667e-05,
      "loss": 0.1481,
      "step": 2190
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 2.936753749847412,
      "learning_rate": 1.608888888888889e-05,
      "loss": 0.1693,
      "step": 2200
    },
    {
      "epoch": 1.1786666666666668,
      "grad_norm": 2.514097213745117,
      "learning_rate": 1.607111111111111e-05,
      "loss": 0.1613,
      "step": 2210
    },
    {
      "epoch": 1.184,
      "grad_norm": 3.268462896347046,
      "learning_rate": 1.6053333333333334e-05,
      "loss": 0.1507,
      "step": 2220
    },
    {
      "epoch": 1.1893333333333334,
      "grad_norm": 3.6877002716064453,
      "learning_rate": 1.6035555555555557e-05,
      "loss": 0.159,
      "step": 2230
    },
    {
      "epoch": 1.1946666666666665,
      "grad_norm": 2.2801175117492676,
      "learning_rate": 1.601777777777778e-05,
      "loss": 0.1366,
      "step": 2240
    },
    {
      "epoch": 1.2,
      "grad_norm": 2.6054985523223877,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.1645,
      "step": 2250
    },
    {
      "epoch": 1.2053333333333334,
      "grad_norm": 2.885674476623535,
      "learning_rate": 1.5982222222222222e-05,
      "loss": 0.1477,
      "step": 2260
    },
    {
      "epoch": 1.2106666666666666,
      "grad_norm": 3.5102169513702393,
      "learning_rate": 1.5964444444444448e-05,
      "loss": 0.1572,
      "step": 2270
    },
    {
      "epoch": 1.216,
      "grad_norm": 3.006258010864258,
      "learning_rate": 1.5946666666666668e-05,
      "loss": 0.1531,
      "step": 2280
    },
    {
      "epoch": 1.2213333333333334,
      "grad_norm": 2.3336026668548584,
      "learning_rate": 1.592888888888889e-05,
      "loss": 0.1536,
      "step": 2290
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 2.606433868408203,
      "learning_rate": 1.5911111111111113e-05,
      "loss": 0.1584,
      "step": 2300
    },
    {
      "epoch": 1.232,
      "grad_norm": 3.569857358932495,
      "learning_rate": 1.5893333333333333e-05,
      "loss": 0.1588,
      "step": 2310
    },
    {
      "epoch": 1.2373333333333334,
      "grad_norm": 2.543311357498169,
      "learning_rate": 1.587555555555556e-05,
      "loss": 0.1651,
      "step": 2320
    },
    {
      "epoch": 1.2426666666666666,
      "grad_norm": 2.3621773719787598,
      "learning_rate": 1.5857777777777778e-05,
      "loss": 0.1077,
      "step": 2330
    },
    {
      "epoch": 1.248,
      "grad_norm": 2.9883246421813965,
      "learning_rate": 1.584e-05,
      "loss": 0.1606,
      "step": 2340
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 2.522775888442993,
      "learning_rate": 1.5822222222222224e-05,
      "loss": 0.1538,
      "step": 2350
    },
    {
      "epoch": 1.2586666666666666,
      "grad_norm": 2.3827109336853027,
      "learning_rate": 1.5804444444444446e-05,
      "loss": 0.1698,
      "step": 2360
    },
    {
      "epoch": 1.264,
      "grad_norm": 1.211219072341919,
      "learning_rate": 1.578666666666667e-05,
      "loss": 0.1583,
      "step": 2370
    },
    {
      "epoch": 1.2693333333333334,
      "grad_norm": 2.322481870651245,
      "learning_rate": 1.576888888888889e-05,
      "loss": 0.1549,
      "step": 2380
    },
    {
      "epoch": 1.2746666666666666,
      "grad_norm": 2.954284191131592,
      "learning_rate": 1.575111111111111e-05,
      "loss": 0.1279,
      "step": 2390
    },
    {
      "epoch": 1.28,
      "grad_norm": 3.124236822128296,
      "learning_rate": 1.5733333333333334e-05,
      "loss": 0.1941,
      "step": 2400
    },
    {
      "epoch": 1.2853333333333334,
      "grad_norm": 2.8574633598327637,
      "learning_rate": 1.5715555555555557e-05,
      "loss": 0.1288,
      "step": 2410
    },
    {
      "epoch": 1.2906666666666666,
      "grad_norm": 1.7889984846115112,
      "learning_rate": 1.569777777777778e-05,
      "loss": 0.1163,
      "step": 2420
    },
    {
      "epoch": 1.296,
      "grad_norm": 3.1381752490997314,
      "learning_rate": 1.5680000000000002e-05,
      "loss": 0.1509,
      "step": 2430
    },
    {
      "epoch": 1.3013333333333335,
      "grad_norm": 3.0741732120513916,
      "learning_rate": 1.5662222222222222e-05,
      "loss": 0.1591,
      "step": 2440
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 1.6643571853637695,
      "learning_rate": 1.5644444444444448e-05,
      "loss": 0.1556,
      "step": 2450
    },
    {
      "epoch": 1.312,
      "grad_norm": 2.39359450340271,
      "learning_rate": 1.5626666666666667e-05,
      "loss": 0.1553,
      "step": 2460
    },
    {
      "epoch": 1.3173333333333335,
      "grad_norm": 1.8114328384399414,
      "learning_rate": 1.560888888888889e-05,
      "loss": 0.1434,
      "step": 2470
    },
    {
      "epoch": 1.3226666666666667,
      "grad_norm": 3.360189437866211,
      "learning_rate": 1.5591111111111113e-05,
      "loss": 0.1555,
      "step": 2480
    },
    {
      "epoch": 1.328,
      "grad_norm": 1.8738735914230347,
      "learning_rate": 1.5573333333333332e-05,
      "loss": 0.122,
      "step": 2490
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 1.3377200365066528,
      "learning_rate": 1.555555555555556e-05,
      "loss": 0.1556,
      "step": 2500
    },
    {
      "epoch": 1.3386666666666667,
      "grad_norm": 3.849900484085083,
      "learning_rate": 1.5537777777777778e-05,
      "loss": 0.1809,
      "step": 2510
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 3.0850749015808105,
      "learning_rate": 1.552e-05,
      "loss": 0.1652,
      "step": 2520
    },
    {
      "epoch": 1.3493333333333333,
      "grad_norm": 2.0143954753875732,
      "learning_rate": 1.5502222222222223e-05,
      "loss": 0.1429,
      "step": 2530
    },
    {
      "epoch": 1.3546666666666667,
      "grad_norm": 2.536146640777588,
      "learning_rate": 1.5484444444444446e-05,
      "loss": 0.1652,
      "step": 2540
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 3.6218037605285645,
      "learning_rate": 1.546666666666667e-05,
      "loss": 0.173,
      "step": 2550
    },
    {
      "epoch": 1.3653333333333333,
      "grad_norm": 1.2426178455352783,
      "learning_rate": 1.544888888888889e-05,
      "loss": 0.1506,
      "step": 2560
    },
    {
      "epoch": 1.3706666666666667,
      "grad_norm": 3.584998846054077,
      "learning_rate": 1.543111111111111e-05,
      "loss": 0.1652,
      "step": 2570
    },
    {
      "epoch": 1.376,
      "grad_norm": 3.6857821941375732,
      "learning_rate": 1.5413333333333337e-05,
      "loss": 0.1372,
      "step": 2580
    },
    {
      "epoch": 1.3813333333333333,
      "grad_norm": 2.637937307357788,
      "learning_rate": 1.5395555555555556e-05,
      "loss": 0.131,
      "step": 2590
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 3.107482671737671,
      "learning_rate": 1.537777777777778e-05,
      "loss": 0.1412,
      "step": 2600
    },
    {
      "epoch": 1.392,
      "grad_norm": 3.5517091751098633,
      "learning_rate": 1.5360000000000002e-05,
      "loss": 0.149,
      "step": 2610
    },
    {
      "epoch": 1.3973333333333333,
      "grad_norm": 1.8662786483764648,
      "learning_rate": 1.534222222222222e-05,
      "loss": 0.1361,
      "step": 2620
    },
    {
      "epoch": 1.4026666666666667,
      "grad_norm": 2.5949339866638184,
      "learning_rate": 1.5324444444444448e-05,
      "loss": 0.1465,
      "step": 2630
    },
    {
      "epoch": 1.408,
      "grad_norm": 3.543020248413086,
      "learning_rate": 1.5306666666666667e-05,
      "loss": 0.1919,
      "step": 2640
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 2.0948755741119385,
      "learning_rate": 1.528888888888889e-05,
      "loss": 0.1284,
      "step": 2650
    },
    {
      "epoch": 1.4186666666666667,
      "grad_norm": 2.4864001274108887,
      "learning_rate": 1.5271111111111112e-05,
      "loss": 0.1156,
      "step": 2660
    },
    {
      "epoch": 1.424,
      "grad_norm": 2.35933780670166,
      "learning_rate": 1.5253333333333335e-05,
      "loss": 0.1125,
      "step": 2670
    },
    {
      "epoch": 1.4293333333333333,
      "grad_norm": 2.679353713989258,
      "learning_rate": 1.5235555555555556e-05,
      "loss": 0.1664,
      "step": 2680
    },
    {
      "epoch": 1.4346666666666668,
      "grad_norm": 2.9537599086761475,
      "learning_rate": 1.5217777777777777e-05,
      "loss": 0.1662,
      "step": 2690
    },
    {
      "epoch": 1.44,
      "grad_norm": 2.42971134185791,
      "learning_rate": 1.5200000000000002e-05,
      "loss": 0.1304,
      "step": 2700
    },
    {
      "epoch": 1.4453333333333334,
      "grad_norm": 3.495495557785034,
      "learning_rate": 1.5182222222222223e-05,
      "loss": 0.1671,
      "step": 2710
    },
    {
      "epoch": 1.4506666666666668,
      "grad_norm": 4.07634162902832,
      "learning_rate": 1.5164444444444446e-05,
      "loss": 0.174,
      "step": 2720
    },
    {
      "epoch": 1.456,
      "grad_norm": 2.547858476638794,
      "learning_rate": 1.5146666666666667e-05,
      "loss": 0.1241,
      "step": 2730
    },
    {
      "epoch": 1.4613333333333334,
      "grad_norm": 3.1569254398345947,
      "learning_rate": 1.5128888888888891e-05,
      "loss": 0.1659,
      "step": 2740
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 2.1295053958892822,
      "learning_rate": 1.5111111111111112e-05,
      "loss": 0.1622,
      "step": 2750
    },
    {
      "epoch": 1.472,
      "grad_norm": 2.7306978702545166,
      "learning_rate": 1.5093333333333335e-05,
      "loss": 0.1424,
      "step": 2760
    },
    {
      "epoch": 1.4773333333333334,
      "grad_norm": 2.9341702461242676,
      "learning_rate": 1.5075555555555556e-05,
      "loss": 0.1331,
      "step": 2770
    },
    {
      "epoch": 1.4826666666666668,
      "grad_norm": 2.4492101669311523,
      "learning_rate": 1.505777777777778e-05,
      "loss": 0.2122,
      "step": 2780
    },
    {
      "epoch": 1.488,
      "grad_norm": 2.4894068241119385,
      "learning_rate": 1.5040000000000002e-05,
      "loss": 0.162,
      "step": 2790
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 2.583138942718506,
      "learning_rate": 1.5022222222222223e-05,
      "loss": 0.1399,
      "step": 2800
    },
    {
      "epoch": 1.4986666666666666,
      "grad_norm": 2.5380916595458984,
      "learning_rate": 1.5004444444444446e-05,
      "loss": 0.1428,
      "step": 2810
    },
    {
      "epoch": 1.504,
      "grad_norm": 4.339022159576416,
      "learning_rate": 1.4986666666666667e-05,
      "loss": 0.1711,
      "step": 2820
    },
    {
      "epoch": 1.5093333333333332,
      "grad_norm": 3.205369472503662,
      "learning_rate": 1.4968888888888891e-05,
      "loss": 0.1741,
      "step": 2830
    },
    {
      "epoch": 1.5146666666666668,
      "grad_norm": 5.239588260650635,
      "learning_rate": 1.4951111111111112e-05,
      "loss": 0.1192,
      "step": 2840
    },
    {
      "epoch": 1.52,
      "grad_norm": 3.0619852542877197,
      "learning_rate": 1.4933333333333335e-05,
      "loss": 0.1537,
      "step": 2850
    },
    {
      "epoch": 1.5253333333333332,
      "grad_norm": 3.155547618865967,
      "learning_rate": 1.4915555555555556e-05,
      "loss": 0.1364,
      "step": 2860
    },
    {
      "epoch": 1.5306666666666666,
      "grad_norm": 2.759366989135742,
      "learning_rate": 1.489777777777778e-05,
      "loss": 0.1466,
      "step": 2870
    },
    {
      "epoch": 1.536,
      "grad_norm": 3.3737080097198486,
      "learning_rate": 1.4880000000000002e-05,
      "loss": 0.1235,
      "step": 2880
    },
    {
      "epoch": 1.5413333333333332,
      "grad_norm": 2.3901560306549072,
      "learning_rate": 1.4862222222222223e-05,
      "loss": 0.1577,
      "step": 2890
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 3.444118022918701,
      "learning_rate": 1.4844444444444445e-05,
      "loss": 0.1655,
      "step": 2900
    },
    {
      "epoch": 1.552,
      "grad_norm": 2.112642765045166,
      "learning_rate": 1.4826666666666666e-05,
      "loss": 0.1403,
      "step": 2910
    },
    {
      "epoch": 1.5573333333333332,
      "grad_norm": 1.4391063451766968,
      "learning_rate": 1.4808888888888891e-05,
      "loss": 0.1525,
      "step": 2920
    },
    {
      "epoch": 1.5626666666666666,
      "grad_norm": 2.5086915493011475,
      "learning_rate": 1.4791111111111112e-05,
      "loss": 0.1408,
      "step": 2930
    },
    {
      "epoch": 1.568,
      "grad_norm": 3.9064459800720215,
      "learning_rate": 1.4773333333333335e-05,
      "loss": 0.1567,
      "step": 2940
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 3.9377682209014893,
      "learning_rate": 1.4755555555555556e-05,
      "loss": 0.1487,
      "step": 2950
    },
    {
      "epoch": 1.5786666666666667,
      "grad_norm": 3.0561294555664062,
      "learning_rate": 1.473777777777778e-05,
      "loss": 0.1337,
      "step": 2960
    },
    {
      "epoch": 1.584,
      "grad_norm": 2.30009388923645,
      "learning_rate": 1.4720000000000001e-05,
      "loss": 0.1409,
      "step": 2970
    },
    {
      "epoch": 1.5893333333333333,
      "grad_norm": 2.848390817642212,
      "learning_rate": 1.4702222222222224e-05,
      "loss": 0.1531,
      "step": 2980
    },
    {
      "epoch": 1.5946666666666667,
      "grad_norm": 3.6621346473693848,
      "learning_rate": 1.4684444444444445e-05,
      "loss": 0.1546,
      "step": 2990
    },
    {
      "epoch": 1.6,
      "grad_norm": 3.226128101348877,
      "learning_rate": 1.4666666666666666e-05,
      "loss": 0.1261,
      "step": 3000
    },
    {
      "epoch": 1.6053333333333333,
      "grad_norm": 2.0554580688476562,
      "learning_rate": 1.464888888888889e-05,
      "loss": 0.1823,
      "step": 3010
    },
    {
      "epoch": 1.6106666666666667,
      "grad_norm": 2.969326972961426,
      "learning_rate": 1.4631111111111112e-05,
      "loss": 0.1735,
      "step": 3020
    },
    {
      "epoch": 1.616,
      "grad_norm": 4.0659003257751465,
      "learning_rate": 1.4613333333333335e-05,
      "loss": 0.1453,
      "step": 3030
    },
    {
      "epoch": 1.6213333333333333,
      "grad_norm": 3.3267996311187744,
      "learning_rate": 1.4595555555555556e-05,
      "loss": 0.13,
      "step": 3040
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 3.3597230911254883,
      "learning_rate": 1.457777777777778e-05,
      "loss": 0.1524,
      "step": 3050
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 1.8789424896240234,
      "learning_rate": 1.4560000000000001e-05,
      "loss": 0.1555,
      "step": 3060
    },
    {
      "epoch": 1.6373333333333333,
      "grad_norm": 2.771972894668579,
      "learning_rate": 1.4542222222222224e-05,
      "loss": 0.1578,
      "step": 3070
    },
    {
      "epoch": 1.6426666666666667,
      "grad_norm": 3.734224557876587,
      "learning_rate": 1.4524444444444445e-05,
      "loss": 0.1489,
      "step": 3080
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 2.193277597427368,
      "learning_rate": 1.450666666666667e-05,
      "loss": 0.142,
      "step": 3090
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 2.495779275894165,
      "learning_rate": 1.448888888888889e-05,
      "loss": 0.1705,
      "step": 3100
    },
    {
      "epoch": 1.6586666666666665,
      "grad_norm": 3.3288865089416504,
      "learning_rate": 1.4471111111111112e-05,
      "loss": 0.145,
      "step": 3110
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 3.1698567867279053,
      "learning_rate": 1.4453333333333334e-05,
      "loss": 0.1321,
      "step": 3120
    },
    {
      "epoch": 1.6693333333333333,
      "grad_norm": 1.9046146869659424,
      "learning_rate": 1.4435555555555556e-05,
      "loss": 0.1314,
      "step": 3130
    },
    {
      "epoch": 1.6746666666666665,
      "grad_norm": 3.092376708984375,
      "learning_rate": 1.441777777777778e-05,
      "loss": 0.1596,
      "step": 3140
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 3.0067150592803955,
      "learning_rate": 1.4400000000000001e-05,
      "loss": 0.1178,
      "step": 3150
    },
    {
      "epoch": 1.6853333333333333,
      "grad_norm": 2.1397793292999268,
      "learning_rate": 1.4382222222222224e-05,
      "loss": 0.1728,
      "step": 3160
    },
    {
      "epoch": 1.6906666666666665,
      "grad_norm": 3.5217337608337402,
      "learning_rate": 1.4364444444444445e-05,
      "loss": 0.1555,
      "step": 3170
    },
    {
      "epoch": 1.696,
      "grad_norm": 2.6682181358337402,
      "learning_rate": 1.434666666666667e-05,
      "loss": 0.1177,
      "step": 3180
    },
    {
      "epoch": 1.7013333333333334,
      "grad_norm": 3.716740608215332,
      "learning_rate": 1.432888888888889e-05,
      "loss": 0.1427,
      "step": 3190
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.8339747190475464,
      "learning_rate": 1.4311111111111111e-05,
      "loss": 0.1604,
      "step": 3200
    },
    {
      "epoch": 1.712,
      "grad_norm": 2.177112579345703,
      "learning_rate": 1.4293333333333334e-05,
      "loss": 0.1296,
      "step": 3210
    },
    {
      "epoch": 1.7173333333333334,
      "grad_norm": 2.3886795043945312,
      "learning_rate": 1.4275555555555555e-05,
      "loss": 0.1308,
      "step": 3220
    },
    {
      "epoch": 1.7226666666666666,
      "grad_norm": 2.0668349266052246,
      "learning_rate": 1.425777777777778e-05,
      "loss": 0.1513,
      "step": 3230
    },
    {
      "epoch": 1.728,
      "grad_norm": 2.5753350257873535,
      "learning_rate": 1.4240000000000001e-05,
      "loss": 0.1571,
      "step": 3240
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 2.6335554122924805,
      "learning_rate": 1.4222222222222224e-05,
      "loss": 0.1253,
      "step": 3250
    },
    {
      "epoch": 1.7386666666666666,
      "grad_norm": 3.116837978363037,
      "learning_rate": 1.4204444444444445e-05,
      "loss": 0.1427,
      "step": 3260
    },
    {
      "epoch": 1.744,
      "grad_norm": 1.9039371013641357,
      "learning_rate": 1.418666666666667e-05,
      "loss": 0.131,
      "step": 3270
    },
    {
      "epoch": 1.7493333333333334,
      "grad_norm": 2.4038310050964355,
      "learning_rate": 1.416888888888889e-05,
      "loss": 0.1231,
      "step": 3280
    },
    {
      "epoch": 1.7546666666666666,
      "grad_norm": 1.5370198488235474,
      "learning_rate": 1.4151111111111113e-05,
      "loss": 0.128,
      "step": 3290
    },
    {
      "epoch": 1.76,
      "grad_norm": 3.254650115966797,
      "learning_rate": 1.4133333333333334e-05,
      "loss": 0.1447,
      "step": 3300
    },
    {
      "epoch": 1.7653333333333334,
      "grad_norm": 2.7196977138519287,
      "learning_rate": 1.4115555555555555e-05,
      "loss": 0.1497,
      "step": 3310
    },
    {
      "epoch": 1.7706666666666666,
      "grad_norm": 2.954242706298828,
      "learning_rate": 1.409777777777778e-05,
      "loss": 0.1406,
      "step": 3320
    },
    {
      "epoch": 1.776,
      "grad_norm": 2.363593339920044,
      "learning_rate": 1.408e-05,
      "loss": 0.1479,
      "step": 3330
    },
    {
      "epoch": 1.7813333333333334,
      "grad_norm": 3.330514430999756,
      "learning_rate": 1.4062222222222223e-05,
      "loss": 0.2173,
      "step": 3340
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 2.7564027309417725,
      "learning_rate": 1.4044444444444445e-05,
      "loss": 0.1586,
      "step": 3350
    },
    {
      "epoch": 1.792,
      "grad_norm": 2.0126872062683105,
      "learning_rate": 1.4026666666666669e-05,
      "loss": 0.1434,
      "step": 3360
    },
    {
      "epoch": 1.7973333333333334,
      "grad_norm": 3.062717914581299,
      "learning_rate": 1.400888888888889e-05,
      "loss": 0.1087,
      "step": 3370
    },
    {
      "epoch": 1.8026666666666666,
      "grad_norm": 2.6111974716186523,
      "learning_rate": 1.3991111111111113e-05,
      "loss": 0.1467,
      "step": 3380
    },
    {
      "epoch": 1.808,
      "grad_norm": 1.7258812189102173,
      "learning_rate": 1.3973333333333334e-05,
      "loss": 0.112,
      "step": 3390
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 4.1073408126831055,
      "learning_rate": 1.3955555555555558e-05,
      "loss": 0.1413,
      "step": 3400
    },
    {
      "epoch": 1.8186666666666667,
      "grad_norm": 2.5441410541534424,
      "learning_rate": 1.393777777777778e-05,
      "loss": 0.1526,
      "step": 3410
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 2.8192391395568848,
      "learning_rate": 1.392e-05,
      "loss": 0.1555,
      "step": 3420
    },
    {
      "epoch": 1.8293333333333335,
      "grad_norm": 3.050295114517212,
      "learning_rate": 1.3902222222222223e-05,
      "loss": 0.1739,
      "step": 3430
    },
    {
      "epoch": 1.8346666666666667,
      "grad_norm": 2.8018479347229004,
      "learning_rate": 1.3884444444444444e-05,
      "loss": 0.1544,
      "step": 3440
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 2.6846845149993896,
      "learning_rate": 1.3866666666666669e-05,
      "loss": 0.1308,
      "step": 3450
    },
    {
      "epoch": 1.8453333333333335,
      "grad_norm": 2.834657907485962,
      "learning_rate": 1.384888888888889e-05,
      "loss": 0.1901,
      "step": 3460
    },
    {
      "epoch": 1.8506666666666667,
      "grad_norm": 2.1398088932037354,
      "learning_rate": 1.3831111111111113e-05,
      "loss": 0.1346,
      "step": 3470
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 2.3643739223480225,
      "learning_rate": 1.3813333333333334e-05,
      "loss": 0.1353,
      "step": 3480
    },
    {
      "epoch": 1.8613333333333333,
      "grad_norm": 3.5314059257507324,
      "learning_rate": 1.3795555555555558e-05,
      "loss": 0.1548,
      "step": 3490
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 1.8087071180343628,
      "learning_rate": 1.377777777777778e-05,
      "loss": 0.1608,
      "step": 3500
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 3.140448808670044,
      "learning_rate": 1.376e-05,
      "loss": 0.1291,
      "step": 3510
    },
    {
      "epoch": 1.8773333333333333,
      "grad_norm": 2.849179267883301,
      "learning_rate": 1.3742222222222223e-05,
      "loss": 0.1433,
      "step": 3520
    },
    {
      "epoch": 1.8826666666666667,
      "grad_norm": 3.1561851501464844,
      "learning_rate": 1.3724444444444444e-05,
      "loss": 0.1501,
      "step": 3530
    },
    {
      "epoch": 1.888,
      "grad_norm": 2.0603373050689697,
      "learning_rate": 1.3706666666666669e-05,
      "loss": 0.1522,
      "step": 3540
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 2.522777795791626,
      "learning_rate": 1.368888888888889e-05,
      "loss": 0.1306,
      "step": 3550
    },
    {
      "epoch": 1.8986666666666667,
      "grad_norm": 3.3453359603881836,
      "learning_rate": 1.3671111111111113e-05,
      "loss": 0.1848,
      "step": 3560
    },
    {
      "epoch": 1.904,
      "grad_norm": 3.637115716934204,
      "learning_rate": 1.3653333333333334e-05,
      "loss": 0.1418,
      "step": 3570
    },
    {
      "epoch": 1.9093333333333333,
      "grad_norm": 3.3314170837402344,
      "learning_rate": 1.3635555555555558e-05,
      "loss": 0.1373,
      "step": 3580
    },
    {
      "epoch": 1.9146666666666667,
      "grad_norm": 3.3508756160736084,
      "learning_rate": 1.361777777777778e-05,
      "loss": 0.1441,
      "step": 3590
    },
    {
      "epoch": 1.92,
      "grad_norm": 4.079929351806641,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.1611,
      "step": 3600
    },
    {
      "epoch": 1.9253333333333333,
      "grad_norm": 3.081526279449463,
      "learning_rate": 1.3582222222222223e-05,
      "loss": 0.1744,
      "step": 3610
    },
    {
      "epoch": 1.9306666666666668,
      "grad_norm": 2.141205310821533,
      "learning_rate": 1.3564444444444444e-05,
      "loss": 0.1458,
      "step": 3620
    },
    {
      "epoch": 1.936,
      "grad_norm": 3.137617588043213,
      "learning_rate": 1.3546666666666669e-05,
      "loss": 0.1589,
      "step": 3630
    },
    {
      "epoch": 1.9413333333333334,
      "grad_norm": 3.0586655139923096,
      "learning_rate": 1.352888888888889e-05,
      "loss": 0.1418,
      "step": 3640
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 2.8604483604431152,
      "learning_rate": 1.3511111111111112e-05,
      "loss": 0.119,
      "step": 3650
    },
    {
      "epoch": 1.952,
      "grad_norm": 3.0054538249969482,
      "learning_rate": 1.3493333333333333e-05,
      "loss": 0.1637,
      "step": 3660
    },
    {
      "epoch": 1.9573333333333334,
      "grad_norm": 3.2180793285369873,
      "learning_rate": 1.3475555555555558e-05,
      "loss": 0.1227,
      "step": 3670
    },
    {
      "epoch": 1.9626666666666668,
      "grad_norm": 1.8601603507995605,
      "learning_rate": 1.3457777777777779e-05,
      "loss": 0.116,
      "step": 3680
    },
    {
      "epoch": 1.968,
      "grad_norm": 3.1639113426208496,
      "learning_rate": 1.3440000000000002e-05,
      "loss": 0.1326,
      "step": 3690
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 2.436915397644043,
      "learning_rate": 1.3422222222222223e-05,
      "loss": 0.1261,
      "step": 3700
    },
    {
      "epoch": 1.9786666666666668,
      "grad_norm": 3.9108121395111084,
      "learning_rate": 1.3404444444444447e-05,
      "loss": 0.1593,
      "step": 3710
    },
    {
      "epoch": 1.984,
      "grad_norm": 1.9920530319213867,
      "learning_rate": 1.3386666666666668e-05,
      "loss": 0.1516,
      "step": 3720
    },
    {
      "epoch": 1.9893333333333332,
      "grad_norm": 2.207017660140991,
      "learning_rate": 1.336888888888889e-05,
      "loss": 0.1238,
      "step": 3730
    },
    {
      "epoch": 1.9946666666666668,
      "grad_norm": 0.9930538535118103,
      "learning_rate": 1.3351111111111112e-05,
      "loss": 0.1274,
      "step": 3740
    },
    {
      "epoch": 2.0,
      "grad_norm": 3.0780577659606934,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.1309,
      "step": 3750
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.95755,
      "eval_loss": 0.13812845945358276,
      "eval_runtime": 47.4727,
      "eval_samples_per_second": 421.294,
      "eval_steps_per_second": 4.403,
      "step": 3750
    },
    {
      "epoch": 2.005333333333333,
      "grad_norm": 1.591410756111145,
      "learning_rate": 1.3315555555555558e-05,
      "loss": 0.112,
      "step": 3760
    },
    {
      "epoch": 2.010666666666667,
      "grad_norm": 4.177635192871094,
      "learning_rate": 1.3297777777777779e-05,
      "loss": 0.1327,
      "step": 3770
    },
    {
      "epoch": 2.016,
      "grad_norm": 2.5043370723724365,
      "learning_rate": 1.3280000000000002e-05,
      "loss": 0.1491,
      "step": 3780
    },
    {
      "epoch": 2.021333333333333,
      "grad_norm": 2.369479179382324,
      "learning_rate": 1.3262222222222223e-05,
      "loss": 0.1051,
      "step": 3790
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 2.7274272441864014,
      "learning_rate": 1.3244444444444447e-05,
      "loss": 0.1133,
      "step": 3800
    },
    {
      "epoch": 2.032,
      "grad_norm": 1.7516822814941406,
      "learning_rate": 1.3226666666666668e-05,
      "loss": 0.1042,
      "step": 3810
    },
    {
      "epoch": 2.037333333333333,
      "grad_norm": 2.0808331966400146,
      "learning_rate": 1.320888888888889e-05,
      "loss": 0.1685,
      "step": 3820
    },
    {
      "epoch": 2.042666666666667,
      "grad_norm": 1.956388235092163,
      "learning_rate": 1.3191111111111112e-05,
      "loss": 0.1323,
      "step": 3830
    },
    {
      "epoch": 2.048,
      "grad_norm": 2.953636407852173,
      "learning_rate": 1.3173333333333333e-05,
      "loss": 0.1317,
      "step": 3840
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 1.3904796838760376,
      "learning_rate": 1.3155555555555558e-05,
      "loss": 0.0967,
      "step": 3850
    },
    {
      "epoch": 2.058666666666667,
      "grad_norm": 1.837289571762085,
      "learning_rate": 1.3137777777777779e-05,
      "loss": 0.1201,
      "step": 3860
    },
    {
      "epoch": 2.064,
      "grad_norm": 2.0440568923950195,
      "learning_rate": 1.3120000000000001e-05,
      "loss": 0.1593,
      "step": 3870
    },
    {
      "epoch": 2.0693333333333332,
      "grad_norm": 2.9943389892578125,
      "learning_rate": 1.3102222222222223e-05,
      "loss": 0.1015,
      "step": 3880
    },
    {
      "epoch": 2.074666666666667,
      "grad_norm": 3.380565643310547,
      "learning_rate": 1.3084444444444447e-05,
      "loss": 0.1147,
      "step": 3890
    },
    {
      "epoch": 2.08,
      "grad_norm": 3.232732057571411,
      "learning_rate": 1.3066666666666668e-05,
      "loss": 0.1127,
      "step": 3900
    },
    {
      "epoch": 2.0853333333333333,
      "grad_norm": 1.892600655555725,
      "learning_rate": 1.304888888888889e-05,
      "loss": 0.104,
      "step": 3910
    },
    {
      "epoch": 2.0906666666666665,
      "grad_norm": 3.0456221103668213,
      "learning_rate": 1.3031111111111112e-05,
      "loss": 0.1354,
      "step": 3920
    },
    {
      "epoch": 2.096,
      "grad_norm": 2.0736966133117676,
      "learning_rate": 1.3013333333333333e-05,
      "loss": 0.1041,
      "step": 3930
    },
    {
      "epoch": 2.1013333333333333,
      "grad_norm": 2.983041286468506,
      "learning_rate": 1.2995555555555557e-05,
      "loss": 0.1421,
      "step": 3940
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 1.996295690536499,
      "learning_rate": 1.2977777777777779e-05,
      "loss": 0.1322,
      "step": 3950
    },
    {
      "epoch": 2.112,
      "grad_norm": 2.0842361450195312,
      "learning_rate": 1.2960000000000001e-05,
      "loss": 0.1373,
      "step": 3960
    },
    {
      "epoch": 2.1173333333333333,
      "grad_norm": 3.120310068130493,
      "learning_rate": 1.2942222222222222e-05,
      "loss": 0.1355,
      "step": 3970
    },
    {
      "epoch": 2.1226666666666665,
      "grad_norm": 2.231929302215576,
      "learning_rate": 1.2924444444444447e-05,
      "loss": 0.1177,
      "step": 3980
    },
    {
      "epoch": 2.128,
      "grad_norm": 3.1680867671966553,
      "learning_rate": 1.2906666666666668e-05,
      "loss": 0.1095,
      "step": 3990
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 2.6572136878967285,
      "learning_rate": 1.288888888888889e-05,
      "loss": 0.1146,
      "step": 4000
    },
    {
      "epoch": 2.1386666666666665,
      "grad_norm": 3.004672050476074,
      "learning_rate": 1.2871111111111112e-05,
      "loss": 0.1032,
      "step": 4010
    },
    {
      "epoch": 2.144,
      "grad_norm": 4.298494815826416,
      "learning_rate": 1.2853333333333336e-05,
      "loss": 0.1473,
      "step": 4020
    },
    {
      "epoch": 2.1493333333333333,
      "grad_norm": 3.680021286010742,
      "learning_rate": 1.2835555555555557e-05,
      "loss": 0.1251,
      "step": 4030
    },
    {
      "epoch": 2.1546666666666665,
      "grad_norm": 1.8566230535507202,
      "learning_rate": 1.2817777777777778e-05,
      "loss": 0.097,
      "step": 4040
    },
    {
      "epoch": 2.16,
      "grad_norm": 2.4633026123046875,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.1101,
      "step": 4050
    },
    {
      "epoch": 2.1653333333333333,
      "grad_norm": 2.8745176792144775,
      "learning_rate": 1.2782222222222222e-05,
      "loss": 0.1343,
      "step": 4060
    },
    {
      "epoch": 2.1706666666666665,
      "grad_norm": 2.4734010696411133,
      "learning_rate": 1.2764444444444447e-05,
      "loss": 0.129,
      "step": 4070
    },
    {
      "epoch": 2.176,
      "grad_norm": 2.410842180252075,
      "learning_rate": 1.2746666666666668e-05,
      "loss": 0.1441,
      "step": 4080
    },
    {
      "epoch": 2.1813333333333333,
      "grad_norm": 2.3362560272216797,
      "learning_rate": 1.272888888888889e-05,
      "loss": 0.1105,
      "step": 4090
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 2.646885871887207,
      "learning_rate": 1.2711111111111112e-05,
      "loss": 0.1495,
      "step": 4100
    },
    {
      "epoch": 2.192,
      "grad_norm": 3.7042927742004395,
      "learning_rate": 1.2693333333333336e-05,
      "loss": 0.1372,
      "step": 4110
    },
    {
      "epoch": 2.1973333333333334,
      "grad_norm": 2.575762987136841,
      "learning_rate": 1.2675555555555557e-05,
      "loss": 0.1244,
      "step": 4120
    },
    {
      "epoch": 2.2026666666666666,
      "grad_norm": 3.7450408935546875,
      "learning_rate": 1.2657777777777778e-05,
      "loss": 0.1177,
      "step": 4130
    },
    {
      "epoch": 2.208,
      "grad_norm": 3.5500450134277344,
      "learning_rate": 1.2640000000000001e-05,
      "loss": 0.1451,
      "step": 4140
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 3.217911958694458,
      "learning_rate": 1.2622222222222222e-05,
      "loss": 0.11,
      "step": 4150
    },
    {
      "epoch": 2.2186666666666666,
      "grad_norm": 2.4138896465301514,
      "learning_rate": 1.2604444444444446e-05,
      "loss": 0.1037,
      "step": 4160
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.8964841365814209,
      "learning_rate": 1.2586666666666668e-05,
      "loss": 0.1337,
      "step": 4170
    },
    {
      "epoch": 2.2293333333333334,
      "grad_norm": 3.5036399364471436,
      "learning_rate": 1.256888888888889e-05,
      "loss": 0.1551,
      "step": 4180
    },
    {
      "epoch": 2.2346666666666666,
      "grad_norm": 1.9698176383972168,
      "learning_rate": 1.2551111111111111e-05,
      "loss": 0.1114,
      "step": 4190
    },
    {
      "epoch": 2.24,
      "grad_norm": 1.9366596937179565,
      "learning_rate": 1.2533333333333336e-05,
      "loss": 0.1194,
      "step": 4200
    },
    {
      "epoch": 2.2453333333333334,
      "grad_norm": 2.0927278995513916,
      "learning_rate": 1.2515555555555557e-05,
      "loss": 0.0997,
      "step": 4210
    },
    {
      "epoch": 2.2506666666666666,
      "grad_norm": 1.061585783958435,
      "learning_rate": 1.249777777777778e-05,
      "loss": 0.117,
      "step": 4220
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 3.506511926651001,
      "learning_rate": 1.248e-05,
      "loss": 0.1014,
      "step": 4230
    },
    {
      "epoch": 2.2613333333333334,
      "grad_norm": 2.4921152591705322,
      "learning_rate": 1.2462222222222222e-05,
      "loss": 0.1084,
      "step": 4240
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 2.272939920425415,
      "learning_rate": 1.2444444444444446e-05,
      "loss": 0.1216,
      "step": 4250
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 3.506897211074829,
      "learning_rate": 1.2426666666666667e-05,
      "loss": 0.1182,
      "step": 4260
    },
    {
      "epoch": 2.2773333333333334,
      "grad_norm": 3.6775786876678467,
      "learning_rate": 1.240888888888889e-05,
      "loss": 0.1407,
      "step": 4270
    },
    {
      "epoch": 2.2826666666666666,
      "grad_norm": 1.6933131217956543,
      "learning_rate": 1.2391111111111111e-05,
      "loss": 0.1321,
      "step": 4280
    },
    {
      "epoch": 2.288,
      "grad_norm": 3.6232361793518066,
      "learning_rate": 1.2373333333333336e-05,
      "loss": 0.1352,
      "step": 4290
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 3.362828493118286,
      "learning_rate": 1.2355555555555557e-05,
      "loss": 0.1303,
      "step": 4300
    },
    {
      "epoch": 2.2986666666666666,
      "grad_norm": 2.60892915725708,
      "learning_rate": 1.233777777777778e-05,
      "loss": 0.1446,
      "step": 4310
    },
    {
      "epoch": 2.304,
      "grad_norm": 3.506943702697754,
      "learning_rate": 1.232e-05,
      "loss": 0.1453,
      "step": 4320
    },
    {
      "epoch": 2.3093333333333335,
      "grad_norm": 2.681145429611206,
      "learning_rate": 1.2302222222222225e-05,
      "loss": 0.1151,
      "step": 4330
    },
    {
      "epoch": 2.3146666666666667,
      "grad_norm": 2.430462121963501,
      "learning_rate": 1.2284444444444446e-05,
      "loss": 0.129,
      "step": 4340
    },
    {
      "epoch": 2.32,
      "grad_norm": 1.7383267879486084,
      "learning_rate": 1.2266666666666667e-05,
      "loss": 0.1098,
      "step": 4350
    },
    {
      "epoch": 2.3253333333333335,
      "grad_norm": 1.4201692342758179,
      "learning_rate": 1.224888888888889e-05,
      "loss": 0.1226,
      "step": 4360
    },
    {
      "epoch": 2.3306666666666667,
      "grad_norm": 2.881488561630249,
      "learning_rate": 1.2231111111111111e-05,
      "loss": 0.1347,
      "step": 4370
    },
    {
      "epoch": 2.336,
      "grad_norm": 2.438725709915161,
      "learning_rate": 1.2213333333333336e-05,
      "loss": 0.0951,
      "step": 4380
    },
    {
      "epoch": 2.3413333333333335,
      "grad_norm": 3.763273000717163,
      "learning_rate": 1.2195555555555557e-05,
      "loss": 0.1358,
      "step": 4390
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 2.441922903060913,
      "learning_rate": 1.217777777777778e-05,
      "loss": 0.1253,
      "step": 4400
    },
    {
      "epoch": 2.352,
      "grad_norm": 2.5848562717437744,
      "learning_rate": 1.216e-05,
      "loss": 0.127,
      "step": 4410
    },
    {
      "epoch": 2.3573333333333335,
      "grad_norm": 1.9739857912063599,
      "learning_rate": 1.2142222222222225e-05,
      "loss": 0.1182,
      "step": 4420
    },
    {
      "epoch": 2.3626666666666667,
      "grad_norm": 1.2749861478805542,
      "learning_rate": 1.2124444444444446e-05,
      "loss": 0.0939,
      "step": 4430
    },
    {
      "epoch": 2.368,
      "grad_norm": 2.3379905223846436,
      "learning_rate": 1.2106666666666667e-05,
      "loss": 0.1088,
      "step": 4440
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 2.8720269203186035,
      "learning_rate": 1.208888888888889e-05,
      "loss": 0.1063,
      "step": 4450
    },
    {
      "epoch": 2.3786666666666667,
      "grad_norm": 3.8488450050354004,
      "learning_rate": 1.2071111111111111e-05,
      "loss": 0.1515,
      "step": 4460
    },
    {
      "epoch": 2.384,
      "grad_norm": 2.9189374446868896,
      "learning_rate": 1.2053333333333335e-05,
      "loss": 0.1305,
      "step": 4470
    },
    {
      "epoch": 2.389333333333333,
      "grad_norm": 2.190711498260498,
      "learning_rate": 1.2035555555555556e-05,
      "loss": 0.1228,
      "step": 4480
    },
    {
      "epoch": 2.3946666666666667,
      "grad_norm": 1.1641578674316406,
      "learning_rate": 1.201777777777778e-05,
      "loss": 0.1318,
      "step": 4490
    },
    {
      "epoch": 2.4,
      "grad_norm": 2.172983169555664,
      "learning_rate": 1.2e-05,
      "loss": 0.1542,
      "step": 4500
    },
    {
      "epoch": 2.405333333333333,
      "grad_norm": 1.464444875717163,
      "learning_rate": 1.1982222222222225e-05,
      "loss": 0.0966,
      "step": 4510
    },
    {
      "epoch": 2.4106666666666667,
      "grad_norm": 2.7035908699035645,
      "learning_rate": 1.1964444444444446e-05,
      "loss": 0.1386,
      "step": 4520
    },
    {
      "epoch": 2.416,
      "grad_norm": 1.5582448244094849,
      "learning_rate": 1.1946666666666669e-05,
      "loss": 0.1226,
      "step": 4530
    },
    {
      "epoch": 2.421333333333333,
      "grad_norm": 0.9444266557693481,
      "learning_rate": 1.192888888888889e-05,
      "loss": 0.1037,
      "step": 4540
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 2.3643338680267334,
      "learning_rate": 1.191111111111111e-05,
      "loss": 0.122,
      "step": 4550
    },
    {
      "epoch": 2.432,
      "grad_norm": 4.056543350219727,
      "learning_rate": 1.1893333333333335e-05,
      "loss": 0.1035,
      "step": 4560
    },
    {
      "epoch": 2.437333333333333,
      "grad_norm": 1.8652719259262085,
      "learning_rate": 1.1875555555555556e-05,
      "loss": 0.0935,
      "step": 4570
    },
    {
      "epoch": 2.4426666666666668,
      "grad_norm": 2.6090590953826904,
      "learning_rate": 1.1857777777777779e-05,
      "loss": 0.14,
      "step": 4580
    },
    {
      "epoch": 2.448,
      "grad_norm": 2.524867057800293,
      "learning_rate": 1.184e-05,
      "loss": 0.1254,
      "step": 4590
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 4.0759806632995605,
      "learning_rate": 1.1822222222222225e-05,
      "loss": 0.0902,
      "step": 4600
    },
    {
      "epoch": 2.458666666666667,
      "grad_norm": 3.6431398391723633,
      "learning_rate": 1.1804444444444446e-05,
      "loss": 0.103,
      "step": 4610
    },
    {
      "epoch": 2.464,
      "grad_norm": 2.1241977214813232,
      "learning_rate": 1.1786666666666668e-05,
      "loss": 0.1461,
      "step": 4620
    },
    {
      "epoch": 2.469333333333333,
      "grad_norm": 2.7928895950317383,
      "learning_rate": 1.176888888888889e-05,
      "loss": 0.1203,
      "step": 4630
    },
    {
      "epoch": 2.474666666666667,
      "grad_norm": 1.9039937257766724,
      "learning_rate": 1.1751111111111112e-05,
      "loss": 0.1152,
      "step": 4640
    },
    {
      "epoch": 2.48,
      "grad_norm": 2.7577102184295654,
      "learning_rate": 1.1733333333333335e-05,
      "loss": 0.1387,
      "step": 4650
    },
    {
      "epoch": 2.485333333333333,
      "grad_norm": 3.4816513061523438,
      "learning_rate": 1.1715555555555556e-05,
      "loss": 0.1318,
      "step": 4660
    },
    {
      "epoch": 2.490666666666667,
      "grad_norm": 2.2267086505889893,
      "learning_rate": 1.1697777777777779e-05,
      "loss": 0.098,
      "step": 4670
    },
    {
      "epoch": 2.496,
      "grad_norm": 2.173872947692871,
      "learning_rate": 1.168e-05,
      "loss": 0.1155,
      "step": 4680
    },
    {
      "epoch": 2.501333333333333,
      "grad_norm": 2.036872386932373,
      "learning_rate": 1.1662222222222224e-05,
      "loss": 0.1162,
      "step": 4690
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 3.5758752822875977,
      "learning_rate": 1.1644444444444446e-05,
      "loss": 0.1392,
      "step": 4700
    },
    {
      "epoch": 2.512,
      "grad_norm": 3.0388126373291016,
      "learning_rate": 1.1626666666666668e-05,
      "loss": 0.1338,
      "step": 4710
    },
    {
      "epoch": 2.517333333333333,
      "grad_norm": 2.230788230895996,
      "learning_rate": 1.160888888888889e-05,
      "loss": 0.1038,
      "step": 4720
    },
    {
      "epoch": 2.522666666666667,
      "grad_norm": 3.9031906127929688,
      "learning_rate": 1.1591111111111114e-05,
      "loss": 0.1246,
      "step": 4730
    },
    {
      "epoch": 2.528,
      "grad_norm": 2.590242862701416,
      "learning_rate": 1.1573333333333335e-05,
      "loss": 0.0912,
      "step": 4740
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 2.481816291809082,
      "learning_rate": 1.1555555555555556e-05,
      "loss": 0.1044,
      "step": 4750
    },
    {
      "epoch": 2.538666666666667,
      "grad_norm": 2.837552547454834,
      "learning_rate": 1.1537777777777779e-05,
      "loss": 0.1405,
      "step": 4760
    },
    {
      "epoch": 2.544,
      "grad_norm": 3.760134220123291,
      "learning_rate": 1.152e-05,
      "loss": 0.1406,
      "step": 4770
    },
    {
      "epoch": 2.5493333333333332,
      "grad_norm": 1.674196481704712,
      "learning_rate": 1.1502222222222224e-05,
      "loss": 0.1358,
      "step": 4780
    },
    {
      "epoch": 2.554666666666667,
      "grad_norm": 2.288304567337036,
      "learning_rate": 1.1484444444444445e-05,
      "loss": 0.0979,
      "step": 4790
    },
    {
      "epoch": 2.56,
      "grad_norm": 3.237534523010254,
      "learning_rate": 1.1466666666666668e-05,
      "loss": 0.1132,
      "step": 4800
    },
    {
      "epoch": 2.5653333333333332,
      "grad_norm": 4.881216526031494,
      "learning_rate": 1.144888888888889e-05,
      "loss": 0.1353,
      "step": 4810
    },
    {
      "epoch": 2.570666666666667,
      "grad_norm": 2.478020668029785,
      "learning_rate": 1.1431111111111114e-05,
      "loss": 0.1271,
      "step": 4820
    },
    {
      "epoch": 2.576,
      "grad_norm": 2.9154040813446045,
      "learning_rate": 1.1413333333333335e-05,
      "loss": 0.1187,
      "step": 4830
    },
    {
      "epoch": 2.5813333333333333,
      "grad_norm": 2.6146726608276367,
      "learning_rate": 1.1395555555555558e-05,
      "loss": 0.1266,
      "step": 4840
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 2.512331485748291,
      "learning_rate": 1.1377777777777779e-05,
      "loss": 0.1051,
      "step": 4850
    },
    {
      "epoch": 2.592,
      "grad_norm": 5.7144036293029785,
      "learning_rate": 1.136e-05,
      "loss": 0.1192,
      "step": 4860
    },
    {
      "epoch": 2.5973333333333333,
      "grad_norm": 2.7632086277008057,
      "learning_rate": 1.1342222222222224e-05,
      "loss": 0.1105,
      "step": 4870
    },
    {
      "epoch": 2.602666666666667,
      "grad_norm": 2.0421581268310547,
      "learning_rate": 1.1324444444444445e-05,
      "loss": 0.0786,
      "step": 4880
    },
    {
      "epoch": 2.608,
      "grad_norm": 2.366762399673462,
      "learning_rate": 1.1306666666666668e-05,
      "loss": 0.1216,
      "step": 4890
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 3.487360954284668,
      "learning_rate": 1.1288888888888889e-05,
      "loss": 0.142,
      "step": 4900
    },
    {
      "epoch": 2.618666666666667,
      "grad_norm": 3.3519349098205566,
      "learning_rate": 1.1271111111111113e-05,
      "loss": 0.1235,
      "step": 4910
    },
    {
      "epoch": 2.624,
      "grad_norm": 4.320370197296143,
      "learning_rate": 1.1253333333333335e-05,
      "loss": 0.1443,
      "step": 4920
    },
    {
      "epoch": 2.6293333333333333,
      "grad_norm": 3.1199517250061035,
      "learning_rate": 1.1235555555555557e-05,
      "loss": 0.1238,
      "step": 4930
    },
    {
      "epoch": 2.634666666666667,
      "grad_norm": 2.096670389175415,
      "learning_rate": 1.1217777777777778e-05,
      "loss": 0.1241,
      "step": 4940
    },
    {
      "epoch": 2.64,
      "grad_norm": 3.8827834129333496,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.166,
      "step": 4950
    },
    {
      "epoch": 2.6453333333333333,
      "grad_norm": 2.8587591648101807,
      "learning_rate": 1.1182222222222224e-05,
      "loss": 0.1051,
      "step": 4960
    },
    {
      "epoch": 2.6506666666666665,
      "grad_norm": 1.3615856170654297,
      "learning_rate": 1.1164444444444445e-05,
      "loss": 0.0847,
      "step": 4970
    },
    {
      "epoch": 2.656,
      "grad_norm": 3.1778948307037354,
      "learning_rate": 1.1146666666666668e-05,
      "loss": 0.1094,
      "step": 4980
    },
    {
      "epoch": 2.6613333333333333,
      "grad_norm": 4.413106918334961,
      "learning_rate": 1.1128888888888889e-05,
      "loss": 0.1173,
      "step": 4990
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 2.8104732036590576,
      "learning_rate": 1.1111111111111113e-05,
      "loss": 0.1442,
      "step": 5000
    },
    {
      "epoch": 2.672,
      "grad_norm": 3.060443162918091,
      "learning_rate": 1.1093333333333334e-05,
      "loss": 0.1604,
      "step": 5010
    },
    {
      "epoch": 2.6773333333333333,
      "grad_norm": 2.7398476600646973,
      "learning_rate": 1.1075555555555557e-05,
      "loss": 0.0944,
      "step": 5020
    },
    {
      "epoch": 2.6826666666666665,
      "grad_norm": 0.9589354395866394,
      "learning_rate": 1.1057777777777778e-05,
      "loss": 0.0985,
      "step": 5030
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 2.016634225845337,
      "learning_rate": 1.1040000000000001e-05,
      "loss": 0.1501,
      "step": 5040
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 4.195804595947266,
      "learning_rate": 1.1022222222222224e-05,
      "loss": 0.1323,
      "step": 5050
    },
    {
      "epoch": 2.6986666666666665,
      "grad_norm": 2.809325933456421,
      "learning_rate": 1.1004444444444445e-05,
      "loss": 0.135,
      "step": 5060
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 2.8179261684417725,
      "learning_rate": 1.0986666666666668e-05,
      "loss": 0.1164,
      "step": 5070
    },
    {
      "epoch": 2.7093333333333334,
      "grad_norm": 2.5281310081481934,
      "learning_rate": 1.0968888888888889e-05,
      "loss": 0.1175,
      "step": 5080
    },
    {
      "epoch": 2.7146666666666666,
      "grad_norm": 1.1991519927978516,
      "learning_rate": 1.0951111111111113e-05,
      "loss": 0.0769,
      "step": 5090
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 2.497110366821289,
      "learning_rate": 1.0933333333333334e-05,
      "loss": 0.1156,
      "step": 5100
    },
    {
      "epoch": 2.7253333333333334,
      "grad_norm": 5.528012752532959,
      "learning_rate": 1.0915555555555557e-05,
      "loss": 0.1422,
      "step": 5110
    },
    {
      "epoch": 2.7306666666666666,
      "grad_norm": 2.8411097526550293,
      "learning_rate": 1.0897777777777778e-05,
      "loss": 0.1052,
      "step": 5120
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 3.7450993061065674,
      "learning_rate": 1.0880000000000001e-05,
      "loss": 0.1092,
      "step": 5130
    },
    {
      "epoch": 2.7413333333333334,
      "grad_norm": 2.2030937671661377,
      "learning_rate": 1.0862222222222224e-05,
      "loss": 0.1336,
      "step": 5140
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 1.867138385772705,
      "learning_rate": 1.0844444444444446e-05,
      "loss": 0.0967,
      "step": 5150
    },
    {
      "epoch": 2.752,
      "grad_norm": 4.786658763885498,
      "learning_rate": 1.0826666666666667e-05,
      "loss": 0.1298,
      "step": 5160
    },
    {
      "epoch": 2.7573333333333334,
      "grad_norm": 3.3210232257843018,
      "learning_rate": 1.0808888888888889e-05,
      "loss": 0.1029,
      "step": 5170
    },
    {
      "epoch": 2.7626666666666666,
      "grad_norm": 2.967869281768799,
      "learning_rate": 1.0791111111111113e-05,
      "loss": 0.1268,
      "step": 5180
    },
    {
      "epoch": 2.768,
      "grad_norm": 3.6225898265838623,
      "learning_rate": 1.0773333333333334e-05,
      "loss": 0.1299,
      "step": 5190
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 2.495469570159912,
      "learning_rate": 1.0755555555555557e-05,
      "loss": 0.1507,
      "step": 5200
    },
    {
      "epoch": 2.7786666666666666,
      "grad_norm": 2.4550020694732666,
      "learning_rate": 1.0737777777777778e-05,
      "loss": 0.1106,
      "step": 5210
    },
    {
      "epoch": 2.784,
      "grad_norm": 3.0362815856933594,
      "learning_rate": 1.072e-05,
      "loss": 0.1059,
      "step": 5220
    },
    {
      "epoch": 2.7893333333333334,
      "grad_norm": 1.2655634880065918,
      "learning_rate": 1.0702222222222223e-05,
      "loss": 0.1212,
      "step": 5230
    },
    {
      "epoch": 2.7946666666666666,
      "grad_norm": 1.3856300115585327,
      "learning_rate": 1.0684444444444446e-05,
      "loss": 0.0914,
      "step": 5240
    },
    {
      "epoch": 2.8,
      "grad_norm": 2.218418836593628,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 0.1042,
      "step": 5250
    },
    {
      "epoch": 2.8053333333333335,
      "grad_norm": 1.9711847305297852,
      "learning_rate": 1.064888888888889e-05,
      "loss": 0.1335,
      "step": 5260
    },
    {
      "epoch": 2.8106666666666666,
      "grad_norm": 3.3763976097106934,
      "learning_rate": 1.0631111111111113e-05,
      "loss": 0.1277,
      "step": 5270
    },
    {
      "epoch": 2.816,
      "grad_norm": 3.966132164001465,
      "learning_rate": 1.0613333333333334e-05,
      "loss": 0.1257,
      "step": 5280
    },
    {
      "epoch": 2.8213333333333335,
      "grad_norm": 1.6972606182098389,
      "learning_rate": 1.0595555555555557e-05,
      "loss": 0.1059,
      "step": 5290
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 4.0111589431762695,
      "learning_rate": 1.0577777777777778e-05,
      "loss": 0.1119,
      "step": 5300
    },
    {
      "epoch": 2.832,
      "grad_norm": 2.6879639625549316,
      "learning_rate": 1.056e-05,
      "loss": 0.1214,
      "step": 5310
    },
    {
      "epoch": 2.8373333333333335,
      "grad_norm": 4.766139030456543,
      "learning_rate": 1.0542222222222223e-05,
      "loss": 0.1415,
      "step": 5320
    },
    {
      "epoch": 2.8426666666666667,
      "grad_norm": 2.5492234230041504,
      "learning_rate": 1.0524444444444446e-05,
      "loss": 0.1144,
      "step": 5330
    },
    {
      "epoch": 2.848,
      "grad_norm": 3.494635581970215,
      "learning_rate": 1.0506666666666667e-05,
      "loss": 0.1113,
      "step": 5340
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 4.055483341217041,
      "learning_rate": 1.048888888888889e-05,
      "loss": 0.1523,
      "step": 5350
    },
    {
      "epoch": 2.8586666666666667,
      "grad_norm": 2.106936454772949,
      "learning_rate": 1.0471111111111113e-05,
      "loss": 0.1285,
      "step": 5360
    },
    {
      "epoch": 2.864,
      "grad_norm": 3.5929722785949707,
      "learning_rate": 1.0453333333333334e-05,
      "loss": 0.1172,
      "step": 5370
    },
    {
      "epoch": 2.8693333333333335,
      "grad_norm": 1.178064227104187,
      "learning_rate": 1.0435555555555557e-05,
      "loss": 0.1018,
      "step": 5380
    },
    {
      "epoch": 2.8746666666666667,
      "grad_norm": 2.5333144664764404,
      "learning_rate": 1.0417777777777778e-05,
      "loss": 0.0868,
      "step": 5390
    },
    {
      "epoch": 2.88,
      "grad_norm": 2.581543445587158,
      "learning_rate": 1.04e-05,
      "loss": 0.1022,
      "step": 5400
    },
    {
      "epoch": 2.8853333333333335,
      "grad_norm": 2.5611841678619385,
      "learning_rate": 1.0382222222222223e-05,
      "loss": 0.1216,
      "step": 5410
    },
    {
      "epoch": 2.8906666666666667,
      "grad_norm": 2.681481122970581,
      "learning_rate": 1.0364444444444446e-05,
      "loss": 0.0965,
      "step": 5420
    },
    {
      "epoch": 2.896,
      "grad_norm": 3.0234453678131104,
      "learning_rate": 1.0346666666666667e-05,
      "loss": 0.1429,
      "step": 5430
    },
    {
      "epoch": 2.9013333333333335,
      "grad_norm": 3.183272361755371,
      "learning_rate": 1.032888888888889e-05,
      "loss": 0.1242,
      "step": 5440
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 2.795175552368164,
      "learning_rate": 1.0311111111111113e-05,
      "loss": 0.1198,
      "step": 5450
    },
    {
      "epoch": 2.912,
      "grad_norm": 2.4186291694641113,
      "learning_rate": 1.0293333333333335e-05,
      "loss": 0.1317,
      "step": 5460
    },
    {
      "epoch": 2.9173333333333336,
      "grad_norm": 3.5247762203216553,
      "learning_rate": 1.0275555555555556e-05,
      "loss": 0.1707,
      "step": 5470
    },
    {
      "epoch": 2.9226666666666667,
      "grad_norm": 3.0361177921295166,
      "learning_rate": 1.0257777777777777e-05,
      "loss": 0.1299,
      "step": 5480
    },
    {
      "epoch": 2.928,
      "grad_norm": 2.3633058071136475,
      "learning_rate": 1.024e-05,
      "loss": 0.0929,
      "step": 5490
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 2.2577219009399414,
      "learning_rate": 1.0222222222222223e-05,
      "loss": 0.0942,
      "step": 5500
    },
    {
      "epoch": 2.9386666666666668,
      "grad_norm": 1.918614149093628,
      "learning_rate": 1.0204444444444446e-05,
      "loss": 0.1513,
      "step": 5510
    },
    {
      "epoch": 2.944,
      "grad_norm": 2.115497350692749,
      "learning_rate": 1.0186666666666667e-05,
      "loss": 0.1202,
      "step": 5520
    },
    {
      "epoch": 2.9493333333333336,
      "grad_norm": 2.6877574920654297,
      "learning_rate": 1.016888888888889e-05,
      "loss": 0.1239,
      "step": 5530
    },
    {
      "epoch": 2.9546666666666668,
      "grad_norm": 1.8050297498703003,
      "learning_rate": 1.0151111111111112e-05,
      "loss": 0.105,
      "step": 5540
    },
    {
      "epoch": 2.96,
      "grad_norm": 3.4264841079711914,
      "learning_rate": 1.0133333333333335e-05,
      "loss": 0.1076,
      "step": 5550
    },
    {
      "epoch": 2.9653333333333336,
      "grad_norm": 4.171085357666016,
      "learning_rate": 1.0115555555555556e-05,
      "loss": 0.1246,
      "step": 5560
    },
    {
      "epoch": 2.970666666666667,
      "grad_norm": 3.0199296474456787,
      "learning_rate": 1.0097777777777779e-05,
      "loss": 0.1414,
      "step": 5570
    },
    {
      "epoch": 2.976,
      "grad_norm": 4.669869422912598,
      "learning_rate": 1.008e-05,
      "loss": 0.1459,
      "step": 5580
    },
    {
      "epoch": 2.981333333333333,
      "grad_norm": 2.078173875808716,
      "learning_rate": 1.0062222222222223e-05,
      "loss": 0.1535,
      "step": 5590
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 1.5296804904937744,
      "learning_rate": 1.0044444444444446e-05,
      "loss": 0.1405,
      "step": 5600
    },
    {
      "epoch": 2.992,
      "grad_norm": 3.9549458026885986,
      "learning_rate": 1.0026666666666667e-05,
      "loss": 0.117,
      "step": 5610
    },
    {
      "epoch": 2.997333333333333,
      "grad_norm": 2.0676591396331787,
      "learning_rate": 1.000888888888889e-05,
      "loss": 0.1125,
      "step": 5620
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.9599,
      "eval_loss": 0.129746213555336,
      "eval_runtime": 47.3399,
      "eval_samples_per_second": 422.477,
      "eval_steps_per_second": 4.415,
      "step": 5625
    },
    {
      "epoch": 3.002666666666667,
      "grad_norm": 1.6207492351531982,
      "learning_rate": 9.991111111111112e-06,
      "loss": 0.1059,
      "step": 5630
    },
    {
      "epoch": 3.008,
      "grad_norm": 1.5174013376235962,
      "learning_rate": 9.973333333333333e-06,
      "loss": 0.1139,
      "step": 5640
    },
    {
      "epoch": 3.013333333333333,
      "grad_norm": 4.936830997467041,
      "learning_rate": 9.955555555555556e-06,
      "loss": 0.0966,
      "step": 5650
    },
    {
      "epoch": 3.018666666666667,
      "grad_norm": 2.162876605987549,
      "learning_rate": 9.937777777777779e-06,
      "loss": 0.0946,
      "step": 5660
    },
    {
      "epoch": 3.024,
      "grad_norm": 2.7867748737335205,
      "learning_rate": 9.920000000000002e-06,
      "loss": 0.0965,
      "step": 5670
    },
    {
      "epoch": 3.029333333333333,
      "grad_norm": 1.6910783052444458,
      "learning_rate": 9.902222222222223e-06,
      "loss": 0.1011,
      "step": 5680
    },
    {
      "epoch": 3.034666666666667,
      "grad_norm": 2.941086530685425,
      "learning_rate": 9.884444444444445e-06,
      "loss": 0.1273,
      "step": 5690
    },
    {
      "epoch": 3.04,
      "grad_norm": 2.704139471054077,
      "learning_rate": 9.866666666666668e-06,
      "loss": 0.1272,
      "step": 5700
    },
    {
      "epoch": 3.0453333333333332,
      "grad_norm": 2.6639046669006348,
      "learning_rate": 9.84888888888889e-06,
      "loss": 0.0829,
      "step": 5710
    },
    {
      "epoch": 3.050666666666667,
      "grad_norm": 1.6811140775680542,
      "learning_rate": 9.831111111111112e-06,
      "loss": 0.1019,
      "step": 5720
    },
    {
      "epoch": 3.056,
      "grad_norm": 1.369370937347412,
      "learning_rate": 9.813333333333333e-06,
      "loss": 0.1,
      "step": 5730
    },
    {
      "epoch": 3.0613333333333332,
      "grad_norm": 2.7999165058135986,
      "learning_rate": 9.795555555555556e-06,
      "loss": 0.0934,
      "step": 5740
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 2.4327640533447266,
      "learning_rate": 9.777777777777779e-06,
      "loss": 0.1435,
      "step": 5750
    },
    {
      "epoch": 3.072,
      "grad_norm": 4.487662315368652,
      "learning_rate": 9.760000000000001e-06,
      "loss": 0.1212,
      "step": 5760
    },
    {
      "epoch": 3.0773333333333333,
      "grad_norm": 2.5181078910827637,
      "learning_rate": 9.742222222222222e-06,
      "loss": 0.1079,
      "step": 5770
    },
    {
      "epoch": 3.0826666666666664,
      "grad_norm": 2.5873661041259766,
      "learning_rate": 9.724444444444445e-06,
      "loss": 0.0842,
      "step": 5780
    },
    {
      "epoch": 3.088,
      "grad_norm": 2.7270078659057617,
      "learning_rate": 9.706666666666668e-06,
      "loss": 0.086,
      "step": 5790
    },
    {
      "epoch": 3.0933333333333333,
      "grad_norm": 2.6184825897216797,
      "learning_rate": 9.688888888888889e-06,
      "loss": 0.1163,
      "step": 5800
    },
    {
      "epoch": 3.0986666666666665,
      "grad_norm": 3.6501126289367676,
      "learning_rate": 9.671111111111112e-06,
      "loss": 0.1331,
      "step": 5810
    },
    {
      "epoch": 3.104,
      "grad_norm": 3.093635082244873,
      "learning_rate": 9.653333333333335e-06,
      "loss": 0.1492,
      "step": 5820
    },
    {
      "epoch": 3.1093333333333333,
      "grad_norm": 2.0141780376434326,
      "learning_rate": 9.635555555555557e-06,
      "loss": 0.0952,
      "step": 5830
    },
    {
      "epoch": 3.1146666666666665,
      "grad_norm": 2.475212335586548,
      "learning_rate": 9.617777777777778e-06,
      "loss": 0.1081,
      "step": 5840
    },
    {
      "epoch": 3.12,
      "grad_norm": 2.100945234298706,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.123,
      "step": 5850
    },
    {
      "epoch": 3.1253333333333333,
      "grad_norm": 2.6051430702209473,
      "learning_rate": 9.582222222222222e-06,
      "loss": 0.1198,
      "step": 5860
    },
    {
      "epoch": 3.1306666666666665,
      "grad_norm": 2.657282829284668,
      "learning_rate": 9.564444444444445e-06,
      "loss": 0.0898,
      "step": 5870
    },
    {
      "epoch": 3.136,
      "grad_norm": 2.1784534454345703,
      "learning_rate": 9.546666666666668e-06,
      "loss": 0.0943,
      "step": 5880
    },
    {
      "epoch": 3.1413333333333333,
      "grad_norm": 2.2362728118896484,
      "learning_rate": 9.528888888888889e-06,
      "loss": 0.1258,
      "step": 5890
    },
    {
      "epoch": 3.1466666666666665,
      "grad_norm": 2.520212411880493,
      "learning_rate": 9.511111111111112e-06,
      "loss": 0.0949,
      "step": 5900
    },
    {
      "epoch": 3.152,
      "grad_norm": 2.201612710952759,
      "learning_rate": 9.493333333333334e-06,
      "loss": 0.1563,
      "step": 5910
    },
    {
      "epoch": 3.1573333333333333,
      "grad_norm": 3.063119649887085,
      "learning_rate": 9.475555555555557e-06,
      "loss": 0.1067,
      "step": 5920
    },
    {
      "epoch": 3.1626666666666665,
      "grad_norm": 3.073002815246582,
      "learning_rate": 9.457777777777778e-06,
      "loss": 0.1314,
      "step": 5930
    },
    {
      "epoch": 3.168,
      "grad_norm": 3.0132675170898438,
      "learning_rate": 9.440000000000001e-06,
      "loss": 0.0897,
      "step": 5940
    },
    {
      "epoch": 3.1733333333333333,
      "grad_norm": 1.616753101348877,
      "learning_rate": 9.422222222222222e-06,
      "loss": 0.115,
      "step": 5950
    },
    {
      "epoch": 3.1786666666666665,
      "grad_norm": 2.394277572631836,
      "learning_rate": 9.404444444444445e-06,
      "loss": 0.1035,
      "step": 5960
    },
    {
      "epoch": 3.184,
      "grad_norm": 2.175737142562866,
      "learning_rate": 9.386666666666668e-06,
      "loss": 0.0785,
      "step": 5970
    },
    {
      "epoch": 3.1893333333333334,
      "grad_norm": 2.5098917484283447,
      "learning_rate": 9.368888888888889e-06,
      "loss": 0.0999,
      "step": 5980
    },
    {
      "epoch": 3.1946666666666665,
      "grad_norm": 2.2290804386138916,
      "learning_rate": 9.351111111111112e-06,
      "loss": 0.1036,
      "step": 5990
    },
    {
      "epoch": 3.2,
      "grad_norm": 1.7348796129226685,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.099,
      "step": 6000
    },
    {
      "epoch": 3.2053333333333334,
      "grad_norm": 2.798473834991455,
      "learning_rate": 9.315555555555557e-06,
      "loss": 0.1009,
      "step": 6010
    },
    {
      "epoch": 3.2106666666666666,
      "grad_norm": 2.227233409881592,
      "learning_rate": 9.297777777777778e-06,
      "loss": 0.1211,
      "step": 6020
    },
    {
      "epoch": 3.216,
      "grad_norm": 2.0560832023620605,
      "learning_rate": 9.280000000000001e-06,
      "loss": 0.0593,
      "step": 6030
    },
    {
      "epoch": 3.2213333333333334,
      "grad_norm": 1.846826195716858,
      "learning_rate": 9.262222222222222e-06,
      "loss": 0.1069,
      "step": 6040
    },
    {
      "epoch": 3.2266666666666666,
      "grad_norm": 2.3470048904418945,
      "learning_rate": 9.244444444444445e-06,
      "loss": 0.0956,
      "step": 6050
    },
    {
      "epoch": 3.232,
      "grad_norm": 2.5662128925323486,
      "learning_rate": 9.226666666666668e-06,
      "loss": 0.1268,
      "step": 6060
    },
    {
      "epoch": 3.2373333333333334,
      "grad_norm": 2.729339122772217,
      "learning_rate": 9.208888888888889e-06,
      "loss": 0.0807,
      "step": 6070
    },
    {
      "epoch": 3.2426666666666666,
      "grad_norm": 1.593264102935791,
      "learning_rate": 9.191111111111111e-06,
      "loss": 0.1084,
      "step": 6080
    },
    {
      "epoch": 3.248,
      "grad_norm": 2.9654974937438965,
      "learning_rate": 9.173333333333334e-06,
      "loss": 0.0789,
      "step": 6090
    },
    {
      "epoch": 3.2533333333333334,
      "grad_norm": 3.1065382957458496,
      "learning_rate": 9.155555555555557e-06,
      "loss": 0.0886,
      "step": 6100
    },
    {
      "epoch": 3.2586666666666666,
      "grad_norm": 1.946056604385376,
      "learning_rate": 9.137777777777778e-06,
      "loss": 0.0976,
      "step": 6110
    },
    {
      "epoch": 3.2640000000000002,
      "grad_norm": 2.0818498134613037,
      "learning_rate": 9.12e-06,
      "loss": 0.0952,
      "step": 6120
    },
    {
      "epoch": 3.2693333333333334,
      "grad_norm": 3.130889415740967,
      "learning_rate": 9.102222222222224e-06,
      "loss": 0.101,
      "step": 6130
    },
    {
      "epoch": 3.2746666666666666,
      "grad_norm": 1.6197556257247925,
      "learning_rate": 9.084444444444446e-06,
      "loss": 0.1335,
      "step": 6140
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 2.963686466217041,
      "learning_rate": 9.066666666666667e-06,
      "loss": 0.1031,
      "step": 6150
    },
    {
      "epoch": 3.2853333333333334,
      "grad_norm": 2.7837107181549072,
      "learning_rate": 9.048888888888888e-06,
      "loss": 0.0906,
      "step": 6160
    },
    {
      "epoch": 3.2906666666666666,
      "grad_norm": 3.164158344268799,
      "learning_rate": 9.031111111111111e-06,
      "loss": 0.0821,
      "step": 6170
    },
    {
      "epoch": 3.296,
      "grad_norm": 1.6210473775863647,
      "learning_rate": 9.013333333333334e-06,
      "loss": 0.1008,
      "step": 6180
    },
    {
      "epoch": 3.3013333333333335,
      "grad_norm": 2.7639784812927246,
      "learning_rate": 8.995555555555557e-06,
      "loss": 0.0932,
      "step": 6190
    },
    {
      "epoch": 3.3066666666666666,
      "grad_norm": 1.8442691564559937,
      "learning_rate": 8.977777777777778e-06,
      "loss": 0.1027,
      "step": 6200
    },
    {
      "epoch": 3.312,
      "grad_norm": 4.307969570159912,
      "learning_rate": 8.96e-06,
      "loss": 0.0894,
      "step": 6210
    },
    {
      "epoch": 3.3173333333333335,
      "grad_norm": 3.0025086402893066,
      "learning_rate": 8.942222222222223e-06,
      "loss": 0.1034,
      "step": 6220
    },
    {
      "epoch": 3.3226666666666667,
      "grad_norm": 2.1617398262023926,
      "learning_rate": 8.924444444444446e-06,
      "loss": 0.0916,
      "step": 6230
    },
    {
      "epoch": 3.328,
      "grad_norm": 2.278290033340454,
      "learning_rate": 8.906666666666667e-06,
      "loss": 0.0937,
      "step": 6240
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 1.9854764938354492,
      "learning_rate": 8.888888888888888e-06,
      "loss": 0.0758,
      "step": 6250
    },
    {
      "epoch": 3.3386666666666667,
      "grad_norm": 5.0201497077941895,
      "learning_rate": 8.871111111111111e-06,
      "loss": 0.0985,
      "step": 6260
    },
    {
      "epoch": 3.344,
      "grad_norm": 3.4725427627563477,
      "learning_rate": 8.853333333333334e-06,
      "loss": 0.117,
      "step": 6270
    },
    {
      "epoch": 3.3493333333333335,
      "grad_norm": 3.752044916152954,
      "learning_rate": 8.835555555555557e-06,
      "loss": 0.1291,
      "step": 6280
    },
    {
      "epoch": 3.3546666666666667,
      "grad_norm": 3.9937474727630615,
      "learning_rate": 8.817777777777778e-06,
      "loss": 0.0923,
      "step": 6290
    },
    {
      "epoch": 3.36,
      "grad_norm": 2.0071747303009033,
      "learning_rate": 8.8e-06,
      "loss": 0.104,
      "step": 6300
    },
    {
      "epoch": 3.3653333333333335,
      "grad_norm": 1.6824272871017456,
      "learning_rate": 8.782222222222223e-06,
      "loss": 0.1006,
      "step": 6310
    },
    {
      "epoch": 3.3706666666666667,
      "grad_norm": 2.334505558013916,
      "learning_rate": 8.764444444444446e-06,
      "loss": 0.0948,
      "step": 6320
    },
    {
      "epoch": 3.376,
      "grad_norm": 2.3608133792877197,
      "learning_rate": 8.746666666666667e-06,
      "loss": 0.088,
      "step": 6330
    },
    {
      "epoch": 3.3813333333333335,
      "grad_norm": 1.679175853729248,
      "learning_rate": 8.72888888888889e-06,
      "loss": 0.081,
      "step": 6340
    },
    {
      "epoch": 3.3866666666666667,
      "grad_norm": 1.4396603107452393,
      "learning_rate": 8.711111111111111e-06,
      "loss": 0.0851,
      "step": 6350
    },
    {
      "epoch": 3.392,
      "grad_norm": 3.041518211364746,
      "learning_rate": 8.693333333333334e-06,
      "loss": 0.0997,
      "step": 6360
    },
    {
      "epoch": 3.397333333333333,
      "grad_norm": 1.9194360971450806,
      "learning_rate": 8.675555555555556e-06,
      "loss": 0.1042,
      "step": 6370
    },
    {
      "epoch": 3.4026666666666667,
      "grad_norm": 4.031148910522461,
      "learning_rate": 8.657777777777778e-06,
      "loss": 0.1326,
      "step": 6380
    },
    {
      "epoch": 3.408,
      "grad_norm": 3.1380765438079834,
      "learning_rate": 8.64e-06,
      "loss": 0.1065,
      "step": 6390
    },
    {
      "epoch": 3.413333333333333,
      "grad_norm": 3.0146210193634033,
      "learning_rate": 8.622222222222223e-06,
      "loss": 0.1077,
      "step": 6400
    },
    {
      "epoch": 3.4186666666666667,
      "grad_norm": 3.0500967502593994,
      "learning_rate": 8.604444444444446e-06,
      "loss": 0.0901,
      "step": 6410
    },
    {
      "epoch": 3.424,
      "grad_norm": 2.603182554244995,
      "learning_rate": 8.586666666666667e-06,
      "loss": 0.103,
      "step": 6420
    },
    {
      "epoch": 3.429333333333333,
      "grad_norm": 2.095453977584839,
      "learning_rate": 8.56888888888889e-06,
      "loss": 0.097,
      "step": 6430
    },
    {
      "epoch": 3.4346666666666668,
      "grad_norm": 2.8609912395477295,
      "learning_rate": 8.551111111111112e-06,
      "loss": 0.1184,
      "step": 6440
    },
    {
      "epoch": 3.44,
      "grad_norm": 3.1966371536254883,
      "learning_rate": 8.533333333333335e-06,
      "loss": 0.1025,
      "step": 6450
    },
    {
      "epoch": 3.445333333333333,
      "grad_norm": 2.3902969360351562,
      "learning_rate": 8.515555555555556e-06,
      "loss": 0.0885,
      "step": 6460
    },
    {
      "epoch": 3.4506666666666668,
      "grad_norm": 2.8605217933654785,
      "learning_rate": 8.497777777777777e-06,
      "loss": 0.0805,
      "step": 6470
    },
    {
      "epoch": 3.456,
      "grad_norm": 3.3562753200531006,
      "learning_rate": 8.48e-06,
      "loss": 0.1168,
      "step": 6480
    },
    {
      "epoch": 3.461333333333333,
      "grad_norm": 2.3446097373962402,
      "learning_rate": 8.462222222222223e-06,
      "loss": 0.0781,
      "step": 6490
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 3.5674734115600586,
      "learning_rate": 8.444444444444446e-06,
      "loss": 0.1145,
      "step": 6500
    },
    {
      "epoch": 3.472,
      "grad_norm": 3.355426073074341,
      "learning_rate": 8.426666666666667e-06,
      "loss": 0.0792,
      "step": 6510
    },
    {
      "epoch": 3.477333333333333,
      "grad_norm": 3.9759936332702637,
      "learning_rate": 8.40888888888889e-06,
      "loss": 0.0873,
      "step": 6520
    },
    {
      "epoch": 3.482666666666667,
      "grad_norm": 2.5477640628814697,
      "learning_rate": 8.391111111111112e-06,
      "loss": 0.1086,
      "step": 6530
    },
    {
      "epoch": 3.488,
      "grad_norm": 1.6126012802124023,
      "learning_rate": 8.373333333333335e-06,
      "loss": 0.1022,
      "step": 6540
    },
    {
      "epoch": 3.493333333333333,
      "grad_norm": 1.9733190536499023,
      "learning_rate": 8.355555555555556e-06,
      "loss": 0.0916,
      "step": 6550
    },
    {
      "epoch": 3.498666666666667,
      "grad_norm": 3.77982234954834,
      "learning_rate": 8.337777777777777e-06,
      "loss": 0.0906,
      "step": 6560
    },
    {
      "epoch": 3.504,
      "grad_norm": 2.9487526416778564,
      "learning_rate": 8.32e-06,
      "loss": 0.1118,
      "step": 6570
    },
    {
      "epoch": 3.509333333333333,
      "grad_norm": 3.444317102432251,
      "learning_rate": 8.302222222222223e-06,
      "loss": 0.0921,
      "step": 6580
    },
    {
      "epoch": 3.514666666666667,
      "grad_norm": 3.2643332481384277,
      "learning_rate": 8.284444444444446e-06,
      "loss": 0.1139,
      "step": 6590
    },
    {
      "epoch": 3.52,
      "grad_norm": 2.314826488494873,
      "learning_rate": 8.266666666666667e-06,
      "loss": 0.1156,
      "step": 6600
    },
    {
      "epoch": 3.525333333333333,
      "grad_norm": 3.8405227661132812,
      "learning_rate": 8.24888888888889e-06,
      "loss": 0.109,
      "step": 6610
    },
    {
      "epoch": 3.530666666666667,
      "grad_norm": 2.890455961227417,
      "learning_rate": 8.231111111111112e-06,
      "loss": 0.0969,
      "step": 6620
    },
    {
      "epoch": 3.536,
      "grad_norm": 2.689349412918091,
      "learning_rate": 8.213333333333335e-06,
      "loss": 0.1008,
      "step": 6630
    },
    {
      "epoch": 3.541333333333333,
      "grad_norm": 1.7544970512390137,
      "learning_rate": 8.195555555555556e-06,
      "loss": 0.0976,
      "step": 6640
    },
    {
      "epoch": 3.546666666666667,
      "grad_norm": 2.210378885269165,
      "learning_rate": 8.177777777777779e-06,
      "loss": 0.0963,
      "step": 6650
    },
    {
      "epoch": 3.552,
      "grad_norm": 2.0065088272094727,
      "learning_rate": 8.16e-06,
      "loss": 0.1048,
      "step": 6660
    },
    {
      "epoch": 3.5573333333333332,
      "grad_norm": 1.9585292339324951,
      "learning_rate": 8.142222222222223e-06,
      "loss": 0.102,
      "step": 6670
    },
    {
      "epoch": 3.562666666666667,
      "grad_norm": 2.5662362575531006,
      "learning_rate": 8.124444444444445e-06,
      "loss": 0.1019,
      "step": 6680
    },
    {
      "epoch": 3.568,
      "grad_norm": 2.247817277908325,
      "learning_rate": 8.106666666666666e-06,
      "loss": 0.0953,
      "step": 6690
    },
    {
      "epoch": 3.5733333333333333,
      "grad_norm": 2.949094533920288,
      "learning_rate": 8.08888888888889e-06,
      "loss": 0.0705,
      "step": 6700
    },
    {
      "epoch": 3.578666666666667,
      "grad_norm": 2.7169761657714844,
      "learning_rate": 8.071111111111112e-06,
      "loss": 0.1171,
      "step": 6710
    },
    {
      "epoch": 3.584,
      "grad_norm": 2.7244932651519775,
      "learning_rate": 8.053333333333335e-06,
      "loss": 0.132,
      "step": 6720
    },
    {
      "epoch": 3.5893333333333333,
      "grad_norm": 1.9780471324920654,
      "learning_rate": 8.035555555555556e-06,
      "loss": 0.1103,
      "step": 6730
    },
    {
      "epoch": 3.594666666666667,
      "grad_norm": 2.035146713256836,
      "learning_rate": 8.017777777777779e-06,
      "loss": 0.0933,
      "step": 6740
    },
    {
      "epoch": 3.6,
      "grad_norm": 3.4130988121032715,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.0903,
      "step": 6750
    },
    {
      "epoch": 3.6053333333333333,
      "grad_norm": 3.467355966567993,
      "learning_rate": 7.982222222222224e-06,
      "loss": 0.1274,
      "step": 6760
    },
    {
      "epoch": 3.610666666666667,
      "grad_norm": 3.0650558471679688,
      "learning_rate": 7.964444444444445e-06,
      "loss": 0.1417,
      "step": 6770
    },
    {
      "epoch": 3.616,
      "grad_norm": 1.966594934463501,
      "learning_rate": 7.946666666666666e-06,
      "loss": 0.1017,
      "step": 6780
    },
    {
      "epoch": 3.6213333333333333,
      "grad_norm": 1.2236440181732178,
      "learning_rate": 7.928888888888889e-06,
      "loss": 0.0839,
      "step": 6790
    },
    {
      "epoch": 3.626666666666667,
      "grad_norm": 1.5390713214874268,
      "learning_rate": 7.911111111111112e-06,
      "loss": 0.0856,
      "step": 6800
    },
    {
      "epoch": 3.632,
      "grad_norm": 3.179536819458008,
      "learning_rate": 7.893333333333335e-06,
      "loss": 0.0846,
      "step": 6810
    },
    {
      "epoch": 3.6373333333333333,
      "grad_norm": 2.912862777709961,
      "learning_rate": 7.875555555555556e-06,
      "loss": 0.1199,
      "step": 6820
    },
    {
      "epoch": 3.642666666666667,
      "grad_norm": 3.3249099254608154,
      "learning_rate": 7.857777777777778e-06,
      "loss": 0.0688,
      "step": 6830
    },
    {
      "epoch": 3.648,
      "grad_norm": 4.032297134399414,
      "learning_rate": 7.840000000000001e-06,
      "loss": 0.1153,
      "step": 6840
    },
    {
      "epoch": 3.6533333333333333,
      "grad_norm": 4.019280433654785,
      "learning_rate": 7.822222222222224e-06,
      "loss": 0.0962,
      "step": 6850
    },
    {
      "epoch": 3.6586666666666665,
      "grad_norm": 3.08276629447937,
      "learning_rate": 7.804444444444445e-06,
      "loss": 0.125,
      "step": 6860
    },
    {
      "epoch": 3.664,
      "grad_norm": 2.4244039058685303,
      "learning_rate": 7.786666666666666e-06,
      "loss": 0.0989,
      "step": 6870
    },
    {
      "epoch": 3.6693333333333333,
      "grad_norm": 2.05977463722229,
      "learning_rate": 7.768888888888889e-06,
      "loss": 0.0828,
      "step": 6880
    },
    {
      "epoch": 3.6746666666666665,
      "grad_norm": 1.8426988124847412,
      "learning_rate": 7.751111111111112e-06,
      "loss": 0.0993,
      "step": 6890
    },
    {
      "epoch": 3.68,
      "grad_norm": 1.6717811822891235,
      "learning_rate": 7.733333333333334e-06,
      "loss": 0.1039,
      "step": 6900
    },
    {
      "epoch": 3.6853333333333333,
      "grad_norm": 2.6610891819000244,
      "learning_rate": 7.715555555555555e-06,
      "loss": 0.096,
      "step": 6910
    },
    {
      "epoch": 3.6906666666666665,
      "grad_norm": 2.395117998123169,
      "learning_rate": 7.697777777777778e-06,
      "loss": 0.1096,
      "step": 6920
    },
    {
      "epoch": 3.6959999999999997,
      "grad_norm": 2.698321580886841,
      "learning_rate": 7.680000000000001e-06,
      "loss": 0.1188,
      "step": 6930
    },
    {
      "epoch": 3.7013333333333334,
      "grad_norm": 2.993983745574951,
      "learning_rate": 7.662222222222224e-06,
      "loss": 0.0999,
      "step": 6940
    },
    {
      "epoch": 3.7066666666666666,
      "grad_norm": 3.7191238403320312,
      "learning_rate": 7.644444444444445e-06,
      "loss": 0.1183,
      "step": 6950
    },
    {
      "epoch": 3.7119999999999997,
      "grad_norm": 3.1088526248931885,
      "learning_rate": 7.626666666666668e-06,
      "loss": 0.0998,
      "step": 6960
    },
    {
      "epoch": 3.7173333333333334,
      "grad_norm": 2.8341691493988037,
      "learning_rate": 7.608888888888889e-06,
      "loss": 0.0977,
      "step": 6970
    },
    {
      "epoch": 3.7226666666666666,
      "grad_norm": 2.4281811714172363,
      "learning_rate": 7.5911111111111115e-06,
      "loss": 0.0965,
      "step": 6980
    },
    {
      "epoch": 3.7279999999999998,
      "grad_norm": 2.227375030517578,
      "learning_rate": 7.573333333333333e-06,
      "loss": 0.1159,
      "step": 6990
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 4.010915279388428,
      "learning_rate": 7.555555555555556e-06,
      "loss": 0.12,
      "step": 7000
    },
    {
      "epoch": 3.7386666666666666,
      "grad_norm": 2.2501914501190186,
      "learning_rate": 7.537777777777778e-06,
      "loss": 0.1369,
      "step": 7010
    },
    {
      "epoch": 3.7439999999999998,
      "grad_norm": 2.3327133655548096,
      "learning_rate": 7.520000000000001e-06,
      "loss": 0.0804,
      "step": 7020
    },
    {
      "epoch": 3.7493333333333334,
      "grad_norm": 4.310269355773926,
      "learning_rate": 7.502222222222223e-06,
      "loss": 0.1182,
      "step": 7030
    },
    {
      "epoch": 3.7546666666666666,
      "grad_norm": 2.525419235229492,
      "learning_rate": 7.4844444444444455e-06,
      "loss": 0.0867,
      "step": 7040
    },
    {
      "epoch": 3.76,
      "grad_norm": 2.950848340988159,
      "learning_rate": 7.4666666666666675e-06,
      "loss": 0.0922,
      "step": 7050
    },
    {
      "epoch": 3.7653333333333334,
      "grad_norm": 4.915851593017578,
      "learning_rate": 7.44888888888889e-06,
      "loss": 0.1293,
      "step": 7060
    },
    {
      "epoch": 3.7706666666666666,
      "grad_norm": 3.840559959411621,
      "learning_rate": 7.431111111111111e-06,
      "loss": 0.1205,
      "step": 7070
    },
    {
      "epoch": 3.776,
      "grad_norm": 1.6051186323165894,
      "learning_rate": 7.413333333333333e-06,
      "loss": 0.1242,
      "step": 7080
    },
    {
      "epoch": 3.7813333333333334,
      "grad_norm": 1.6050946712493896,
      "learning_rate": 7.395555555555556e-06,
      "loss": 0.1134,
      "step": 7090
    },
    {
      "epoch": 3.7866666666666666,
      "grad_norm": 2.7960867881774902,
      "learning_rate": 7.377777777777778e-06,
      "loss": 0.117,
      "step": 7100
    },
    {
      "epoch": 3.792,
      "grad_norm": 2.9954833984375,
      "learning_rate": 7.360000000000001e-06,
      "loss": 0.1311,
      "step": 7110
    },
    {
      "epoch": 3.7973333333333334,
      "grad_norm": 1.7935731410980225,
      "learning_rate": 7.342222222222223e-06,
      "loss": 0.0959,
      "step": 7120
    },
    {
      "epoch": 3.8026666666666666,
      "grad_norm": 3.5057315826416016,
      "learning_rate": 7.324444444444445e-06,
      "loss": 0.1089,
      "step": 7130
    },
    {
      "epoch": 3.808,
      "grad_norm": 3.081242561340332,
      "learning_rate": 7.306666666666667e-06,
      "loss": 0.1007,
      "step": 7140
    },
    {
      "epoch": 3.8133333333333335,
      "grad_norm": 2.496211528778076,
      "learning_rate": 7.28888888888889e-06,
      "loss": 0.1052,
      "step": 7150
    },
    {
      "epoch": 3.8186666666666667,
      "grad_norm": 3.0785067081451416,
      "learning_rate": 7.271111111111112e-06,
      "loss": 0.0924,
      "step": 7160
    },
    {
      "epoch": 3.824,
      "grad_norm": 2.086514711380005,
      "learning_rate": 7.253333333333335e-06,
      "loss": 0.0992,
      "step": 7170
    },
    {
      "epoch": 3.8293333333333335,
      "grad_norm": 2.577737808227539,
      "learning_rate": 7.235555555555556e-06,
      "loss": 0.1144,
      "step": 7180
    },
    {
      "epoch": 3.8346666666666667,
      "grad_norm": 4.187099456787109,
      "learning_rate": 7.217777777777778e-06,
      "loss": 0.1012,
      "step": 7190
    },
    {
      "epoch": 3.84,
      "grad_norm": 2.4733986854553223,
      "learning_rate": 7.2000000000000005e-06,
      "loss": 0.1006,
      "step": 7200
    },
    {
      "epoch": 3.8453333333333335,
      "grad_norm": 2.603299856185913,
      "learning_rate": 7.1822222222222224e-06,
      "loss": 0.1219,
      "step": 7210
    },
    {
      "epoch": 3.8506666666666667,
      "grad_norm": 2.369760751724243,
      "learning_rate": 7.164444444444445e-06,
      "loss": 0.1037,
      "step": 7220
    },
    {
      "epoch": 3.856,
      "grad_norm": 2.7949111461639404,
      "learning_rate": 7.146666666666667e-06,
      "loss": 0.1443,
      "step": 7230
    },
    {
      "epoch": 3.8613333333333335,
      "grad_norm": 3.6506495475769043,
      "learning_rate": 7.12888888888889e-06,
      "loss": 0.1105,
      "step": 7240
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 2.654771089553833,
      "learning_rate": 7.111111111111112e-06,
      "loss": 0.123,
      "step": 7250
    },
    {
      "epoch": 3.872,
      "grad_norm": 1.7529010772705078,
      "learning_rate": 7.093333333333335e-06,
      "loss": 0.087,
      "step": 7260
    },
    {
      "epoch": 3.8773333333333335,
      "grad_norm": 2.3522658348083496,
      "learning_rate": 7.0755555555555565e-06,
      "loss": 0.1207,
      "step": 7270
    },
    {
      "epoch": 3.8826666666666667,
      "grad_norm": 0.9248053431510925,
      "learning_rate": 7.057777777777778e-06,
      "loss": 0.0793,
      "step": 7280
    },
    {
      "epoch": 3.888,
      "grad_norm": 2.003427267074585,
      "learning_rate": 7.04e-06,
      "loss": 0.0994,
      "step": 7290
    },
    {
      "epoch": 3.8933333333333335,
      "grad_norm": 3.9124414920806885,
      "learning_rate": 7.022222222222222e-06,
      "loss": 0.1288,
      "step": 7300
    },
    {
      "epoch": 3.8986666666666667,
      "grad_norm": 3.4036850929260254,
      "learning_rate": 7.004444444444445e-06,
      "loss": 0.1051,
      "step": 7310
    },
    {
      "epoch": 3.904,
      "grad_norm": 3.60709810256958,
      "learning_rate": 6.986666666666667e-06,
      "loss": 0.1155,
      "step": 7320
    },
    {
      "epoch": 3.9093333333333335,
      "grad_norm": 2.6742448806762695,
      "learning_rate": 6.96888888888889e-06,
      "loss": 0.1061,
      "step": 7330
    },
    {
      "epoch": 3.9146666666666667,
      "grad_norm": 3.995893955230713,
      "learning_rate": 6.951111111111112e-06,
      "loss": 0.0768,
      "step": 7340
    },
    {
      "epoch": 3.92,
      "grad_norm": 2.215306282043457,
      "learning_rate": 6.9333333333333344e-06,
      "loss": 0.0789,
      "step": 7350
    },
    {
      "epoch": 3.9253333333333336,
      "grad_norm": 3.1047134399414062,
      "learning_rate": 6.915555555555556e-06,
      "loss": 0.0934,
      "step": 7360
    },
    {
      "epoch": 3.9306666666666668,
      "grad_norm": 3.2998313903808594,
      "learning_rate": 6.897777777777779e-06,
      "loss": 0.1167,
      "step": 7370
    },
    {
      "epoch": 3.936,
      "grad_norm": 2.648094654083252,
      "learning_rate": 6.88e-06,
      "loss": 0.0762,
      "step": 7380
    },
    {
      "epoch": 3.9413333333333336,
      "grad_norm": 2.316305637359619,
      "learning_rate": 6.862222222222222e-06,
      "loss": 0.1244,
      "step": 7390
    },
    {
      "epoch": 3.9466666666666668,
      "grad_norm": 1.5222088098526,
      "learning_rate": 6.844444444444445e-06,
      "loss": 0.1054,
      "step": 7400
    },
    {
      "epoch": 3.952,
      "grad_norm": 3.478053331375122,
      "learning_rate": 6.826666666666667e-06,
      "loss": 0.1202,
      "step": 7410
    },
    {
      "epoch": 3.9573333333333336,
      "grad_norm": 2.590526819229126,
      "learning_rate": 6.80888888888889e-06,
      "loss": 0.1267,
      "step": 7420
    },
    {
      "epoch": 3.962666666666667,
      "grad_norm": 1.4463313817977905,
      "learning_rate": 6.7911111111111115e-06,
      "loss": 0.1198,
      "step": 7430
    },
    {
      "epoch": 3.968,
      "grad_norm": 1.5336976051330566,
      "learning_rate": 6.773333333333334e-06,
      "loss": 0.0767,
      "step": 7440
    },
    {
      "epoch": 3.9733333333333336,
      "grad_norm": 1.9651087522506714,
      "learning_rate": 6.755555555555556e-06,
      "loss": 0.1033,
      "step": 7450
    },
    {
      "epoch": 3.978666666666667,
      "grad_norm": 2.506377935409546,
      "learning_rate": 6.737777777777779e-06,
      "loss": 0.1034,
      "step": 7460
    },
    {
      "epoch": 3.984,
      "grad_norm": 2.366981029510498,
      "learning_rate": 6.720000000000001e-06,
      "loss": 0.1276,
      "step": 7470
    },
    {
      "epoch": 3.989333333333333,
      "grad_norm": 2.1251282691955566,
      "learning_rate": 6.702222222222224e-06,
      "loss": 0.1057,
      "step": 7480
    },
    {
      "epoch": 3.994666666666667,
      "grad_norm": 3.692077159881592,
      "learning_rate": 6.684444444444445e-06,
      "loss": 0.1151,
      "step": 7490
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.7635507583618164,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.0956,
      "step": 7500
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.962,
      "eval_loss": 0.12631645798683167,
      "eval_runtime": 47.3905,
      "eval_samples_per_second": 422.025,
      "eval_steps_per_second": 4.41,
      "step": 7500
    }
  ],
  "logging_steps": 10,
  "max_steps": 11250,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.848937951232e+16,
  "train_batch_size": 96,
  "trial_name": null,
  "trial_params": null
}
