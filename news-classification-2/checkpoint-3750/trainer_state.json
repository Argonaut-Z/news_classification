{
  "best_metric": 0.95755,
  "best_model_checkpoint": "news-classification-2/checkpoint-3750",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 3750,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 2.340062379837036,
      "learning_rate": 1.9982222222222224e-05,
      "loss": 2.4833,
      "step": 10
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 2.3409371376037598,
      "learning_rate": 1.9964444444444447e-05,
      "loss": 2.1901,
      "step": 20
    },
    {
      "epoch": 0.016,
      "grad_norm": 2.1555848121643066,
      "learning_rate": 1.9946666666666667e-05,
      "loss": 1.9212,
      "step": 30
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 2.0559258460998535,
      "learning_rate": 1.992888888888889e-05,
      "loss": 1.7008,
      "step": 40
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 2.086430549621582,
      "learning_rate": 1.9911111111111112e-05,
      "loss": 1.4714,
      "step": 50
    },
    {
      "epoch": 0.032,
      "grad_norm": 1.9018769264221191,
      "learning_rate": 1.9893333333333335e-05,
      "loss": 1.3258,
      "step": 60
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 2.6979124546051025,
      "learning_rate": 1.9875555555555558e-05,
      "loss": 1.2274,
      "step": 70
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 2.662576675415039,
      "learning_rate": 1.985777777777778e-05,
      "loss": 1.041,
      "step": 80
    },
    {
      "epoch": 0.048,
      "grad_norm": 1.9613078832626343,
      "learning_rate": 1.9840000000000003e-05,
      "loss": 0.9608,
      "step": 90
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 2.3540279865264893,
      "learning_rate": 1.9822222222222226e-05,
      "loss": 0.867,
      "step": 100
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 2.195451259613037,
      "learning_rate": 1.9804444444444445e-05,
      "loss": 0.805,
      "step": 110
    },
    {
      "epoch": 0.064,
      "grad_norm": 2.2827296257019043,
      "learning_rate": 1.9786666666666668e-05,
      "loss": 0.717,
      "step": 120
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 2.4951789379119873,
      "learning_rate": 1.976888888888889e-05,
      "loss": 0.6991,
      "step": 130
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 2.4580349922180176,
      "learning_rate": 1.9751111111111114e-05,
      "loss": 0.6415,
      "step": 140
    },
    {
      "epoch": 0.08,
      "grad_norm": 2.92366099357605,
      "learning_rate": 1.9733333333333336e-05,
      "loss": 0.5664,
      "step": 150
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 2.836576461791992,
      "learning_rate": 1.9715555555555556e-05,
      "loss": 0.5436,
      "step": 160
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 2.117300510406494,
      "learning_rate": 1.969777777777778e-05,
      "loss": 0.5142,
      "step": 170
    },
    {
      "epoch": 0.096,
      "grad_norm": 1.8074114322662354,
      "learning_rate": 1.968e-05,
      "loss": 0.4426,
      "step": 180
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 2.525524139404297,
      "learning_rate": 1.9662222222222224e-05,
      "loss": 0.5051,
      "step": 190
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 1.9512096643447876,
      "learning_rate": 1.9644444444444447e-05,
      "loss": 0.429,
      "step": 200
    },
    {
      "epoch": 0.112,
      "grad_norm": 2.3497812747955322,
      "learning_rate": 1.9626666666666666e-05,
      "loss": 0.4546,
      "step": 210
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 2.7469656467437744,
      "learning_rate": 1.960888888888889e-05,
      "loss": 0.4736,
      "step": 220
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 2.4587910175323486,
      "learning_rate": 1.9591111111111112e-05,
      "loss": 0.4574,
      "step": 230
    },
    {
      "epoch": 0.128,
      "grad_norm": 2.678929090499878,
      "learning_rate": 1.9573333333333335e-05,
      "loss": 0.4563,
      "step": 240
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 2.524336099624634,
      "learning_rate": 1.9555555555555557e-05,
      "loss": 0.4102,
      "step": 250
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 3.5036823749542236,
      "learning_rate": 1.953777777777778e-05,
      "loss": 0.3958,
      "step": 260
    },
    {
      "epoch": 0.144,
      "grad_norm": 2.847472906112671,
      "learning_rate": 1.9520000000000003e-05,
      "loss": 0.3817,
      "step": 270
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 2.873018503189087,
      "learning_rate": 1.9502222222222226e-05,
      "loss": 0.4008,
      "step": 280
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 2.7622461318969727,
      "learning_rate": 1.9484444444444445e-05,
      "loss": 0.3814,
      "step": 290
    },
    {
      "epoch": 0.16,
      "grad_norm": 3.2881574630737305,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 0.3392,
      "step": 300
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 2.1540071964263916,
      "learning_rate": 1.944888888888889e-05,
      "loss": 0.3804,
      "step": 310
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 2.4652674198150635,
      "learning_rate": 1.9431111111111113e-05,
      "loss": 0.3656,
      "step": 320
    },
    {
      "epoch": 0.176,
      "grad_norm": 2.6359810829162598,
      "learning_rate": 1.9413333333333336e-05,
      "loss": 0.3478,
      "step": 330
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 1.7288661003112793,
      "learning_rate": 1.9395555555555555e-05,
      "loss": 0.345,
      "step": 340
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 4.310972690582275,
      "learning_rate": 1.9377777777777778e-05,
      "loss": 0.3711,
      "step": 350
    },
    {
      "epoch": 0.192,
      "grad_norm": 2.8951938152313232,
      "learning_rate": 1.936e-05,
      "loss": 0.3385,
      "step": 360
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 2.653503656387329,
      "learning_rate": 1.9342222222222224e-05,
      "loss": 0.3717,
      "step": 370
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 1.7922152280807495,
      "learning_rate": 1.9324444444444447e-05,
      "loss": 0.3024,
      "step": 380
    },
    {
      "epoch": 0.208,
      "grad_norm": 2.9919824600219727,
      "learning_rate": 1.930666666666667e-05,
      "loss": 0.3837,
      "step": 390
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 4.290222644805908,
      "learning_rate": 1.928888888888889e-05,
      "loss": 0.3877,
      "step": 400
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 1.981816053390503,
      "learning_rate": 1.9271111111111115e-05,
      "loss": 0.3121,
      "step": 410
    },
    {
      "epoch": 0.224,
      "grad_norm": 2.787489652633667,
      "learning_rate": 1.9253333333333334e-05,
      "loss": 0.29,
      "step": 420
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 2.284292221069336,
      "learning_rate": 1.9235555555555557e-05,
      "loss": 0.2845,
      "step": 430
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 2.2889513969421387,
      "learning_rate": 1.921777777777778e-05,
      "loss": 0.302,
      "step": 440
    },
    {
      "epoch": 0.24,
      "grad_norm": 3.1597371101379395,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.3186,
      "step": 450
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 3.7679572105407715,
      "learning_rate": 1.9182222222222225e-05,
      "loss": 0.3009,
      "step": 460
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 3.072195529937744,
      "learning_rate": 1.9164444444444445e-05,
      "loss": 0.3224,
      "step": 470
    },
    {
      "epoch": 0.256,
      "grad_norm": 2.4539129734039307,
      "learning_rate": 1.9146666666666667e-05,
      "loss": 0.2782,
      "step": 480
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 2.8653109073638916,
      "learning_rate": 1.912888888888889e-05,
      "loss": 0.2589,
      "step": 490
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 2.474698305130005,
      "learning_rate": 1.9111111111111113e-05,
      "loss": 0.2742,
      "step": 500
    },
    {
      "epoch": 0.272,
      "grad_norm": 1.7876826524734497,
      "learning_rate": 1.9093333333333336e-05,
      "loss": 0.2879,
      "step": 510
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 1.8565173149108887,
      "learning_rate": 1.9075555555555555e-05,
      "loss": 0.2343,
      "step": 520
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 2.3621513843536377,
      "learning_rate": 1.9057777777777778e-05,
      "loss": 0.2787,
      "step": 530
    },
    {
      "epoch": 0.288,
      "grad_norm": 2.279662609100342,
      "learning_rate": 1.904e-05,
      "loss": 0.298,
      "step": 540
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 2.2688517570495605,
      "learning_rate": 1.9022222222222223e-05,
      "loss": 0.2551,
      "step": 550
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 2.961923122406006,
      "learning_rate": 1.9004444444444446e-05,
      "loss": 0.2878,
      "step": 560
    },
    {
      "epoch": 0.304,
      "grad_norm": 3.4555869102478027,
      "learning_rate": 1.898666666666667e-05,
      "loss": 0.2674,
      "step": 570
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 3.4741828441619873,
      "learning_rate": 1.896888888888889e-05,
      "loss": 0.2678,
      "step": 580
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 2.9928531646728516,
      "learning_rate": 1.8951111111111115e-05,
      "loss": 0.2409,
      "step": 590
    },
    {
      "epoch": 0.32,
      "grad_norm": 3.1215834617614746,
      "learning_rate": 1.8933333333333334e-05,
      "loss": 0.2827,
      "step": 600
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 2.1197874546051025,
      "learning_rate": 1.8915555555555557e-05,
      "loss": 0.2862,
      "step": 610
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 1.7706376314163208,
      "learning_rate": 1.889777777777778e-05,
      "loss": 0.2415,
      "step": 620
    },
    {
      "epoch": 0.336,
      "grad_norm": 1.7263665199279785,
      "learning_rate": 1.8880000000000002e-05,
      "loss": 0.2782,
      "step": 630
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 2.8460729122161865,
      "learning_rate": 1.8862222222222225e-05,
      "loss": 0.3126,
      "step": 640
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 3.8757948875427246,
      "learning_rate": 1.8844444444444444e-05,
      "loss": 0.2266,
      "step": 650
    },
    {
      "epoch": 0.352,
      "grad_norm": 2.9246137142181396,
      "learning_rate": 1.8826666666666667e-05,
      "loss": 0.2851,
      "step": 660
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 2.8725013732910156,
      "learning_rate": 1.880888888888889e-05,
      "loss": 0.2669,
      "step": 670
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 2.025454044342041,
      "learning_rate": 1.8791111111111113e-05,
      "loss": 0.2901,
      "step": 680
    },
    {
      "epoch": 0.368,
      "grad_norm": 2.534980535507202,
      "learning_rate": 1.8773333333333335e-05,
      "loss": 0.2823,
      "step": 690
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 2.8048534393310547,
      "learning_rate": 1.8755555555555558e-05,
      "loss": 0.2384,
      "step": 700
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 2.4496400356292725,
      "learning_rate": 1.8737777777777778e-05,
      "loss": 0.285,
      "step": 710
    },
    {
      "epoch": 0.384,
      "grad_norm": 1.9532679319381714,
      "learning_rate": 1.8720000000000004e-05,
      "loss": 0.1976,
      "step": 720
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 3.9647562503814697,
      "learning_rate": 1.8702222222222223e-05,
      "loss": 0.2677,
      "step": 730
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 3.576298236846924,
      "learning_rate": 1.8684444444444446e-05,
      "loss": 0.2581,
      "step": 740
    },
    {
      "epoch": 0.4,
      "grad_norm": 3.9200901985168457,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.2654,
      "step": 750
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 2.4719555377960205,
      "learning_rate": 1.8648888888888888e-05,
      "loss": 0.2445,
      "step": 760
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 2.23500394821167,
      "learning_rate": 1.8631111111111114e-05,
      "loss": 0.2681,
      "step": 770
    },
    {
      "epoch": 0.416,
      "grad_norm": 3.358092784881592,
      "learning_rate": 1.8613333333333334e-05,
      "loss": 0.2571,
      "step": 780
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 2.4395995140075684,
      "learning_rate": 1.8595555555555556e-05,
      "loss": 0.2871,
      "step": 790
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 2.0386977195739746,
      "learning_rate": 1.857777777777778e-05,
      "loss": 0.2569,
      "step": 800
    },
    {
      "epoch": 0.432,
      "grad_norm": 2.3983476161956787,
      "learning_rate": 1.8560000000000002e-05,
      "loss": 0.2103,
      "step": 810
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 2.9427950382232666,
      "learning_rate": 1.8542222222222225e-05,
      "loss": 0.24,
      "step": 820
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 2.028043508529663,
      "learning_rate": 1.8524444444444444e-05,
      "loss": 0.2415,
      "step": 830
    },
    {
      "epoch": 0.448,
      "grad_norm": 2.403047800064087,
      "learning_rate": 1.8506666666666667e-05,
      "loss": 0.2305,
      "step": 840
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 3.737492084503174,
      "learning_rate": 1.848888888888889e-05,
      "loss": 0.2183,
      "step": 850
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 1.7343499660491943,
      "learning_rate": 1.8471111111111112e-05,
      "loss": 0.2477,
      "step": 860
    },
    {
      "epoch": 0.464,
      "grad_norm": 2.6099650859832764,
      "learning_rate": 1.8453333333333335e-05,
      "loss": 0.2471,
      "step": 870
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 2.934739828109741,
      "learning_rate": 1.8435555555555558e-05,
      "loss": 0.2116,
      "step": 880
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 3.5620553493499756,
      "learning_rate": 1.8417777777777777e-05,
      "loss": 0.2487,
      "step": 890
    },
    {
      "epoch": 0.48,
      "grad_norm": 2.037273645401001,
      "learning_rate": 1.8400000000000003e-05,
      "loss": 0.2105,
      "step": 900
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 1.7175401449203491,
      "learning_rate": 1.8382222222222223e-05,
      "loss": 0.2401,
      "step": 910
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 3.304821729660034,
      "learning_rate": 1.8364444444444446e-05,
      "loss": 0.2362,
      "step": 920
    },
    {
      "epoch": 0.496,
      "grad_norm": 2.1050398349761963,
      "learning_rate": 1.834666666666667e-05,
      "loss": 0.2473,
      "step": 930
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 3.5820703506469727,
      "learning_rate": 1.832888888888889e-05,
      "loss": 0.2546,
      "step": 940
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 4.120608329772949,
      "learning_rate": 1.8311111111111114e-05,
      "loss": 0.2051,
      "step": 950
    },
    {
      "epoch": 0.512,
      "grad_norm": 2.4124093055725098,
      "learning_rate": 1.8293333333333333e-05,
      "loss": 0.213,
      "step": 960
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 3.3322222232818604,
      "learning_rate": 1.8275555555555556e-05,
      "loss": 0.2348,
      "step": 970
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 2.556062698364258,
      "learning_rate": 1.825777777777778e-05,
      "loss": 0.191,
      "step": 980
    },
    {
      "epoch": 0.528,
      "grad_norm": 3.3162920475006104,
      "learning_rate": 1.824e-05,
      "loss": 0.1954,
      "step": 990
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 3.3258445262908936,
      "learning_rate": 1.8222222222222224e-05,
      "loss": 0.2115,
      "step": 1000
    },
    {
      "epoch": 0.5386666666666666,
      "grad_norm": 2.0840632915496826,
      "learning_rate": 1.8204444444444447e-05,
      "loss": 0.2379,
      "step": 1010
    },
    {
      "epoch": 0.544,
      "grad_norm": 2.310572385787964,
      "learning_rate": 1.8186666666666666e-05,
      "loss": 0.1978,
      "step": 1020
    },
    {
      "epoch": 0.5493333333333333,
      "grad_norm": 3.293782949447632,
      "learning_rate": 1.8168888888888893e-05,
      "loss": 0.1668,
      "step": 1030
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 4.34323787689209,
      "learning_rate": 1.8151111111111112e-05,
      "loss": 0.211,
      "step": 1040
    },
    {
      "epoch": 0.56,
      "grad_norm": 3.9913852214813232,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 0.2473,
      "step": 1050
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 4.405982494354248,
      "learning_rate": 1.8115555555555558e-05,
      "loss": 0.2394,
      "step": 1060
    },
    {
      "epoch": 0.5706666666666667,
      "grad_norm": 2.1960771083831787,
      "learning_rate": 1.8097777777777777e-05,
      "loss": 0.2123,
      "step": 1070
    },
    {
      "epoch": 0.576,
      "grad_norm": 3.6924808025360107,
      "learning_rate": 1.8080000000000003e-05,
      "loss": 0.1912,
      "step": 1080
    },
    {
      "epoch": 0.5813333333333334,
      "grad_norm": 1.9034916162490845,
      "learning_rate": 1.8062222222222222e-05,
      "loss": 0.1843,
      "step": 1090
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 3.310030221939087,
      "learning_rate": 1.8044444444444445e-05,
      "loss": 0.2214,
      "step": 1100
    },
    {
      "epoch": 0.592,
      "grad_norm": 2.9158389568328857,
      "learning_rate": 1.8026666666666668e-05,
      "loss": 0.209,
      "step": 1110
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 2.7702107429504395,
      "learning_rate": 1.800888888888889e-05,
      "loss": 0.1999,
      "step": 1120
    },
    {
      "epoch": 0.6026666666666667,
      "grad_norm": 2.4558396339416504,
      "learning_rate": 1.7991111111111114e-05,
      "loss": 0.2504,
      "step": 1130
    },
    {
      "epoch": 0.608,
      "grad_norm": 1.7969404458999634,
      "learning_rate": 1.7973333333333333e-05,
      "loss": 0.2028,
      "step": 1140
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 3.390717029571533,
      "learning_rate": 1.7955555555555556e-05,
      "loss": 0.2005,
      "step": 1150
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 2.6017730236053467,
      "learning_rate": 1.793777777777778e-05,
      "loss": 0.2209,
      "step": 1160
    },
    {
      "epoch": 0.624,
      "grad_norm": 2.357858419418335,
      "learning_rate": 1.792e-05,
      "loss": 0.2234,
      "step": 1170
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 4.677999973297119,
      "learning_rate": 1.7902222222222224e-05,
      "loss": 0.2309,
      "step": 1180
    },
    {
      "epoch": 0.6346666666666667,
      "grad_norm": 1.8672105073928833,
      "learning_rate": 1.7884444444444447e-05,
      "loss": 0.197,
      "step": 1190
    },
    {
      "epoch": 0.64,
      "grad_norm": 3.6874146461486816,
      "learning_rate": 1.7866666666666666e-05,
      "loss": 0.1959,
      "step": 1200
    },
    {
      "epoch": 0.6453333333333333,
      "grad_norm": 4.376802921295166,
      "learning_rate": 1.7848888888888892e-05,
      "loss": 0.2181,
      "step": 1210
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 4.053673267364502,
      "learning_rate": 1.783111111111111e-05,
      "loss": 0.216,
      "step": 1220
    },
    {
      "epoch": 0.656,
      "grad_norm": 4.080118656158447,
      "learning_rate": 1.7813333333333334e-05,
      "loss": 0.2424,
      "step": 1230
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 1.7412792444229126,
      "learning_rate": 1.7795555555555557e-05,
      "loss": 0.1821,
      "step": 1240
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 3.284057140350342,
      "learning_rate": 1.7777777777777777e-05,
      "loss": 0.2298,
      "step": 1250
    },
    {
      "epoch": 0.672,
      "grad_norm": 3.2456586360931396,
      "learning_rate": 1.7760000000000003e-05,
      "loss": 0.1813,
      "step": 1260
    },
    {
      "epoch": 0.6773333333333333,
      "grad_norm": 2.6079695224761963,
      "learning_rate": 1.7742222222222222e-05,
      "loss": 0.2,
      "step": 1270
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 3.006305456161499,
      "learning_rate": 1.7724444444444445e-05,
      "loss": 0.1972,
      "step": 1280
    },
    {
      "epoch": 0.688,
      "grad_norm": 2.8665385246276855,
      "learning_rate": 1.7706666666666668e-05,
      "loss": 0.1762,
      "step": 1290
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 3.0240628719329834,
      "learning_rate": 1.768888888888889e-05,
      "loss": 0.2271,
      "step": 1300
    },
    {
      "epoch": 0.6986666666666667,
      "grad_norm": 3.154467821121216,
      "learning_rate": 1.7671111111111113e-05,
      "loss": 0.1822,
      "step": 1310
    },
    {
      "epoch": 0.704,
      "grad_norm": 2.85445499420166,
      "learning_rate": 1.7653333333333336e-05,
      "loss": 0.2175,
      "step": 1320
    },
    {
      "epoch": 0.7093333333333334,
      "grad_norm": 1.9365614652633667,
      "learning_rate": 1.7635555555555555e-05,
      "loss": 0.1735,
      "step": 1330
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 1.8405073881149292,
      "learning_rate": 1.761777777777778e-05,
      "loss": 0.2431,
      "step": 1340
    },
    {
      "epoch": 0.72,
      "grad_norm": 3.301166534423828,
      "learning_rate": 1.76e-05,
      "loss": 0.1927,
      "step": 1350
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 2.8007845878601074,
      "learning_rate": 1.7582222222222224e-05,
      "loss": 0.2075,
      "step": 1360
    },
    {
      "epoch": 0.7306666666666667,
      "grad_norm": 2.0972020626068115,
      "learning_rate": 1.7564444444444446e-05,
      "loss": 0.2142,
      "step": 1370
    },
    {
      "epoch": 0.736,
      "grad_norm": 2.102510690689087,
      "learning_rate": 1.7546666666666666e-05,
      "loss": 0.2069,
      "step": 1380
    },
    {
      "epoch": 0.7413333333333333,
      "grad_norm": 2.2479143142700195,
      "learning_rate": 1.7528888888888892e-05,
      "loss": 0.1962,
      "step": 1390
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 2.675490379333496,
      "learning_rate": 1.751111111111111e-05,
      "loss": 0.2127,
      "step": 1400
    },
    {
      "epoch": 0.752,
      "grad_norm": 2.9734835624694824,
      "learning_rate": 1.7493333333333334e-05,
      "loss": 0.1647,
      "step": 1410
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 3.5934033393859863,
      "learning_rate": 1.7475555555555557e-05,
      "loss": 0.2008,
      "step": 1420
    },
    {
      "epoch": 0.7626666666666667,
      "grad_norm": 2.324141025543213,
      "learning_rate": 1.745777777777778e-05,
      "loss": 0.2008,
      "step": 1430
    },
    {
      "epoch": 0.768,
      "grad_norm": 2.764238119125366,
      "learning_rate": 1.7440000000000002e-05,
      "loss": 0.1899,
      "step": 1440
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 2.915379524230957,
      "learning_rate": 1.7422222222222222e-05,
      "loss": 0.1946,
      "step": 1450
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 3.5540740489959717,
      "learning_rate": 1.7404444444444445e-05,
      "loss": 0.2025,
      "step": 1460
    },
    {
      "epoch": 0.784,
      "grad_norm": 2.3161509037017822,
      "learning_rate": 1.7386666666666667e-05,
      "loss": 0.1835,
      "step": 1470
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 1.988091230392456,
      "learning_rate": 1.736888888888889e-05,
      "loss": 0.1699,
      "step": 1480
    },
    {
      "epoch": 0.7946666666666666,
      "grad_norm": 5.1438374519348145,
      "learning_rate": 1.7351111111111113e-05,
      "loss": 0.2117,
      "step": 1490
    },
    {
      "epoch": 0.8,
      "grad_norm": 4.64500617980957,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.2147,
      "step": 1500
    },
    {
      "epoch": 0.8053333333333333,
      "grad_norm": 3.162074327468872,
      "learning_rate": 1.7315555555555555e-05,
      "loss": 0.2074,
      "step": 1510
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 3.123638153076172,
      "learning_rate": 1.729777777777778e-05,
      "loss": 0.222,
      "step": 1520
    },
    {
      "epoch": 0.816,
      "grad_norm": 3.1180825233459473,
      "learning_rate": 1.728e-05,
      "loss": 0.1994,
      "step": 1530
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 2.9998488426208496,
      "learning_rate": 1.7262222222222223e-05,
      "loss": 0.2055,
      "step": 1540
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 3.0389208793640137,
      "learning_rate": 1.7244444444444446e-05,
      "loss": 0.2007,
      "step": 1550
    },
    {
      "epoch": 0.832,
      "grad_norm": 2.6446969509124756,
      "learning_rate": 1.7226666666666665e-05,
      "loss": 0.2006,
      "step": 1560
    },
    {
      "epoch": 0.8373333333333334,
      "grad_norm": 3.8680787086486816,
      "learning_rate": 1.720888888888889e-05,
      "loss": 0.1862,
      "step": 1570
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 3.1376025676727295,
      "learning_rate": 1.719111111111111e-05,
      "loss": 0.1932,
      "step": 1580
    },
    {
      "epoch": 0.848,
      "grad_norm": 3.2271485328674316,
      "learning_rate": 1.7173333333333334e-05,
      "loss": 0.1996,
      "step": 1590
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 2.6277849674224854,
      "learning_rate": 1.7155555555555557e-05,
      "loss": 0.1438,
      "step": 1600
    },
    {
      "epoch": 0.8586666666666667,
      "grad_norm": 4.179690837860107,
      "learning_rate": 1.713777777777778e-05,
      "loss": 0.1793,
      "step": 1610
    },
    {
      "epoch": 0.864,
      "grad_norm": 2.0434560775756836,
      "learning_rate": 1.7120000000000002e-05,
      "loss": 0.1621,
      "step": 1620
    },
    {
      "epoch": 0.8693333333333333,
      "grad_norm": 3.5542125701904297,
      "learning_rate": 1.7102222222222225e-05,
      "loss": 0.1815,
      "step": 1630
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 2.0336673259735107,
      "learning_rate": 1.7084444444444444e-05,
      "loss": 0.1871,
      "step": 1640
    },
    {
      "epoch": 0.88,
      "grad_norm": 2.940936803817749,
      "learning_rate": 1.706666666666667e-05,
      "loss": 0.1845,
      "step": 1650
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 2.652810573577881,
      "learning_rate": 1.704888888888889e-05,
      "loss": 0.2206,
      "step": 1660
    },
    {
      "epoch": 0.8906666666666667,
      "grad_norm": 2.7457945346832275,
      "learning_rate": 1.7031111111111113e-05,
      "loss": 0.1743,
      "step": 1670
    },
    {
      "epoch": 0.896,
      "grad_norm": 3.310251474380493,
      "learning_rate": 1.7013333333333335e-05,
      "loss": 0.2144,
      "step": 1680
    },
    {
      "epoch": 0.9013333333333333,
      "grad_norm": 1.6278772354125977,
      "learning_rate": 1.6995555555555555e-05,
      "loss": 0.1921,
      "step": 1690
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 2.5803816318511963,
      "learning_rate": 1.697777777777778e-05,
      "loss": 0.1905,
      "step": 1700
    },
    {
      "epoch": 0.912,
      "grad_norm": 2.628675937652588,
      "learning_rate": 1.696e-05,
      "loss": 0.1888,
      "step": 1710
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 2.649833917617798,
      "learning_rate": 1.6942222222222223e-05,
      "loss": 0.1945,
      "step": 1720
    },
    {
      "epoch": 0.9226666666666666,
      "grad_norm": 3.0059049129486084,
      "learning_rate": 1.6924444444444446e-05,
      "loss": 0.1921,
      "step": 1730
    },
    {
      "epoch": 0.928,
      "grad_norm": 2.3337936401367188,
      "learning_rate": 1.690666666666667e-05,
      "loss": 0.1673,
      "step": 1740
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 3.3886091709136963,
      "learning_rate": 1.688888888888889e-05,
      "loss": 0.1799,
      "step": 1750
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 3.1414694786071777,
      "learning_rate": 1.687111111111111e-05,
      "loss": 0.1949,
      "step": 1760
    },
    {
      "epoch": 0.944,
      "grad_norm": 3.861191987991333,
      "learning_rate": 1.6853333333333333e-05,
      "loss": 0.2004,
      "step": 1770
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 3.5039045810699463,
      "learning_rate": 1.6835555555555556e-05,
      "loss": 0.1855,
      "step": 1780
    },
    {
      "epoch": 0.9546666666666667,
      "grad_norm": 2.468499183654785,
      "learning_rate": 1.681777777777778e-05,
      "loss": 0.1543,
      "step": 1790
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.7443112134933472,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.1443,
      "step": 1800
    },
    {
      "epoch": 0.9653333333333334,
      "grad_norm": 2.543004274368286,
      "learning_rate": 1.6782222222222225e-05,
      "loss": 0.2252,
      "step": 1810
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 2.444000482559204,
      "learning_rate": 1.6764444444444444e-05,
      "loss": 0.1437,
      "step": 1820
    },
    {
      "epoch": 0.976,
      "grad_norm": 2.1584396362304688,
      "learning_rate": 1.674666666666667e-05,
      "loss": 0.1633,
      "step": 1830
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 3.2354915142059326,
      "learning_rate": 1.672888888888889e-05,
      "loss": 0.1837,
      "step": 1840
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 1.441145420074463,
      "learning_rate": 1.6711111111111112e-05,
      "loss": 0.1456,
      "step": 1850
    },
    {
      "epoch": 0.992,
      "grad_norm": 1.8785091638565063,
      "learning_rate": 1.6693333333333335e-05,
      "loss": 0.1388,
      "step": 1860
    },
    {
      "epoch": 0.9973333333333333,
      "grad_norm": 2.640275478363037,
      "learning_rate": 1.6675555555555554e-05,
      "loss": 0.1874,
      "step": 1870
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9522,
      "eval_loss": 0.16177479922771454,
      "eval_runtime": 47.3293,
      "eval_samples_per_second": 422.571,
      "eval_steps_per_second": 4.416,
      "step": 1875
    },
    {
      "epoch": 1.0026666666666666,
      "grad_norm": 2.7024121284484863,
      "learning_rate": 1.665777777777778e-05,
      "loss": 0.1813,
      "step": 1880
    },
    {
      "epoch": 1.008,
      "grad_norm": 2.4247000217437744,
      "learning_rate": 1.664e-05,
      "loss": 0.1849,
      "step": 1890
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 2.5334632396698,
      "learning_rate": 1.6622222222222223e-05,
      "loss": 0.1499,
      "step": 1900
    },
    {
      "epoch": 1.0186666666666666,
      "grad_norm": 2.380298137664795,
      "learning_rate": 1.6604444444444445e-05,
      "loss": 0.1636,
      "step": 1910
    },
    {
      "epoch": 1.024,
      "grad_norm": 3.6392579078674316,
      "learning_rate": 1.6586666666666668e-05,
      "loss": 0.1699,
      "step": 1920
    },
    {
      "epoch": 1.0293333333333334,
      "grad_norm": 3.3060684204101562,
      "learning_rate": 1.656888888888889e-05,
      "loss": 0.1763,
      "step": 1930
    },
    {
      "epoch": 1.0346666666666666,
      "grad_norm": 2.6601383686065674,
      "learning_rate": 1.6551111111111114e-05,
      "loss": 0.169,
      "step": 1940
    },
    {
      "epoch": 1.04,
      "grad_norm": 3.023397207260132,
      "learning_rate": 1.6533333333333333e-05,
      "loss": 0.1955,
      "step": 1950
    },
    {
      "epoch": 1.0453333333333332,
      "grad_norm": 2.5607194900512695,
      "learning_rate": 1.651555555555556e-05,
      "loss": 0.1433,
      "step": 1960
    },
    {
      "epoch": 1.0506666666666666,
      "grad_norm": 3.2736470699310303,
      "learning_rate": 1.649777777777778e-05,
      "loss": 0.1447,
      "step": 1970
    },
    {
      "epoch": 1.056,
      "grad_norm": 4.233142852783203,
      "learning_rate": 1.648e-05,
      "loss": 0.1547,
      "step": 1980
    },
    {
      "epoch": 1.0613333333333332,
      "grad_norm": 2.2918291091918945,
      "learning_rate": 1.6462222222222224e-05,
      "loss": 0.1457,
      "step": 1990
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 4.092354774475098,
      "learning_rate": 1.6444444444444444e-05,
      "loss": 0.194,
      "step": 2000
    },
    {
      "epoch": 1.072,
      "grad_norm": 2.0499868392944336,
      "learning_rate": 1.642666666666667e-05,
      "loss": 0.1691,
      "step": 2010
    },
    {
      "epoch": 1.0773333333333333,
      "grad_norm": 3.1880156993865967,
      "learning_rate": 1.640888888888889e-05,
      "loss": 0.1564,
      "step": 2020
    },
    {
      "epoch": 1.0826666666666667,
      "grad_norm": 2.7348995208740234,
      "learning_rate": 1.6391111111111112e-05,
      "loss": 0.1736,
      "step": 2030
    },
    {
      "epoch": 1.088,
      "grad_norm": 3.316302537918091,
      "learning_rate": 1.6373333333333335e-05,
      "loss": 0.1381,
      "step": 2040
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 1.785122275352478,
      "learning_rate": 1.6355555555555557e-05,
      "loss": 0.1945,
      "step": 2050
    },
    {
      "epoch": 1.0986666666666667,
      "grad_norm": 3.1424431800842285,
      "learning_rate": 1.633777777777778e-05,
      "loss": 0.1614,
      "step": 2060
    },
    {
      "epoch": 1.104,
      "grad_norm": 2.629422187805176,
      "learning_rate": 1.632e-05,
      "loss": 0.1648,
      "step": 2070
    },
    {
      "epoch": 1.1093333333333333,
      "grad_norm": 3.1990158557891846,
      "learning_rate": 1.6302222222222222e-05,
      "loss": 0.1631,
      "step": 2080
    },
    {
      "epoch": 1.1146666666666667,
      "grad_norm": 2.902897596359253,
      "learning_rate": 1.6284444444444445e-05,
      "loss": 0.1496,
      "step": 2090
    },
    {
      "epoch": 1.12,
      "grad_norm": 1.5730047225952148,
      "learning_rate": 1.6266666666666668e-05,
      "loss": 0.137,
      "step": 2100
    },
    {
      "epoch": 1.1253333333333333,
      "grad_norm": 2.105755567550659,
      "learning_rate": 1.624888888888889e-05,
      "loss": 0.1209,
      "step": 2110
    },
    {
      "epoch": 1.1306666666666667,
      "grad_norm": 3.0864460468292236,
      "learning_rate": 1.6231111111111113e-05,
      "loss": 0.152,
      "step": 2120
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 4.047958850860596,
      "learning_rate": 1.6213333333333333e-05,
      "loss": 0.1543,
      "step": 2130
    },
    {
      "epoch": 1.1413333333333333,
      "grad_norm": 3.352796792984009,
      "learning_rate": 1.619555555555556e-05,
      "loss": 0.2019,
      "step": 2140
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 2.140758991241455,
      "learning_rate": 1.617777777777778e-05,
      "loss": 0.1372,
      "step": 2150
    },
    {
      "epoch": 1.152,
      "grad_norm": 3.941197633743286,
      "learning_rate": 1.616e-05,
      "loss": 0.1749,
      "step": 2160
    },
    {
      "epoch": 1.1573333333333333,
      "grad_norm": 4.9209465980529785,
      "learning_rate": 1.6142222222222224e-05,
      "loss": 0.1715,
      "step": 2170
    },
    {
      "epoch": 1.1626666666666667,
      "grad_norm": 2.121079683303833,
      "learning_rate": 1.6124444444444443e-05,
      "loss": 0.1538,
      "step": 2180
    },
    {
      "epoch": 1.168,
      "grad_norm": 1.96491277217865,
      "learning_rate": 1.610666666666667e-05,
      "loss": 0.1481,
      "step": 2190
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 2.936753749847412,
      "learning_rate": 1.608888888888889e-05,
      "loss": 0.1693,
      "step": 2200
    },
    {
      "epoch": 1.1786666666666668,
      "grad_norm": 2.514097213745117,
      "learning_rate": 1.607111111111111e-05,
      "loss": 0.1613,
      "step": 2210
    },
    {
      "epoch": 1.184,
      "grad_norm": 3.268462896347046,
      "learning_rate": 1.6053333333333334e-05,
      "loss": 0.1507,
      "step": 2220
    },
    {
      "epoch": 1.1893333333333334,
      "grad_norm": 3.6877002716064453,
      "learning_rate": 1.6035555555555557e-05,
      "loss": 0.159,
      "step": 2230
    },
    {
      "epoch": 1.1946666666666665,
      "grad_norm": 2.2801175117492676,
      "learning_rate": 1.601777777777778e-05,
      "loss": 0.1366,
      "step": 2240
    },
    {
      "epoch": 1.2,
      "grad_norm": 2.6054985523223877,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.1645,
      "step": 2250
    },
    {
      "epoch": 1.2053333333333334,
      "grad_norm": 2.885674476623535,
      "learning_rate": 1.5982222222222222e-05,
      "loss": 0.1477,
      "step": 2260
    },
    {
      "epoch": 1.2106666666666666,
      "grad_norm": 3.5102169513702393,
      "learning_rate": 1.5964444444444448e-05,
      "loss": 0.1572,
      "step": 2270
    },
    {
      "epoch": 1.216,
      "grad_norm": 3.006258010864258,
      "learning_rate": 1.5946666666666668e-05,
      "loss": 0.1531,
      "step": 2280
    },
    {
      "epoch": 1.2213333333333334,
      "grad_norm": 2.3336026668548584,
      "learning_rate": 1.592888888888889e-05,
      "loss": 0.1536,
      "step": 2290
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 2.606433868408203,
      "learning_rate": 1.5911111111111113e-05,
      "loss": 0.1584,
      "step": 2300
    },
    {
      "epoch": 1.232,
      "grad_norm": 3.569857358932495,
      "learning_rate": 1.5893333333333333e-05,
      "loss": 0.1588,
      "step": 2310
    },
    {
      "epoch": 1.2373333333333334,
      "grad_norm": 2.543311357498169,
      "learning_rate": 1.587555555555556e-05,
      "loss": 0.1651,
      "step": 2320
    },
    {
      "epoch": 1.2426666666666666,
      "grad_norm": 2.3621773719787598,
      "learning_rate": 1.5857777777777778e-05,
      "loss": 0.1077,
      "step": 2330
    },
    {
      "epoch": 1.248,
      "grad_norm": 2.9883246421813965,
      "learning_rate": 1.584e-05,
      "loss": 0.1606,
      "step": 2340
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 2.522775888442993,
      "learning_rate": 1.5822222222222224e-05,
      "loss": 0.1538,
      "step": 2350
    },
    {
      "epoch": 1.2586666666666666,
      "grad_norm": 2.3827109336853027,
      "learning_rate": 1.5804444444444446e-05,
      "loss": 0.1698,
      "step": 2360
    },
    {
      "epoch": 1.264,
      "grad_norm": 1.211219072341919,
      "learning_rate": 1.578666666666667e-05,
      "loss": 0.1583,
      "step": 2370
    },
    {
      "epoch": 1.2693333333333334,
      "grad_norm": 2.322481870651245,
      "learning_rate": 1.576888888888889e-05,
      "loss": 0.1549,
      "step": 2380
    },
    {
      "epoch": 1.2746666666666666,
      "grad_norm": 2.954284191131592,
      "learning_rate": 1.575111111111111e-05,
      "loss": 0.1279,
      "step": 2390
    },
    {
      "epoch": 1.28,
      "grad_norm": 3.124236822128296,
      "learning_rate": 1.5733333333333334e-05,
      "loss": 0.1941,
      "step": 2400
    },
    {
      "epoch": 1.2853333333333334,
      "grad_norm": 2.8574633598327637,
      "learning_rate": 1.5715555555555557e-05,
      "loss": 0.1288,
      "step": 2410
    },
    {
      "epoch": 1.2906666666666666,
      "grad_norm": 1.7889984846115112,
      "learning_rate": 1.569777777777778e-05,
      "loss": 0.1163,
      "step": 2420
    },
    {
      "epoch": 1.296,
      "grad_norm": 3.1381752490997314,
      "learning_rate": 1.5680000000000002e-05,
      "loss": 0.1509,
      "step": 2430
    },
    {
      "epoch": 1.3013333333333335,
      "grad_norm": 3.0741732120513916,
      "learning_rate": 1.5662222222222222e-05,
      "loss": 0.1591,
      "step": 2440
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 1.6643571853637695,
      "learning_rate": 1.5644444444444448e-05,
      "loss": 0.1556,
      "step": 2450
    },
    {
      "epoch": 1.312,
      "grad_norm": 2.39359450340271,
      "learning_rate": 1.5626666666666667e-05,
      "loss": 0.1553,
      "step": 2460
    },
    {
      "epoch": 1.3173333333333335,
      "grad_norm": 1.8114328384399414,
      "learning_rate": 1.560888888888889e-05,
      "loss": 0.1434,
      "step": 2470
    },
    {
      "epoch": 1.3226666666666667,
      "grad_norm": 3.360189437866211,
      "learning_rate": 1.5591111111111113e-05,
      "loss": 0.1555,
      "step": 2480
    },
    {
      "epoch": 1.328,
      "grad_norm": 1.8738735914230347,
      "learning_rate": 1.5573333333333332e-05,
      "loss": 0.122,
      "step": 2490
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 1.3377200365066528,
      "learning_rate": 1.555555555555556e-05,
      "loss": 0.1556,
      "step": 2500
    },
    {
      "epoch": 1.3386666666666667,
      "grad_norm": 3.849900484085083,
      "learning_rate": 1.5537777777777778e-05,
      "loss": 0.1809,
      "step": 2510
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 3.0850749015808105,
      "learning_rate": 1.552e-05,
      "loss": 0.1652,
      "step": 2520
    },
    {
      "epoch": 1.3493333333333333,
      "grad_norm": 2.0143954753875732,
      "learning_rate": 1.5502222222222223e-05,
      "loss": 0.1429,
      "step": 2530
    },
    {
      "epoch": 1.3546666666666667,
      "grad_norm": 2.536146640777588,
      "learning_rate": 1.5484444444444446e-05,
      "loss": 0.1652,
      "step": 2540
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 3.6218037605285645,
      "learning_rate": 1.546666666666667e-05,
      "loss": 0.173,
      "step": 2550
    },
    {
      "epoch": 1.3653333333333333,
      "grad_norm": 1.2426178455352783,
      "learning_rate": 1.544888888888889e-05,
      "loss": 0.1506,
      "step": 2560
    },
    {
      "epoch": 1.3706666666666667,
      "grad_norm": 3.584998846054077,
      "learning_rate": 1.543111111111111e-05,
      "loss": 0.1652,
      "step": 2570
    },
    {
      "epoch": 1.376,
      "grad_norm": 3.6857821941375732,
      "learning_rate": 1.5413333333333337e-05,
      "loss": 0.1372,
      "step": 2580
    },
    {
      "epoch": 1.3813333333333333,
      "grad_norm": 2.637937307357788,
      "learning_rate": 1.5395555555555556e-05,
      "loss": 0.131,
      "step": 2590
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 3.107482671737671,
      "learning_rate": 1.537777777777778e-05,
      "loss": 0.1412,
      "step": 2600
    },
    {
      "epoch": 1.392,
      "grad_norm": 3.5517091751098633,
      "learning_rate": 1.5360000000000002e-05,
      "loss": 0.149,
      "step": 2610
    },
    {
      "epoch": 1.3973333333333333,
      "grad_norm": 1.8662786483764648,
      "learning_rate": 1.534222222222222e-05,
      "loss": 0.1361,
      "step": 2620
    },
    {
      "epoch": 1.4026666666666667,
      "grad_norm": 2.5949339866638184,
      "learning_rate": 1.5324444444444448e-05,
      "loss": 0.1465,
      "step": 2630
    },
    {
      "epoch": 1.408,
      "grad_norm": 3.543020248413086,
      "learning_rate": 1.5306666666666667e-05,
      "loss": 0.1919,
      "step": 2640
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 2.0948755741119385,
      "learning_rate": 1.528888888888889e-05,
      "loss": 0.1284,
      "step": 2650
    },
    {
      "epoch": 1.4186666666666667,
      "grad_norm": 2.4864001274108887,
      "learning_rate": 1.5271111111111112e-05,
      "loss": 0.1156,
      "step": 2660
    },
    {
      "epoch": 1.424,
      "grad_norm": 2.35933780670166,
      "learning_rate": 1.5253333333333335e-05,
      "loss": 0.1125,
      "step": 2670
    },
    {
      "epoch": 1.4293333333333333,
      "grad_norm": 2.679353713989258,
      "learning_rate": 1.5235555555555556e-05,
      "loss": 0.1664,
      "step": 2680
    },
    {
      "epoch": 1.4346666666666668,
      "grad_norm": 2.9537599086761475,
      "learning_rate": 1.5217777777777777e-05,
      "loss": 0.1662,
      "step": 2690
    },
    {
      "epoch": 1.44,
      "grad_norm": 2.42971134185791,
      "learning_rate": 1.5200000000000002e-05,
      "loss": 0.1304,
      "step": 2700
    },
    {
      "epoch": 1.4453333333333334,
      "grad_norm": 3.495495557785034,
      "learning_rate": 1.5182222222222223e-05,
      "loss": 0.1671,
      "step": 2710
    },
    {
      "epoch": 1.4506666666666668,
      "grad_norm": 4.07634162902832,
      "learning_rate": 1.5164444444444446e-05,
      "loss": 0.174,
      "step": 2720
    },
    {
      "epoch": 1.456,
      "grad_norm": 2.547858476638794,
      "learning_rate": 1.5146666666666667e-05,
      "loss": 0.1241,
      "step": 2730
    },
    {
      "epoch": 1.4613333333333334,
      "grad_norm": 3.1569254398345947,
      "learning_rate": 1.5128888888888891e-05,
      "loss": 0.1659,
      "step": 2740
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 2.1295053958892822,
      "learning_rate": 1.5111111111111112e-05,
      "loss": 0.1622,
      "step": 2750
    },
    {
      "epoch": 1.472,
      "grad_norm": 2.7306978702545166,
      "learning_rate": 1.5093333333333335e-05,
      "loss": 0.1424,
      "step": 2760
    },
    {
      "epoch": 1.4773333333333334,
      "grad_norm": 2.9341702461242676,
      "learning_rate": 1.5075555555555556e-05,
      "loss": 0.1331,
      "step": 2770
    },
    {
      "epoch": 1.4826666666666668,
      "grad_norm": 2.4492101669311523,
      "learning_rate": 1.505777777777778e-05,
      "loss": 0.2122,
      "step": 2780
    },
    {
      "epoch": 1.488,
      "grad_norm": 2.4894068241119385,
      "learning_rate": 1.5040000000000002e-05,
      "loss": 0.162,
      "step": 2790
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 2.583138942718506,
      "learning_rate": 1.5022222222222223e-05,
      "loss": 0.1399,
      "step": 2800
    },
    {
      "epoch": 1.4986666666666666,
      "grad_norm": 2.5380916595458984,
      "learning_rate": 1.5004444444444446e-05,
      "loss": 0.1428,
      "step": 2810
    },
    {
      "epoch": 1.504,
      "grad_norm": 4.339022159576416,
      "learning_rate": 1.4986666666666667e-05,
      "loss": 0.1711,
      "step": 2820
    },
    {
      "epoch": 1.5093333333333332,
      "grad_norm": 3.205369472503662,
      "learning_rate": 1.4968888888888891e-05,
      "loss": 0.1741,
      "step": 2830
    },
    {
      "epoch": 1.5146666666666668,
      "grad_norm": 5.239588260650635,
      "learning_rate": 1.4951111111111112e-05,
      "loss": 0.1192,
      "step": 2840
    },
    {
      "epoch": 1.52,
      "grad_norm": 3.0619852542877197,
      "learning_rate": 1.4933333333333335e-05,
      "loss": 0.1537,
      "step": 2850
    },
    {
      "epoch": 1.5253333333333332,
      "grad_norm": 3.155547618865967,
      "learning_rate": 1.4915555555555556e-05,
      "loss": 0.1364,
      "step": 2860
    },
    {
      "epoch": 1.5306666666666666,
      "grad_norm": 2.759366989135742,
      "learning_rate": 1.489777777777778e-05,
      "loss": 0.1466,
      "step": 2870
    },
    {
      "epoch": 1.536,
      "grad_norm": 3.3737080097198486,
      "learning_rate": 1.4880000000000002e-05,
      "loss": 0.1235,
      "step": 2880
    },
    {
      "epoch": 1.5413333333333332,
      "grad_norm": 2.3901560306549072,
      "learning_rate": 1.4862222222222223e-05,
      "loss": 0.1577,
      "step": 2890
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 3.444118022918701,
      "learning_rate": 1.4844444444444445e-05,
      "loss": 0.1655,
      "step": 2900
    },
    {
      "epoch": 1.552,
      "grad_norm": 2.112642765045166,
      "learning_rate": 1.4826666666666666e-05,
      "loss": 0.1403,
      "step": 2910
    },
    {
      "epoch": 1.5573333333333332,
      "grad_norm": 1.4391063451766968,
      "learning_rate": 1.4808888888888891e-05,
      "loss": 0.1525,
      "step": 2920
    },
    {
      "epoch": 1.5626666666666666,
      "grad_norm": 2.5086915493011475,
      "learning_rate": 1.4791111111111112e-05,
      "loss": 0.1408,
      "step": 2930
    },
    {
      "epoch": 1.568,
      "grad_norm": 3.9064459800720215,
      "learning_rate": 1.4773333333333335e-05,
      "loss": 0.1567,
      "step": 2940
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 3.9377682209014893,
      "learning_rate": 1.4755555555555556e-05,
      "loss": 0.1487,
      "step": 2950
    },
    {
      "epoch": 1.5786666666666667,
      "grad_norm": 3.0561294555664062,
      "learning_rate": 1.473777777777778e-05,
      "loss": 0.1337,
      "step": 2960
    },
    {
      "epoch": 1.584,
      "grad_norm": 2.30009388923645,
      "learning_rate": 1.4720000000000001e-05,
      "loss": 0.1409,
      "step": 2970
    },
    {
      "epoch": 1.5893333333333333,
      "grad_norm": 2.848390817642212,
      "learning_rate": 1.4702222222222224e-05,
      "loss": 0.1531,
      "step": 2980
    },
    {
      "epoch": 1.5946666666666667,
      "grad_norm": 3.6621346473693848,
      "learning_rate": 1.4684444444444445e-05,
      "loss": 0.1546,
      "step": 2990
    },
    {
      "epoch": 1.6,
      "grad_norm": 3.226128101348877,
      "learning_rate": 1.4666666666666666e-05,
      "loss": 0.1261,
      "step": 3000
    },
    {
      "epoch": 1.6053333333333333,
      "grad_norm": 2.0554580688476562,
      "learning_rate": 1.464888888888889e-05,
      "loss": 0.1823,
      "step": 3010
    },
    {
      "epoch": 1.6106666666666667,
      "grad_norm": 2.969326972961426,
      "learning_rate": 1.4631111111111112e-05,
      "loss": 0.1735,
      "step": 3020
    },
    {
      "epoch": 1.616,
      "grad_norm": 4.0659003257751465,
      "learning_rate": 1.4613333333333335e-05,
      "loss": 0.1453,
      "step": 3030
    },
    {
      "epoch": 1.6213333333333333,
      "grad_norm": 3.3267996311187744,
      "learning_rate": 1.4595555555555556e-05,
      "loss": 0.13,
      "step": 3040
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 3.3597230911254883,
      "learning_rate": 1.457777777777778e-05,
      "loss": 0.1524,
      "step": 3050
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 1.8789424896240234,
      "learning_rate": 1.4560000000000001e-05,
      "loss": 0.1555,
      "step": 3060
    },
    {
      "epoch": 1.6373333333333333,
      "grad_norm": 2.771972894668579,
      "learning_rate": 1.4542222222222224e-05,
      "loss": 0.1578,
      "step": 3070
    },
    {
      "epoch": 1.6426666666666667,
      "grad_norm": 3.734224557876587,
      "learning_rate": 1.4524444444444445e-05,
      "loss": 0.1489,
      "step": 3080
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 2.193277597427368,
      "learning_rate": 1.450666666666667e-05,
      "loss": 0.142,
      "step": 3090
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 2.495779275894165,
      "learning_rate": 1.448888888888889e-05,
      "loss": 0.1705,
      "step": 3100
    },
    {
      "epoch": 1.6586666666666665,
      "grad_norm": 3.3288865089416504,
      "learning_rate": 1.4471111111111112e-05,
      "loss": 0.145,
      "step": 3110
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 3.1698567867279053,
      "learning_rate": 1.4453333333333334e-05,
      "loss": 0.1321,
      "step": 3120
    },
    {
      "epoch": 1.6693333333333333,
      "grad_norm": 1.9046146869659424,
      "learning_rate": 1.4435555555555556e-05,
      "loss": 0.1314,
      "step": 3130
    },
    {
      "epoch": 1.6746666666666665,
      "grad_norm": 3.092376708984375,
      "learning_rate": 1.441777777777778e-05,
      "loss": 0.1596,
      "step": 3140
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 3.0067150592803955,
      "learning_rate": 1.4400000000000001e-05,
      "loss": 0.1178,
      "step": 3150
    },
    {
      "epoch": 1.6853333333333333,
      "grad_norm": 2.1397793292999268,
      "learning_rate": 1.4382222222222224e-05,
      "loss": 0.1728,
      "step": 3160
    },
    {
      "epoch": 1.6906666666666665,
      "grad_norm": 3.5217337608337402,
      "learning_rate": 1.4364444444444445e-05,
      "loss": 0.1555,
      "step": 3170
    },
    {
      "epoch": 1.696,
      "grad_norm": 2.6682181358337402,
      "learning_rate": 1.434666666666667e-05,
      "loss": 0.1177,
      "step": 3180
    },
    {
      "epoch": 1.7013333333333334,
      "grad_norm": 3.716740608215332,
      "learning_rate": 1.432888888888889e-05,
      "loss": 0.1427,
      "step": 3190
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.8339747190475464,
      "learning_rate": 1.4311111111111111e-05,
      "loss": 0.1604,
      "step": 3200
    },
    {
      "epoch": 1.712,
      "grad_norm": 2.177112579345703,
      "learning_rate": 1.4293333333333334e-05,
      "loss": 0.1296,
      "step": 3210
    },
    {
      "epoch": 1.7173333333333334,
      "grad_norm": 2.3886795043945312,
      "learning_rate": 1.4275555555555555e-05,
      "loss": 0.1308,
      "step": 3220
    },
    {
      "epoch": 1.7226666666666666,
      "grad_norm": 2.0668349266052246,
      "learning_rate": 1.425777777777778e-05,
      "loss": 0.1513,
      "step": 3230
    },
    {
      "epoch": 1.728,
      "grad_norm": 2.5753350257873535,
      "learning_rate": 1.4240000000000001e-05,
      "loss": 0.1571,
      "step": 3240
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 2.6335554122924805,
      "learning_rate": 1.4222222222222224e-05,
      "loss": 0.1253,
      "step": 3250
    },
    {
      "epoch": 1.7386666666666666,
      "grad_norm": 3.116837978363037,
      "learning_rate": 1.4204444444444445e-05,
      "loss": 0.1427,
      "step": 3260
    },
    {
      "epoch": 1.744,
      "grad_norm": 1.9039371013641357,
      "learning_rate": 1.418666666666667e-05,
      "loss": 0.131,
      "step": 3270
    },
    {
      "epoch": 1.7493333333333334,
      "grad_norm": 2.4038310050964355,
      "learning_rate": 1.416888888888889e-05,
      "loss": 0.1231,
      "step": 3280
    },
    {
      "epoch": 1.7546666666666666,
      "grad_norm": 1.5370198488235474,
      "learning_rate": 1.4151111111111113e-05,
      "loss": 0.128,
      "step": 3290
    },
    {
      "epoch": 1.76,
      "grad_norm": 3.254650115966797,
      "learning_rate": 1.4133333333333334e-05,
      "loss": 0.1447,
      "step": 3300
    },
    {
      "epoch": 1.7653333333333334,
      "grad_norm": 2.7196977138519287,
      "learning_rate": 1.4115555555555555e-05,
      "loss": 0.1497,
      "step": 3310
    },
    {
      "epoch": 1.7706666666666666,
      "grad_norm": 2.954242706298828,
      "learning_rate": 1.409777777777778e-05,
      "loss": 0.1406,
      "step": 3320
    },
    {
      "epoch": 1.776,
      "grad_norm": 2.363593339920044,
      "learning_rate": 1.408e-05,
      "loss": 0.1479,
      "step": 3330
    },
    {
      "epoch": 1.7813333333333334,
      "grad_norm": 3.330514430999756,
      "learning_rate": 1.4062222222222223e-05,
      "loss": 0.2173,
      "step": 3340
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 2.7564027309417725,
      "learning_rate": 1.4044444444444445e-05,
      "loss": 0.1586,
      "step": 3350
    },
    {
      "epoch": 1.792,
      "grad_norm": 2.0126872062683105,
      "learning_rate": 1.4026666666666669e-05,
      "loss": 0.1434,
      "step": 3360
    },
    {
      "epoch": 1.7973333333333334,
      "grad_norm": 3.062717914581299,
      "learning_rate": 1.400888888888889e-05,
      "loss": 0.1087,
      "step": 3370
    },
    {
      "epoch": 1.8026666666666666,
      "grad_norm": 2.6111974716186523,
      "learning_rate": 1.3991111111111113e-05,
      "loss": 0.1467,
      "step": 3380
    },
    {
      "epoch": 1.808,
      "grad_norm": 1.7258812189102173,
      "learning_rate": 1.3973333333333334e-05,
      "loss": 0.112,
      "step": 3390
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 4.1073408126831055,
      "learning_rate": 1.3955555555555558e-05,
      "loss": 0.1413,
      "step": 3400
    },
    {
      "epoch": 1.8186666666666667,
      "grad_norm": 2.5441410541534424,
      "learning_rate": 1.393777777777778e-05,
      "loss": 0.1526,
      "step": 3410
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 2.8192391395568848,
      "learning_rate": 1.392e-05,
      "loss": 0.1555,
      "step": 3420
    },
    {
      "epoch": 1.8293333333333335,
      "grad_norm": 3.050295114517212,
      "learning_rate": 1.3902222222222223e-05,
      "loss": 0.1739,
      "step": 3430
    },
    {
      "epoch": 1.8346666666666667,
      "grad_norm": 2.8018479347229004,
      "learning_rate": 1.3884444444444444e-05,
      "loss": 0.1544,
      "step": 3440
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 2.6846845149993896,
      "learning_rate": 1.3866666666666669e-05,
      "loss": 0.1308,
      "step": 3450
    },
    {
      "epoch": 1.8453333333333335,
      "grad_norm": 2.834657907485962,
      "learning_rate": 1.384888888888889e-05,
      "loss": 0.1901,
      "step": 3460
    },
    {
      "epoch": 1.8506666666666667,
      "grad_norm": 2.1398088932037354,
      "learning_rate": 1.3831111111111113e-05,
      "loss": 0.1346,
      "step": 3470
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 2.3643739223480225,
      "learning_rate": 1.3813333333333334e-05,
      "loss": 0.1353,
      "step": 3480
    },
    {
      "epoch": 1.8613333333333333,
      "grad_norm": 3.5314059257507324,
      "learning_rate": 1.3795555555555558e-05,
      "loss": 0.1548,
      "step": 3490
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 1.8087071180343628,
      "learning_rate": 1.377777777777778e-05,
      "loss": 0.1608,
      "step": 3500
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 3.140448808670044,
      "learning_rate": 1.376e-05,
      "loss": 0.1291,
      "step": 3510
    },
    {
      "epoch": 1.8773333333333333,
      "grad_norm": 2.849179267883301,
      "learning_rate": 1.3742222222222223e-05,
      "loss": 0.1433,
      "step": 3520
    },
    {
      "epoch": 1.8826666666666667,
      "grad_norm": 3.1561851501464844,
      "learning_rate": 1.3724444444444444e-05,
      "loss": 0.1501,
      "step": 3530
    },
    {
      "epoch": 1.888,
      "grad_norm": 2.0603373050689697,
      "learning_rate": 1.3706666666666669e-05,
      "loss": 0.1522,
      "step": 3540
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 2.522777795791626,
      "learning_rate": 1.368888888888889e-05,
      "loss": 0.1306,
      "step": 3550
    },
    {
      "epoch": 1.8986666666666667,
      "grad_norm": 3.3453359603881836,
      "learning_rate": 1.3671111111111113e-05,
      "loss": 0.1848,
      "step": 3560
    },
    {
      "epoch": 1.904,
      "grad_norm": 3.637115716934204,
      "learning_rate": 1.3653333333333334e-05,
      "loss": 0.1418,
      "step": 3570
    },
    {
      "epoch": 1.9093333333333333,
      "grad_norm": 3.3314170837402344,
      "learning_rate": 1.3635555555555558e-05,
      "loss": 0.1373,
      "step": 3580
    },
    {
      "epoch": 1.9146666666666667,
      "grad_norm": 3.3508756160736084,
      "learning_rate": 1.361777777777778e-05,
      "loss": 0.1441,
      "step": 3590
    },
    {
      "epoch": 1.92,
      "grad_norm": 4.079929351806641,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.1611,
      "step": 3600
    },
    {
      "epoch": 1.9253333333333333,
      "grad_norm": 3.081526279449463,
      "learning_rate": 1.3582222222222223e-05,
      "loss": 0.1744,
      "step": 3610
    },
    {
      "epoch": 1.9306666666666668,
      "grad_norm": 2.141205310821533,
      "learning_rate": 1.3564444444444444e-05,
      "loss": 0.1458,
      "step": 3620
    },
    {
      "epoch": 1.936,
      "grad_norm": 3.137617588043213,
      "learning_rate": 1.3546666666666669e-05,
      "loss": 0.1589,
      "step": 3630
    },
    {
      "epoch": 1.9413333333333334,
      "grad_norm": 3.0586655139923096,
      "learning_rate": 1.352888888888889e-05,
      "loss": 0.1418,
      "step": 3640
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 2.8604483604431152,
      "learning_rate": 1.3511111111111112e-05,
      "loss": 0.119,
      "step": 3650
    },
    {
      "epoch": 1.952,
      "grad_norm": 3.0054538249969482,
      "learning_rate": 1.3493333333333333e-05,
      "loss": 0.1637,
      "step": 3660
    },
    {
      "epoch": 1.9573333333333334,
      "grad_norm": 3.2180793285369873,
      "learning_rate": 1.3475555555555558e-05,
      "loss": 0.1227,
      "step": 3670
    },
    {
      "epoch": 1.9626666666666668,
      "grad_norm": 1.8601603507995605,
      "learning_rate": 1.3457777777777779e-05,
      "loss": 0.116,
      "step": 3680
    },
    {
      "epoch": 1.968,
      "grad_norm": 3.1639113426208496,
      "learning_rate": 1.3440000000000002e-05,
      "loss": 0.1326,
      "step": 3690
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 2.436915397644043,
      "learning_rate": 1.3422222222222223e-05,
      "loss": 0.1261,
      "step": 3700
    },
    {
      "epoch": 1.9786666666666668,
      "grad_norm": 3.9108121395111084,
      "learning_rate": 1.3404444444444447e-05,
      "loss": 0.1593,
      "step": 3710
    },
    {
      "epoch": 1.984,
      "grad_norm": 1.9920530319213867,
      "learning_rate": 1.3386666666666668e-05,
      "loss": 0.1516,
      "step": 3720
    },
    {
      "epoch": 1.9893333333333332,
      "grad_norm": 2.207017660140991,
      "learning_rate": 1.336888888888889e-05,
      "loss": 0.1238,
      "step": 3730
    },
    {
      "epoch": 1.9946666666666668,
      "grad_norm": 0.9930538535118103,
      "learning_rate": 1.3351111111111112e-05,
      "loss": 0.1274,
      "step": 3740
    },
    {
      "epoch": 2.0,
      "grad_norm": 3.0780577659606934,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.1309,
      "step": 3750
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.95755,
      "eval_loss": 0.13812845945358276,
      "eval_runtime": 47.4727,
      "eval_samples_per_second": 421.294,
      "eval_steps_per_second": 4.403,
      "step": 3750
    }
  ],
  "logging_steps": 10,
  "max_steps": 11250,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.424468975616e+16,
  "train_batch_size": 96,
  "trial_name": null,
  "trial_params": null
}
