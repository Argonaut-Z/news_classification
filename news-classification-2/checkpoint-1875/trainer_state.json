{
  "best_metric": 0.9522,
  "best_model_checkpoint": "news-classification-2/checkpoint-1875",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1875,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 2.340062379837036,
      "learning_rate": 1.9982222222222224e-05,
      "loss": 2.4833,
      "step": 10
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 2.3409371376037598,
      "learning_rate": 1.9964444444444447e-05,
      "loss": 2.1901,
      "step": 20
    },
    {
      "epoch": 0.016,
      "grad_norm": 2.1555848121643066,
      "learning_rate": 1.9946666666666667e-05,
      "loss": 1.9212,
      "step": 30
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 2.0559258460998535,
      "learning_rate": 1.992888888888889e-05,
      "loss": 1.7008,
      "step": 40
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 2.086430549621582,
      "learning_rate": 1.9911111111111112e-05,
      "loss": 1.4714,
      "step": 50
    },
    {
      "epoch": 0.032,
      "grad_norm": 1.9018769264221191,
      "learning_rate": 1.9893333333333335e-05,
      "loss": 1.3258,
      "step": 60
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 2.6979124546051025,
      "learning_rate": 1.9875555555555558e-05,
      "loss": 1.2274,
      "step": 70
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 2.662576675415039,
      "learning_rate": 1.985777777777778e-05,
      "loss": 1.041,
      "step": 80
    },
    {
      "epoch": 0.048,
      "grad_norm": 1.9613078832626343,
      "learning_rate": 1.9840000000000003e-05,
      "loss": 0.9608,
      "step": 90
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 2.3540279865264893,
      "learning_rate": 1.9822222222222226e-05,
      "loss": 0.867,
      "step": 100
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 2.195451259613037,
      "learning_rate": 1.9804444444444445e-05,
      "loss": 0.805,
      "step": 110
    },
    {
      "epoch": 0.064,
      "grad_norm": 2.2827296257019043,
      "learning_rate": 1.9786666666666668e-05,
      "loss": 0.717,
      "step": 120
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 2.4951789379119873,
      "learning_rate": 1.976888888888889e-05,
      "loss": 0.6991,
      "step": 130
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 2.4580349922180176,
      "learning_rate": 1.9751111111111114e-05,
      "loss": 0.6415,
      "step": 140
    },
    {
      "epoch": 0.08,
      "grad_norm": 2.92366099357605,
      "learning_rate": 1.9733333333333336e-05,
      "loss": 0.5664,
      "step": 150
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 2.836576461791992,
      "learning_rate": 1.9715555555555556e-05,
      "loss": 0.5436,
      "step": 160
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 2.117300510406494,
      "learning_rate": 1.969777777777778e-05,
      "loss": 0.5142,
      "step": 170
    },
    {
      "epoch": 0.096,
      "grad_norm": 1.8074114322662354,
      "learning_rate": 1.968e-05,
      "loss": 0.4426,
      "step": 180
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 2.525524139404297,
      "learning_rate": 1.9662222222222224e-05,
      "loss": 0.5051,
      "step": 190
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 1.9512096643447876,
      "learning_rate": 1.9644444444444447e-05,
      "loss": 0.429,
      "step": 200
    },
    {
      "epoch": 0.112,
      "grad_norm": 2.3497812747955322,
      "learning_rate": 1.9626666666666666e-05,
      "loss": 0.4546,
      "step": 210
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 2.7469656467437744,
      "learning_rate": 1.960888888888889e-05,
      "loss": 0.4736,
      "step": 220
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 2.4587910175323486,
      "learning_rate": 1.9591111111111112e-05,
      "loss": 0.4574,
      "step": 230
    },
    {
      "epoch": 0.128,
      "grad_norm": 2.678929090499878,
      "learning_rate": 1.9573333333333335e-05,
      "loss": 0.4563,
      "step": 240
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 2.524336099624634,
      "learning_rate": 1.9555555555555557e-05,
      "loss": 0.4102,
      "step": 250
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 3.5036823749542236,
      "learning_rate": 1.953777777777778e-05,
      "loss": 0.3958,
      "step": 260
    },
    {
      "epoch": 0.144,
      "grad_norm": 2.847472906112671,
      "learning_rate": 1.9520000000000003e-05,
      "loss": 0.3817,
      "step": 270
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 2.873018503189087,
      "learning_rate": 1.9502222222222226e-05,
      "loss": 0.4008,
      "step": 280
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 2.7622461318969727,
      "learning_rate": 1.9484444444444445e-05,
      "loss": 0.3814,
      "step": 290
    },
    {
      "epoch": 0.16,
      "grad_norm": 3.2881574630737305,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 0.3392,
      "step": 300
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 2.1540071964263916,
      "learning_rate": 1.944888888888889e-05,
      "loss": 0.3804,
      "step": 310
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 2.4652674198150635,
      "learning_rate": 1.9431111111111113e-05,
      "loss": 0.3656,
      "step": 320
    },
    {
      "epoch": 0.176,
      "grad_norm": 2.6359810829162598,
      "learning_rate": 1.9413333333333336e-05,
      "loss": 0.3478,
      "step": 330
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 1.7288661003112793,
      "learning_rate": 1.9395555555555555e-05,
      "loss": 0.345,
      "step": 340
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 4.310972690582275,
      "learning_rate": 1.9377777777777778e-05,
      "loss": 0.3711,
      "step": 350
    },
    {
      "epoch": 0.192,
      "grad_norm": 2.8951938152313232,
      "learning_rate": 1.936e-05,
      "loss": 0.3385,
      "step": 360
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 2.653503656387329,
      "learning_rate": 1.9342222222222224e-05,
      "loss": 0.3717,
      "step": 370
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 1.7922152280807495,
      "learning_rate": 1.9324444444444447e-05,
      "loss": 0.3024,
      "step": 380
    },
    {
      "epoch": 0.208,
      "grad_norm": 2.9919824600219727,
      "learning_rate": 1.930666666666667e-05,
      "loss": 0.3837,
      "step": 390
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 4.290222644805908,
      "learning_rate": 1.928888888888889e-05,
      "loss": 0.3877,
      "step": 400
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 1.981816053390503,
      "learning_rate": 1.9271111111111115e-05,
      "loss": 0.3121,
      "step": 410
    },
    {
      "epoch": 0.224,
      "grad_norm": 2.787489652633667,
      "learning_rate": 1.9253333333333334e-05,
      "loss": 0.29,
      "step": 420
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 2.284292221069336,
      "learning_rate": 1.9235555555555557e-05,
      "loss": 0.2845,
      "step": 430
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 2.2889513969421387,
      "learning_rate": 1.921777777777778e-05,
      "loss": 0.302,
      "step": 440
    },
    {
      "epoch": 0.24,
      "grad_norm": 3.1597371101379395,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.3186,
      "step": 450
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 3.7679572105407715,
      "learning_rate": 1.9182222222222225e-05,
      "loss": 0.3009,
      "step": 460
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 3.072195529937744,
      "learning_rate": 1.9164444444444445e-05,
      "loss": 0.3224,
      "step": 470
    },
    {
      "epoch": 0.256,
      "grad_norm": 2.4539129734039307,
      "learning_rate": 1.9146666666666667e-05,
      "loss": 0.2782,
      "step": 480
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 2.8653109073638916,
      "learning_rate": 1.912888888888889e-05,
      "loss": 0.2589,
      "step": 490
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 2.474698305130005,
      "learning_rate": 1.9111111111111113e-05,
      "loss": 0.2742,
      "step": 500
    },
    {
      "epoch": 0.272,
      "grad_norm": 1.7876826524734497,
      "learning_rate": 1.9093333333333336e-05,
      "loss": 0.2879,
      "step": 510
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 1.8565173149108887,
      "learning_rate": 1.9075555555555555e-05,
      "loss": 0.2343,
      "step": 520
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 2.3621513843536377,
      "learning_rate": 1.9057777777777778e-05,
      "loss": 0.2787,
      "step": 530
    },
    {
      "epoch": 0.288,
      "grad_norm": 2.279662609100342,
      "learning_rate": 1.904e-05,
      "loss": 0.298,
      "step": 540
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 2.2688517570495605,
      "learning_rate": 1.9022222222222223e-05,
      "loss": 0.2551,
      "step": 550
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 2.961923122406006,
      "learning_rate": 1.9004444444444446e-05,
      "loss": 0.2878,
      "step": 560
    },
    {
      "epoch": 0.304,
      "grad_norm": 3.4555869102478027,
      "learning_rate": 1.898666666666667e-05,
      "loss": 0.2674,
      "step": 570
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 3.4741828441619873,
      "learning_rate": 1.896888888888889e-05,
      "loss": 0.2678,
      "step": 580
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 2.9928531646728516,
      "learning_rate": 1.8951111111111115e-05,
      "loss": 0.2409,
      "step": 590
    },
    {
      "epoch": 0.32,
      "grad_norm": 3.1215834617614746,
      "learning_rate": 1.8933333333333334e-05,
      "loss": 0.2827,
      "step": 600
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 2.1197874546051025,
      "learning_rate": 1.8915555555555557e-05,
      "loss": 0.2862,
      "step": 610
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 1.7706376314163208,
      "learning_rate": 1.889777777777778e-05,
      "loss": 0.2415,
      "step": 620
    },
    {
      "epoch": 0.336,
      "grad_norm": 1.7263665199279785,
      "learning_rate": 1.8880000000000002e-05,
      "loss": 0.2782,
      "step": 630
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 2.8460729122161865,
      "learning_rate": 1.8862222222222225e-05,
      "loss": 0.3126,
      "step": 640
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 3.8757948875427246,
      "learning_rate": 1.8844444444444444e-05,
      "loss": 0.2266,
      "step": 650
    },
    {
      "epoch": 0.352,
      "grad_norm": 2.9246137142181396,
      "learning_rate": 1.8826666666666667e-05,
      "loss": 0.2851,
      "step": 660
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 2.8725013732910156,
      "learning_rate": 1.880888888888889e-05,
      "loss": 0.2669,
      "step": 670
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 2.025454044342041,
      "learning_rate": 1.8791111111111113e-05,
      "loss": 0.2901,
      "step": 680
    },
    {
      "epoch": 0.368,
      "grad_norm": 2.534980535507202,
      "learning_rate": 1.8773333333333335e-05,
      "loss": 0.2823,
      "step": 690
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 2.8048534393310547,
      "learning_rate": 1.8755555555555558e-05,
      "loss": 0.2384,
      "step": 700
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 2.4496400356292725,
      "learning_rate": 1.8737777777777778e-05,
      "loss": 0.285,
      "step": 710
    },
    {
      "epoch": 0.384,
      "grad_norm": 1.9532679319381714,
      "learning_rate": 1.8720000000000004e-05,
      "loss": 0.1976,
      "step": 720
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 3.9647562503814697,
      "learning_rate": 1.8702222222222223e-05,
      "loss": 0.2677,
      "step": 730
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 3.576298236846924,
      "learning_rate": 1.8684444444444446e-05,
      "loss": 0.2581,
      "step": 740
    },
    {
      "epoch": 0.4,
      "grad_norm": 3.9200901985168457,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.2654,
      "step": 750
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 2.4719555377960205,
      "learning_rate": 1.8648888888888888e-05,
      "loss": 0.2445,
      "step": 760
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 2.23500394821167,
      "learning_rate": 1.8631111111111114e-05,
      "loss": 0.2681,
      "step": 770
    },
    {
      "epoch": 0.416,
      "grad_norm": 3.358092784881592,
      "learning_rate": 1.8613333333333334e-05,
      "loss": 0.2571,
      "step": 780
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 2.4395995140075684,
      "learning_rate": 1.8595555555555556e-05,
      "loss": 0.2871,
      "step": 790
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 2.0386977195739746,
      "learning_rate": 1.857777777777778e-05,
      "loss": 0.2569,
      "step": 800
    },
    {
      "epoch": 0.432,
      "grad_norm": 2.3983476161956787,
      "learning_rate": 1.8560000000000002e-05,
      "loss": 0.2103,
      "step": 810
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 2.9427950382232666,
      "learning_rate": 1.8542222222222225e-05,
      "loss": 0.24,
      "step": 820
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 2.028043508529663,
      "learning_rate": 1.8524444444444444e-05,
      "loss": 0.2415,
      "step": 830
    },
    {
      "epoch": 0.448,
      "grad_norm": 2.403047800064087,
      "learning_rate": 1.8506666666666667e-05,
      "loss": 0.2305,
      "step": 840
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 3.737492084503174,
      "learning_rate": 1.848888888888889e-05,
      "loss": 0.2183,
      "step": 850
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 1.7343499660491943,
      "learning_rate": 1.8471111111111112e-05,
      "loss": 0.2477,
      "step": 860
    },
    {
      "epoch": 0.464,
      "grad_norm": 2.6099650859832764,
      "learning_rate": 1.8453333333333335e-05,
      "loss": 0.2471,
      "step": 870
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 2.934739828109741,
      "learning_rate": 1.8435555555555558e-05,
      "loss": 0.2116,
      "step": 880
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 3.5620553493499756,
      "learning_rate": 1.8417777777777777e-05,
      "loss": 0.2487,
      "step": 890
    },
    {
      "epoch": 0.48,
      "grad_norm": 2.037273645401001,
      "learning_rate": 1.8400000000000003e-05,
      "loss": 0.2105,
      "step": 900
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 1.7175401449203491,
      "learning_rate": 1.8382222222222223e-05,
      "loss": 0.2401,
      "step": 910
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 3.304821729660034,
      "learning_rate": 1.8364444444444446e-05,
      "loss": 0.2362,
      "step": 920
    },
    {
      "epoch": 0.496,
      "grad_norm": 2.1050398349761963,
      "learning_rate": 1.834666666666667e-05,
      "loss": 0.2473,
      "step": 930
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 3.5820703506469727,
      "learning_rate": 1.832888888888889e-05,
      "loss": 0.2546,
      "step": 940
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 4.120608329772949,
      "learning_rate": 1.8311111111111114e-05,
      "loss": 0.2051,
      "step": 950
    },
    {
      "epoch": 0.512,
      "grad_norm": 2.4124093055725098,
      "learning_rate": 1.8293333333333333e-05,
      "loss": 0.213,
      "step": 960
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 3.3322222232818604,
      "learning_rate": 1.8275555555555556e-05,
      "loss": 0.2348,
      "step": 970
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 2.556062698364258,
      "learning_rate": 1.825777777777778e-05,
      "loss": 0.191,
      "step": 980
    },
    {
      "epoch": 0.528,
      "grad_norm": 3.3162920475006104,
      "learning_rate": 1.824e-05,
      "loss": 0.1954,
      "step": 990
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 3.3258445262908936,
      "learning_rate": 1.8222222222222224e-05,
      "loss": 0.2115,
      "step": 1000
    },
    {
      "epoch": 0.5386666666666666,
      "grad_norm": 2.0840632915496826,
      "learning_rate": 1.8204444444444447e-05,
      "loss": 0.2379,
      "step": 1010
    },
    {
      "epoch": 0.544,
      "grad_norm": 2.310572385787964,
      "learning_rate": 1.8186666666666666e-05,
      "loss": 0.1978,
      "step": 1020
    },
    {
      "epoch": 0.5493333333333333,
      "grad_norm": 3.293782949447632,
      "learning_rate": 1.8168888888888893e-05,
      "loss": 0.1668,
      "step": 1030
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 4.34323787689209,
      "learning_rate": 1.8151111111111112e-05,
      "loss": 0.211,
      "step": 1040
    },
    {
      "epoch": 0.56,
      "grad_norm": 3.9913852214813232,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 0.2473,
      "step": 1050
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 4.405982494354248,
      "learning_rate": 1.8115555555555558e-05,
      "loss": 0.2394,
      "step": 1060
    },
    {
      "epoch": 0.5706666666666667,
      "grad_norm": 2.1960771083831787,
      "learning_rate": 1.8097777777777777e-05,
      "loss": 0.2123,
      "step": 1070
    },
    {
      "epoch": 0.576,
      "grad_norm": 3.6924808025360107,
      "learning_rate": 1.8080000000000003e-05,
      "loss": 0.1912,
      "step": 1080
    },
    {
      "epoch": 0.5813333333333334,
      "grad_norm": 1.9034916162490845,
      "learning_rate": 1.8062222222222222e-05,
      "loss": 0.1843,
      "step": 1090
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 3.310030221939087,
      "learning_rate": 1.8044444444444445e-05,
      "loss": 0.2214,
      "step": 1100
    },
    {
      "epoch": 0.592,
      "grad_norm": 2.9158389568328857,
      "learning_rate": 1.8026666666666668e-05,
      "loss": 0.209,
      "step": 1110
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 2.7702107429504395,
      "learning_rate": 1.800888888888889e-05,
      "loss": 0.1999,
      "step": 1120
    },
    {
      "epoch": 0.6026666666666667,
      "grad_norm": 2.4558396339416504,
      "learning_rate": 1.7991111111111114e-05,
      "loss": 0.2504,
      "step": 1130
    },
    {
      "epoch": 0.608,
      "grad_norm": 1.7969404458999634,
      "learning_rate": 1.7973333333333333e-05,
      "loss": 0.2028,
      "step": 1140
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 3.390717029571533,
      "learning_rate": 1.7955555555555556e-05,
      "loss": 0.2005,
      "step": 1150
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 2.6017730236053467,
      "learning_rate": 1.793777777777778e-05,
      "loss": 0.2209,
      "step": 1160
    },
    {
      "epoch": 0.624,
      "grad_norm": 2.357858419418335,
      "learning_rate": 1.792e-05,
      "loss": 0.2234,
      "step": 1170
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 4.677999973297119,
      "learning_rate": 1.7902222222222224e-05,
      "loss": 0.2309,
      "step": 1180
    },
    {
      "epoch": 0.6346666666666667,
      "grad_norm": 1.8672105073928833,
      "learning_rate": 1.7884444444444447e-05,
      "loss": 0.197,
      "step": 1190
    },
    {
      "epoch": 0.64,
      "grad_norm": 3.6874146461486816,
      "learning_rate": 1.7866666666666666e-05,
      "loss": 0.1959,
      "step": 1200
    },
    {
      "epoch": 0.6453333333333333,
      "grad_norm": 4.376802921295166,
      "learning_rate": 1.7848888888888892e-05,
      "loss": 0.2181,
      "step": 1210
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 4.053673267364502,
      "learning_rate": 1.783111111111111e-05,
      "loss": 0.216,
      "step": 1220
    },
    {
      "epoch": 0.656,
      "grad_norm": 4.080118656158447,
      "learning_rate": 1.7813333333333334e-05,
      "loss": 0.2424,
      "step": 1230
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 1.7412792444229126,
      "learning_rate": 1.7795555555555557e-05,
      "loss": 0.1821,
      "step": 1240
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 3.284057140350342,
      "learning_rate": 1.7777777777777777e-05,
      "loss": 0.2298,
      "step": 1250
    },
    {
      "epoch": 0.672,
      "grad_norm": 3.2456586360931396,
      "learning_rate": 1.7760000000000003e-05,
      "loss": 0.1813,
      "step": 1260
    },
    {
      "epoch": 0.6773333333333333,
      "grad_norm": 2.6079695224761963,
      "learning_rate": 1.7742222222222222e-05,
      "loss": 0.2,
      "step": 1270
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 3.006305456161499,
      "learning_rate": 1.7724444444444445e-05,
      "loss": 0.1972,
      "step": 1280
    },
    {
      "epoch": 0.688,
      "grad_norm": 2.8665385246276855,
      "learning_rate": 1.7706666666666668e-05,
      "loss": 0.1762,
      "step": 1290
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 3.0240628719329834,
      "learning_rate": 1.768888888888889e-05,
      "loss": 0.2271,
      "step": 1300
    },
    {
      "epoch": 0.6986666666666667,
      "grad_norm": 3.154467821121216,
      "learning_rate": 1.7671111111111113e-05,
      "loss": 0.1822,
      "step": 1310
    },
    {
      "epoch": 0.704,
      "grad_norm": 2.85445499420166,
      "learning_rate": 1.7653333333333336e-05,
      "loss": 0.2175,
      "step": 1320
    },
    {
      "epoch": 0.7093333333333334,
      "grad_norm": 1.9365614652633667,
      "learning_rate": 1.7635555555555555e-05,
      "loss": 0.1735,
      "step": 1330
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 1.8405073881149292,
      "learning_rate": 1.761777777777778e-05,
      "loss": 0.2431,
      "step": 1340
    },
    {
      "epoch": 0.72,
      "grad_norm": 3.301166534423828,
      "learning_rate": 1.76e-05,
      "loss": 0.1927,
      "step": 1350
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 2.8007845878601074,
      "learning_rate": 1.7582222222222224e-05,
      "loss": 0.2075,
      "step": 1360
    },
    {
      "epoch": 0.7306666666666667,
      "grad_norm": 2.0972020626068115,
      "learning_rate": 1.7564444444444446e-05,
      "loss": 0.2142,
      "step": 1370
    },
    {
      "epoch": 0.736,
      "grad_norm": 2.102510690689087,
      "learning_rate": 1.7546666666666666e-05,
      "loss": 0.2069,
      "step": 1380
    },
    {
      "epoch": 0.7413333333333333,
      "grad_norm": 2.2479143142700195,
      "learning_rate": 1.7528888888888892e-05,
      "loss": 0.1962,
      "step": 1390
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 2.675490379333496,
      "learning_rate": 1.751111111111111e-05,
      "loss": 0.2127,
      "step": 1400
    },
    {
      "epoch": 0.752,
      "grad_norm": 2.9734835624694824,
      "learning_rate": 1.7493333333333334e-05,
      "loss": 0.1647,
      "step": 1410
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 3.5934033393859863,
      "learning_rate": 1.7475555555555557e-05,
      "loss": 0.2008,
      "step": 1420
    },
    {
      "epoch": 0.7626666666666667,
      "grad_norm": 2.324141025543213,
      "learning_rate": 1.745777777777778e-05,
      "loss": 0.2008,
      "step": 1430
    },
    {
      "epoch": 0.768,
      "grad_norm": 2.764238119125366,
      "learning_rate": 1.7440000000000002e-05,
      "loss": 0.1899,
      "step": 1440
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 2.915379524230957,
      "learning_rate": 1.7422222222222222e-05,
      "loss": 0.1946,
      "step": 1450
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 3.5540740489959717,
      "learning_rate": 1.7404444444444445e-05,
      "loss": 0.2025,
      "step": 1460
    },
    {
      "epoch": 0.784,
      "grad_norm": 2.3161509037017822,
      "learning_rate": 1.7386666666666667e-05,
      "loss": 0.1835,
      "step": 1470
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 1.988091230392456,
      "learning_rate": 1.736888888888889e-05,
      "loss": 0.1699,
      "step": 1480
    },
    {
      "epoch": 0.7946666666666666,
      "grad_norm": 5.1438374519348145,
      "learning_rate": 1.7351111111111113e-05,
      "loss": 0.2117,
      "step": 1490
    },
    {
      "epoch": 0.8,
      "grad_norm": 4.64500617980957,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.2147,
      "step": 1500
    },
    {
      "epoch": 0.8053333333333333,
      "grad_norm": 3.162074327468872,
      "learning_rate": 1.7315555555555555e-05,
      "loss": 0.2074,
      "step": 1510
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 3.123638153076172,
      "learning_rate": 1.729777777777778e-05,
      "loss": 0.222,
      "step": 1520
    },
    {
      "epoch": 0.816,
      "grad_norm": 3.1180825233459473,
      "learning_rate": 1.728e-05,
      "loss": 0.1994,
      "step": 1530
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 2.9998488426208496,
      "learning_rate": 1.7262222222222223e-05,
      "loss": 0.2055,
      "step": 1540
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 3.0389208793640137,
      "learning_rate": 1.7244444444444446e-05,
      "loss": 0.2007,
      "step": 1550
    },
    {
      "epoch": 0.832,
      "grad_norm": 2.6446969509124756,
      "learning_rate": 1.7226666666666665e-05,
      "loss": 0.2006,
      "step": 1560
    },
    {
      "epoch": 0.8373333333333334,
      "grad_norm": 3.8680787086486816,
      "learning_rate": 1.720888888888889e-05,
      "loss": 0.1862,
      "step": 1570
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 3.1376025676727295,
      "learning_rate": 1.719111111111111e-05,
      "loss": 0.1932,
      "step": 1580
    },
    {
      "epoch": 0.848,
      "grad_norm": 3.2271485328674316,
      "learning_rate": 1.7173333333333334e-05,
      "loss": 0.1996,
      "step": 1590
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 2.6277849674224854,
      "learning_rate": 1.7155555555555557e-05,
      "loss": 0.1438,
      "step": 1600
    },
    {
      "epoch": 0.8586666666666667,
      "grad_norm": 4.179690837860107,
      "learning_rate": 1.713777777777778e-05,
      "loss": 0.1793,
      "step": 1610
    },
    {
      "epoch": 0.864,
      "grad_norm": 2.0434560775756836,
      "learning_rate": 1.7120000000000002e-05,
      "loss": 0.1621,
      "step": 1620
    },
    {
      "epoch": 0.8693333333333333,
      "grad_norm": 3.5542125701904297,
      "learning_rate": 1.7102222222222225e-05,
      "loss": 0.1815,
      "step": 1630
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 2.0336673259735107,
      "learning_rate": 1.7084444444444444e-05,
      "loss": 0.1871,
      "step": 1640
    },
    {
      "epoch": 0.88,
      "grad_norm": 2.940936803817749,
      "learning_rate": 1.706666666666667e-05,
      "loss": 0.1845,
      "step": 1650
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 2.652810573577881,
      "learning_rate": 1.704888888888889e-05,
      "loss": 0.2206,
      "step": 1660
    },
    {
      "epoch": 0.8906666666666667,
      "grad_norm": 2.7457945346832275,
      "learning_rate": 1.7031111111111113e-05,
      "loss": 0.1743,
      "step": 1670
    },
    {
      "epoch": 0.896,
      "grad_norm": 3.310251474380493,
      "learning_rate": 1.7013333333333335e-05,
      "loss": 0.2144,
      "step": 1680
    },
    {
      "epoch": 0.9013333333333333,
      "grad_norm": 1.6278772354125977,
      "learning_rate": 1.6995555555555555e-05,
      "loss": 0.1921,
      "step": 1690
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 2.5803816318511963,
      "learning_rate": 1.697777777777778e-05,
      "loss": 0.1905,
      "step": 1700
    },
    {
      "epoch": 0.912,
      "grad_norm": 2.628675937652588,
      "learning_rate": 1.696e-05,
      "loss": 0.1888,
      "step": 1710
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 2.649833917617798,
      "learning_rate": 1.6942222222222223e-05,
      "loss": 0.1945,
      "step": 1720
    },
    {
      "epoch": 0.9226666666666666,
      "grad_norm": 3.0059049129486084,
      "learning_rate": 1.6924444444444446e-05,
      "loss": 0.1921,
      "step": 1730
    },
    {
      "epoch": 0.928,
      "grad_norm": 2.3337936401367188,
      "learning_rate": 1.690666666666667e-05,
      "loss": 0.1673,
      "step": 1740
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 3.3886091709136963,
      "learning_rate": 1.688888888888889e-05,
      "loss": 0.1799,
      "step": 1750
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 3.1414694786071777,
      "learning_rate": 1.687111111111111e-05,
      "loss": 0.1949,
      "step": 1760
    },
    {
      "epoch": 0.944,
      "grad_norm": 3.861191987991333,
      "learning_rate": 1.6853333333333333e-05,
      "loss": 0.2004,
      "step": 1770
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 3.5039045810699463,
      "learning_rate": 1.6835555555555556e-05,
      "loss": 0.1855,
      "step": 1780
    },
    {
      "epoch": 0.9546666666666667,
      "grad_norm": 2.468499183654785,
      "learning_rate": 1.681777777777778e-05,
      "loss": 0.1543,
      "step": 1790
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.7443112134933472,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.1443,
      "step": 1800
    },
    {
      "epoch": 0.9653333333333334,
      "grad_norm": 2.543004274368286,
      "learning_rate": 1.6782222222222225e-05,
      "loss": 0.2252,
      "step": 1810
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 2.444000482559204,
      "learning_rate": 1.6764444444444444e-05,
      "loss": 0.1437,
      "step": 1820
    },
    {
      "epoch": 0.976,
      "grad_norm": 2.1584396362304688,
      "learning_rate": 1.674666666666667e-05,
      "loss": 0.1633,
      "step": 1830
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 3.2354915142059326,
      "learning_rate": 1.672888888888889e-05,
      "loss": 0.1837,
      "step": 1840
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 1.441145420074463,
      "learning_rate": 1.6711111111111112e-05,
      "loss": 0.1456,
      "step": 1850
    },
    {
      "epoch": 0.992,
      "grad_norm": 1.8785091638565063,
      "learning_rate": 1.6693333333333335e-05,
      "loss": 0.1388,
      "step": 1860
    },
    {
      "epoch": 0.9973333333333333,
      "grad_norm": 2.640275478363037,
      "learning_rate": 1.6675555555555554e-05,
      "loss": 0.1874,
      "step": 1870
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9522,
      "eval_loss": 0.16177479922771454,
      "eval_runtime": 47.3293,
      "eval_samples_per_second": 422.571,
      "eval_steps_per_second": 4.416,
      "step": 1875
    }
  ],
  "logging_steps": 10,
  "max_steps": 11250,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7122344878080000.0,
  "train_batch_size": 96,
  "trial_name": null,
  "trial_params": null
}
